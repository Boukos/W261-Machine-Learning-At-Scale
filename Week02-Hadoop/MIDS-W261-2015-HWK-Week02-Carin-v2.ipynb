{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 02\n",
    "- Submission date: 9/15/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is a race condition in the context of parallel computation? Give an example.**\n",
    "\n",
    "2. **What is MapReduce?**\n",
    "\n",
    "3. **How does it differ from Hadoop?**\n",
    "\n",
    "4. **Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A **race condition** is a situation, in parallel computation, in which the final value can be different depending on the order of parallel processes. For example, it can occur if two threads read and write (after performing some computation) the same variable: it one thread reads and writes the variable, and then the other thread reads and writes that same variable after that, the result will be different than what it would be if the second threads reads the variable before the first one has written it.\n",
    "\n",
    "2. **MapReduce** is a programming model (and an associated implementation) for processing and generating large data sets with a parallel, distributed algorithm on a cluster. An implementation of this model (i.e., a MapReduce program) is composed of a `map` procedure (that performs filtering and sorting) and a `reduce` method (that performs a summary operation).\n",
    "\n",
    "3. (Apache) **Hadoop** uses MapReduce, but is more than that: it is a software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware, whose core consists of MapReduce (for processing) and HDFS (Hadoop Distributed File System; for storage). It is also composed of other modules, apart from those two: Hadoop Common, which contains libraries and utilities needed by other Hadoop modules, and Hadoop YARN, which is a resource-management platform responsible for managing computing resources in clusters and using them for scheduling of users' applications. Hadoop can also refer to the whole ecosystem or collection of additional software packages that can be installed on top of or alongside it, such as Apache Pig, Apache Hive, Apache Spark, Apache Storm, etc.\n",
    "\n",
    "4. As mentioned, Hadoop is based on the MapReduce (or map-and-reduce) programming paradigm, which in turn is based on functional programming and parallel computation. What Hadoop does (in its simplest form) is splitting the input into several chunks, processing those chunks in parallel with a `map` task, and combine the intermediate result of each mapper by means of `reducer` tasks.  **An example is given below, in HW2.1, where the first 10,000 integers, which are shuffled and in string format, are sorted.** Sorting the strings would not work, because they would be sorted according to the leading digit(s) . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798,\\n 98,\\n 2043,\r\n"
     ]
    }
   ],
   "source": [
    "!echo '798,\\n' '98,\\n' '2043,' | sort -k1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". . . so the mappers include leading zeros to each number (in string format) in the portion of the data passed to each one, and the reducer discards those leading zeros (and the sorting is done by the Hadoop framework, no need to code it!).\n",
    "\n",
    "So the command line equivalent to Hadoop code (given the same mapper and reducer, as briefly described above) would be equivalent to:\n",
    "\n",
    "```python\n",
    "!echo '798,\\n' '98,\\n' '2043,' | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```\n",
    "\n",
    "which would sort those three numbers correctly:\n",
    "\n",
    "`98,\n",
    "798,\n",
    "2043`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort in Hadoop MapReduce**\n",
    "\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where `integer` is any integer, and `“NA”` is just the empty string.**\n",
    "\n",
    "**Output: sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "**Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000.**\n",
    "\n",
    "**Write the python Hadoop streaming map-reduce job to perform this sort.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6837', '6188', '8462', '7963', '7003', '1219', '7617', '2948', '173', '708']\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "from random import sample\n",
    "with open('input', 'w') as myfile:\n",
    "    # Sample (without replacement) from 1 to N\n",
    "    integer = sample(range(1, N+1), N) \n",
    "    for i in range(N):\n",
    "        # Add \",\" and an empty string to each integer\n",
    "        myfile.write(str(integer[i])+',\\n')\n",
    "    myfile.close()\n",
    "\n",
    "# Let's check a few of the generated records, to see if they're random\n",
    "with open('input', 'r') as myfile:\n",
    "    text = [word.strip(\",.\") for line in myfile for word in line.split()]\n",
    "print text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Discard the \",\" (keep only the integer=key)\n",
    "    line = line.strip().rstrip(',')\n",
    "    # Convert to string and add leading zeros\n",
    "    integer = str(line).zfill(5)\n",
    "    # Intermediate result (empty value)\n",
    "    print '%s\\t%s' % (integer, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Remove value (empty) keeping only the key and convert to int \n",
    "        # (which discards leading zeros)\n",
    "    integer = int(line.strip())\n",
    "    # Same format as the original input (\",\" and empty string)\n",
    "    print '%s%s' % (integer, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-juanjo-VB.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-juanjo-VB.out\n",
      "15/12/09 14:52:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-juanjo-VB.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-juanjo-VB.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-juanjo-VB.out\n",
      "15/12/09 14:53:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Hadoop\n",
    "!/usr/local/hadoop/sbin/start-yarn.sh\n",
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:53:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Create new folder\n",
    "!hdfs dfs -mkdir -p /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:53:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: Cannot create file/user/hadoop/dirhw21/input._COPYING_. Name node is in safe mode.\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f input /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:54:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/09 14:54:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted sortOutput\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r sortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:54:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/09 14:54:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/12/09 14:54:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/12/09 14:54:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/12/09 14:54:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/12/09 14:54:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/12/09 14:54:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/12/09 14:54:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1008758741_0001\n",
      "15/12/09 14:54:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/12/09 14:54:51 INFO mapreduce.Job: Running job: job_local1008758741_0001\n",
      "15/12/09 14:54:51 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/12/09 14:54:51 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/12/09 14:54:51 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/12/09 14:54:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1008758741_0001_m_000000_0\n",
      "15/12/09 14:54:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/12/09 14:54:51 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/input:0+58894\n",
      "15/12/09 14:54:51 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/12/09 14:54:52 INFO mapreduce.Job: Job job_local1008758741_0001 running in uber mode : false\n",
      "15/12/09 14:54:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/12/09 14:54:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/12/09 14:54:52 INFO streaming.PipeMapRed: PipeMapRed exec [/home/shared/disk/Dropbox2/Dropbox/W261/HW2/./mapper.py]\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/12/09 14:54:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/12/09 14:54:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:54 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/12/09 14:54:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/12/09 14:54:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/12/09 14:54:54 INFO mapred.LocalJobRunner: \n",
      "15/12/09 14:54:54 INFO mapred.MapTask: Starting flush of map output\n",
      "15/12/09 14:54:54 INFO mapred.MapTask: Spilling map output\n",
      "15/12/09 14:54:54 INFO mapred.MapTask: bufstart = 0; bufend = 70000; bufvoid = 104857600\n",
      "15/12/09 14:54:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "15/12/09 14:54:55 INFO mapred.MapTask: Finished spill 0\n",
      "15/12/09 14:54:55 INFO mapred.Task: Task:attempt_local1008758741_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "15/12/09 14:54:55 INFO mapred.Task: Task 'attempt_local1008758741_0001_m_000000_0' done.\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local1008758741_0001_m_000000_0\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: Starting task: attempt_local1008758741_0001_r_000000_0\n",
      "15/12/09 14:54:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/12/09 14:54:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5092a298\n",
      "15/12/09 14:54:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/12/09 14:54:55 INFO reduce.EventFetcher: attempt_local1008758741_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/12/09 14:54:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1008758741_0001_m_000000_0 decomp: 90002 len: 90006 to MEMORY\n",
      "15/12/09 14:54:55 INFO reduce.InMemoryMapOutput: Read 90002 bytes from map-output for attempt_local1008758741_0001_m_000000_0\n",
      "15/12/09 14:54:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 90002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->90002\n",
      "15/12/09 14:54:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/12/09 14:54:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:54:55 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/12/09 14:54:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/12/09 14:54:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89994 bytes\n",
      "15/12/09 14:54:56 INFO reduce.MergeManagerImpl: Merged 1 segments, 90002 bytes to disk to satisfy reduce memory limit\n",
      "15/12/09 14:54:56 INFO reduce.MergeManagerImpl: Merging 1 files, 90006 bytes from disk\n",
      "15/12/09 14:54:56 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/12/09 14:54:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/12/09 14:54:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89994 bytes\n",
      "15/12/09 14:54:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:54:56 INFO streaming.PipeMapRed: PipeMapRed exec [/home/shared/disk/Dropbox2/Dropbox/W261/HW2/./reducer.py]\n",
      "15/12/09 14:54:56 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/12/09 14:54:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/12/09 14:54:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/12/09 14:54:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:56 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:54:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/12/09 14:54:57 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/12/09 14:54:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/12/09 14:54:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/12/09 14:54:58 INFO mapred.Task: Task:attempt_local1008758741_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/12/09 14:54:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:54:58 INFO mapred.Task: Task attempt_local1008758741_0001_r_000000_0 is allowed to commit now\n",
      "15/12/09 14:54:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1008758741_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/sortOutput/_temporary/0/task_local1008758741_0001_r_000000\n",
      "15/12/09 14:54:59 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "15/12/09 14:54:59 INFO mapred.Task: Task 'attempt_local1008758741_0001_r_000000_0' done.\n",
      "15/12/09 14:54:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1008758741_0001_r_000000_0\n",
      "15/12/09 14:54:59 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/12/09 14:54:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/12/09 14:55:00 INFO mapreduce.Job: Job job_local1008758741_0001 completed successfully\n",
      "15/12/09 14:55:00 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=390314\n",
      "\t\tFILE: Number of bytes written=992874\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=117788\n",
      "\t\tHDFS: Number of bytes written=68894\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=70000\n",
      "\t\tMap output materialized bytes=90006\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=90006\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=197\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=58894\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=68894\n",
      "15/12/09 14:55:00 INFO streaming.StreamJob: Output directory: sortOutput\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "    # Forcing number of reducers to be 1\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -D mapred.reduce.tasks=1 -mapper mapper.py \\\n",
    "    -reducer reducer.py -input /user/hadoop/dirhw21/input -output sortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘/home/hduser/Downloads/HW2/part-00000’: No such file or directory\n",
      "15/12/09 14:55:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Move output to local (rather than direcly reading its content)\n",
    "#!hdfs dfs -cat sortOutput/part-00000\n",
    "# Delete it from local if a previous version exists\n",
    "!rm ~/Downloads/HW2/part-00000\n",
    "!hdfs dfs -copyToLocal sortOutput/part-00000 ~/Downloads/HW2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\t\r\n",
      "2,\t\r\n",
      "3,\t\r\n",
      "4,\t\r\n",
      "5,\t\r\n",
      "6,\t\r\n",
      "7,\t\r\n",
      "8,\t\r\n",
      "9,\t\r\n",
      "10,\t\r\n",
      "11,\t\r\n",
      "12,\t\r\n",
      "13,\t\r\n",
      "14,\t\r\n",
      "15,\t\r\n",
      "16,\t\r\n",
      "17,\t\r\n",
      "18,\t\r\n",
      "19,\t\r\n",
      "20,\t\r\n"
     ]
    }
   ],
   "source": [
    "# Read first records to check they were sorted\n",
    "!head -20 ~/Downloads/HW2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use multiple reducers, running the following, for example:\n",
    "\n",
    "```python\n",
    "# Hadoop streaming command\n",
    "!hadoop jar hadoop-streaming*.jar -D mapred.reduce.tasks=2 -mapper mapper.py \\\n",
    "    -reducer reducer.py -input /user/hadoop/dirhw21/input -output sortOutput\n",
    "```\n",
    "the first records of output `part-00000` are:\n",
    "\n",
    "`1,\t\n",
    "3,\t\n",
    "5,\t\n",
    "7,\t\n",
    "9,\t\n",
    "10,\t\n",
    "12,\t\n",
    "14,\t\n",
    "16,\t\n",
    "18,\t\n",
    "21,\t\n",
    "23,\t\n",
    "25,\t\n",
    "27,\t\n",
    "29,\t\n",
    "30,\t\n",
    "32,\t\n",
    "34,\t\n",
    "36,\t\n",
    "38,\t`\n",
    "\n",
    "I.e., there are more than one output (as many as reducers), and each one is sorted, but comes from just one part of the mappers, so it does not necessarily have to include a consecutive subset of records.\n",
    "\n",
    "Say there are 4 mappers whose inputs are `<4,>`, `<11,>`, `<7,>`, `<2,>`, `<6,>`, `<10,>`, `<8,>`, `<5,>`, `<12,>`, `<3,>`, `<9,>`, `<1,>`. If the outputs of the first 2 mappers are processed by a reducer, and the outputs of the remaining 2 mappers are processed by a second reducer, their outputs would be `<2,>`, `<4,>`, `<6,>`, `<7,>`, `<10,>`, `<11,>`, and `<1,>`, `<3,>`, `<5,>`, `<8,>`, `<9,>`, `<12,>`, respectively.\n",
    "\n",
    "**We would have to apply a 2nd MapReduce layer, this one with only one Reducer at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.**\n",
    "\n",
    "**To do so, make sure that**\n",
    "\n",
    "- **mapper.py counts all occurrences of a single word, and**\n",
    "- **reducer.py collates the counts of the single word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword = env_vars['findword']\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "word_count = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    for w in WORD_RE.findall(line):\n",
    "        if findword.lower() == w.lower():\n",
    "            word_count += 1\n",
    "print findword + '\\t' + str(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sum_words = 0\n",
    "for line in sys.stdin:\n",
    "    key_value = line.split('\\t')\n",
    "    # The key is the single word we're counting\n",
    "    key = key_value[0]\n",
    "    # And each value, its count from a mapper\n",
    "    value = key_value[1]\n",
    "    sum_words += int(value)\n",
    "print key + '\\t' + str(sum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:55:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f enronemail_1h.txt /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:55:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/09 14:55:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted countWordEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r countWordEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:55:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/12/09 14:55:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/12/09 14:55:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/12/09 14:55:46 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/12/09 14:55:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/12/09 14:55:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/12/09 14:55:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1448764630_0001\n",
      "15/12/09 14:55:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/12/09 14:55:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/12/09 14:55:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/12/09 14:55:49 INFO mapreduce.Job: Running job: job_local1448764630_0001\n",
      "15/12/09 14:55:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/12/09 14:55:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1448764630_0001_m_000000_0\n",
      "15/12/09 14:55:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/12/09 14:55:49 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/12/09 14:55:49 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/12/09 14:55:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/12/09 14:55:50 INFO streaming.PipeMapRed: PipeMapRed exec [/home/shared/disk/Dropbox2/Dropbox/W261/HW2/./mapper.py]\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/12/09 14:55:50 INFO mapreduce.Job: Job job_local1448764630_0001 running in uber mode : false\n",
      "15/12/09 14:55:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/12/09 14:55:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/12/09 14:55:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:55:50 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:55:50 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:55:51 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/12/09 14:55:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/12/09 14:55:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: \n",
      "15/12/09 14:55:51 INFO mapred.MapTask: Starting flush of map output\n",
      "15/12/09 14:55:51 INFO mapred.MapTask: Spilling map output\n",
      "15/12/09 14:55:51 INFO mapred.MapTask: bufstart = 0; bufend = 14; bufvoid = 104857600\n",
      "15/12/09 14:55:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "15/12/09 14:55:51 INFO mapred.MapTask: Finished spill 0\n",
      "15/12/09 14:55:51 INFO mapred.Task: Task:attempt_local1448764630_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/12/09 14:55:51 INFO mapred.Task: Task 'attempt_local1448764630_0001_m_000000_0' done.\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local1448764630_0001_m_000000_0\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1448764630_0001_r_000000_0\n",
      "15/12/09 14:55:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/12/09 14:55:51 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/12/09 14:55:51 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3fe59a4d\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/12/09 14:55:51 INFO reduce.EventFetcher: attempt_local1448764630_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/12/09 14:55:51 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1448764630_0001_m_000000_0 decomp: 18 len: 22 to MEMORY\n",
      "15/12/09 14:55:51 INFO reduce.InMemoryMapOutput: Read 18 bytes from map-output for attempt_local1448764630_0001_m_000000_0\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18\n",
      "15/12/09 14:55:51 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/12/09 14:55:51 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/12/09 14:55:51 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5 bytes\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: Merged 1 segments, 18 bytes to disk to satisfy reduce memory limit\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: Merging 1 files, 22 bytes from disk\n",
      "15/12/09 14:55:51 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/12/09 14:55:51 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/12/09 14:55:51 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5 bytes\n",
      "15/12/09 14:55:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:55:51 INFO streaming.PipeMapRed: PipeMapRed exec [/home/shared/disk/Dropbox2/Dropbox/W261/HW2/./reducer.py]\n",
      "15/12/09 14:55:51 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/12/09 14:55:51 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/12/09 14:55:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/12/09 14:55:52 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/12/09 14:55:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/12/09 14:55:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/12/09 14:55:52 INFO mapred.Task: Task:attempt_local1448764630_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/12/09 14:55:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/12/09 14:55:52 INFO mapred.Task: Task attempt_local1448764630_0001_r_000000_0 is allowed to commit now\n",
      "15/12/09 14:55:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1448764630_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/countWordEnron/_temporary/0/task_local1448764630_0001_r_000000\n",
      "15/12/09 14:55:52 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "15/12/09 14:55:52 INFO mapred.Task: Task 'attempt_local1448764630_0001_r_000000_0' done.\n",
      "15/12/09 14:55:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1448764630_0001_r_000000_0\n",
      "15/12/09 14:55:52 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/12/09 14:55:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/12/09 14:55:53 INFO mapreduce.Job: Job job_local1448764630_0001 completed successfully\n",
      "15/12/09 14:55:53 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210372\n",
      "\t\tFILE: Number of bytes written=722886\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=14\n",
      "\t\tMap output materialized bytes=22\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=22\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=166\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "15/12/09 14:55:53 INFO streaming.StreamJob: Output directory: countWordEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance -input /user/hadoop/dirhw21/enronemail_1h.txt \\\n",
    "    -output countWordEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/09 14:56:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat countWordEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result than in HW1, because I'm including the subject of the emails, and all occurrences in a single email are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that**\n",
    "\n",
    "- **mapper.py**\n",
    "- **reducer.py**\n",
    "\n",
    "**performs a single word multinomial Naive Bayes classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword = env_vars['findword']\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    word_count = WORD_RE.findall(content).count(findword)\n",
    "    print findword + '\\t' + str(word_count) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is the single word we're counting\n",
    "    word = key_values[0]\n",
    "    word_count.append(int(key_values[1]))\n",
    "    ID.append(key_values[2])\n",
    "    TRUTH.append(int(key_values[3]))\n",
    "\n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "# Total count of word in spam emails\n",
    "total_word_spam = sum([x*y for (x,y) in zip(TRUTH,word_count)]) \n",
    "# Total count of word in ham emails\n",
    "total_word_ham = sum(word_count) - total_word_spam\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = float(total_word_ham + 1) / (total_word_ham + 1)\n",
    "prob_word_spam = float(total_word_spam + 1) / (total_word_spam + 1)\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*len(TRUTH)\n",
    "for i in range(len(TRUTH)): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + word_count[i]*log(prob_word_ham,10)\n",
    "    prob_spam_word = log(prob_spam,10) + word_count[i]*log(prob_word_spam,10)\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:21:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:21:35 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted singleNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r singleNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:21:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:21:39 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/19 17:21:39 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/19 17:21:39 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/19 17:21:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/19 17:21:40 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/19 17:21:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local82524836_0001\n",
      "15/09/19 17:21:41 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/19 17:21:41 INFO mapreduce.Job: Running job: job_local82524836_0001\n",
      "15/09/19 17:21:41 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/19 17:21:41 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/19 17:21:41 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/19 17:21:41 INFO mapred.LocalJobRunner: Starting task: attempt_local82524836_0001_m_000000_0\n",
      "15/09/19 17:21:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/19 17:21:42 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/19 17:21:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper.py]\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/19 17:21:42 INFO mapreduce.Job: Job job_local82524836_0001 running in uber mode : false\n",
      "15/09/19 17:21:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/19 17:21:42 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:21:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: \n",
      "15/09/19 17:21:43 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/19 17:21:43 INFO mapred.MapTask: Spilling map output\n",
      "15/09/19 17:21:43 INFO mapred.MapTask: bufstart = 0; bufend = 3772; bufvoid = 104857600\n",
      "15/09/19 17:21:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/19 17:21:43 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/19 17:21:43 INFO mapred.Task: Task:attempt_local82524836_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/19 17:21:43 INFO mapred.Task: Task 'attempt_local82524836_0001_m_000000_0' done.\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local82524836_0001_m_000000_0\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/19 17:21:43 INFO mapred.LocalJobRunner: Starting task: attempt_local82524836_0001_r_000000_0\n",
      "15/09/19 17:21:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/19 17:21:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:21:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3b3b2835\n",
      "15/09/19 17:21:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/19 17:21:43 INFO reduce.EventFetcher: attempt_local82524836_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/19 17:21:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local82524836_0001_m_000000_0 decomp: 3974 len: 3978 to MEMORY\n",
      "15/09/19 17:21:44 INFO reduce.InMemoryMapOutput: Read 3974 bytes from map-output for attempt_local82524836_0001_m_000000_0\n",
      "15/09/19 17:21:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3974, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3974\n",
      "15/09/19 17:21:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:21:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/19 17:21:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:21:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/19 17:21:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 3974 bytes to disk to satisfy reduce memory limit\n",
      "15/09/19 17:21:44 INFO reduce.MergeManagerImpl: Merging 1 files, 3978 bytes from disk\n",
      "15/09/19 17:21:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/19 17:21:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:21:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer.py]\n",
      "15/09/19 17:21:44 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/19 17:21:44 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/19 17:21:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:21:44 INFO mapred.Task: Task:attempt_local82524836_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:21:44 INFO mapred.Task: Task attempt_local82524836_0001_r_000000_0 is allowed to commit now\n",
      "15/09/19 17:21:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local82524836_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/singleNBEnron/_temporary/0/task_local82524836_0001_r_000000\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/19 17:21:44 INFO mapred.Task: Task 'attempt_local82524836_0001_r_000000_0' done.\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local82524836_0001_r_000000_0\n",
      "15/09/19 17:21:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/19 17:21:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/19 17:21:45 INFO mapreduce.Job: Job job_local82524836_0001 completed successfully\n",
      "15/09/19 17:21:45 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218284\n",
      "\t\tFILE: Number of bytes written=729230\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2692\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3772\n",
      "\t\tMap output materialized bytes=3978\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=3978\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=169\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2692\n",
      "15/09/19 17:21:45 INFO streaming.StreamJob: Output directory: singleNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance -input /user/hadoop/dirhw21/enronemail_1h.txt \\\n",
    "    -output singleNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:21:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0018.2003-12-18.GP\t1\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "Training Error\t0.44\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat singleNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I also included an additional line in the output with the training error, as defined in HW1. It's the same value that I got in HW1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#HW2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results**\n",
    "\n",
    "**To do so, make sure that**\n",
    "\n",
    "- **mapper.py **\n",
    "- **reducer.py**\n",
    "\n",
    "**performs the multiple-word multinomial Naive Bayes classification via the chosen list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword_list = env_vars['findword'].split(',')\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    for findword in findword_list:\n",
    "        word_count = WORD_RE.findall(content).count(findword)\n",
    "        print findword + '\\t' + str(word_count) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "word = []\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is ONE of the words we're counting\n",
    "    if len(word) != 0:\n",
    "        if word[-1] != key_values[0]:\n",
    "            word.append(key_values[0])\n",
    "    else:\n",
    "        word.append(key_values[0])\n",
    "    word_count.append(int(key_values[1]))\n",
    "    # ID and TRUTH are replicated for each word in the vocabulary\n",
    "        # so we just need to keep track of them once\n",
    "    if len(word) == 1:\n",
    "        ID.append(key_values[2])\n",
    "        TRUTH.append(int(key_values[3]))\n",
    "\n",
    "# The lists above will have a length equal to\n",
    "    # number_different_words_in_vocab * number_emails_in_dataset\n",
    "vocab_size = len(word)\n",
    "num_emails = len(TRUTH)\n",
    "# Reshape the list word_count into a 2-D numpy array\n",
    "word_count = np.array(word_count).reshape(len(word), num_emails)\n",
    "\n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "total_word_spam = [0]*len(word)\n",
    "total_word_ham = [0]*len(word)\n",
    "for i,w in enumerate(word):\n",
    "    # Total count of word w in spam emails\n",
    "    total_word_spam[i] = sum([x*y for (x,y) in zip(TRUTH,word_count[i])]) \n",
    "    # Total count of word w in ham emails\n",
    "    total_word_ham[i] = sum(word_count[i]) - total_word_spam[i]\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham)+vocab_size) for x \\\n",
    "                 in total_word_ham]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam)+vocab_size) for \\\n",
    "                  x in total_word_spam]\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*num_emails\n",
    "for i in range(num_emails): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count[:,i],prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count[:,i],prob_word_spam)])\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS[i] = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `multiNBEnron': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r multiNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:22:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/19 17:22:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/19 17:22:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/19 17:22:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/19 17:22:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/19 17:22:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local270049115_0001\n",
      "15/09/19 17:22:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/19 17:22:11 INFO mapreduce.Job: Running job: job_local270049115_0001\n",
      "15/09/19 17:22:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/19 17:22:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/19 17:22:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/19 17:22:12 INFO mapred.LocalJobRunner: Starting task: attempt_local270049115_0001_m_000000_0\n",
      "15/09/19 17:22:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/19 17:22:12 INFO mapreduce.Job: Job job_local270049115_0001 running in uber mode : false\n",
      "15/09/19 17:22:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/19 17:22:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/19 17:22:12 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper.py]\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/19 17:22:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/19 17:22:13 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/19 17:22:13 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/19 17:22:13 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/19 17:22:13 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/19 17:22:13 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: R/W/S=100/102/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:22:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: \n",
      "15/09/19 17:22:13 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/19 17:22:13 INFO mapred.MapTask: Spilling map output\n",
      "15/09/19 17:22:13 INFO mapred.MapTask: bufstart = 0; bufend = 11916; bufvoid = 104857600\n",
      "15/09/19 17:22:13 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213200(104852800); length = 1197/6553600\n",
      "15/09/19 17:22:13 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/19 17:22:13 INFO mapred.Task: Task:attempt_local270049115_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/19 17:22:13 INFO mapred.Task: Task 'attempt_local270049115_0001_m_000000_0' done.\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local270049115_0001_m_000000_0\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/19 17:22:13 INFO mapred.LocalJobRunner: Starting task: attempt_local270049115_0001_r_000000_0\n",
      "15/09/19 17:22:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:22:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f7422d9\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/19 17:22:14 INFO reduce.EventFetcher: attempt_local270049115_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/19 17:22:14 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local270049115_0001_m_000000_0 decomp: 12518 len: 12522 to MEMORY\n",
      "15/09/19 17:22:14 INFO reduce.InMemoryMapOutput: Read 12518 bytes from map-output for attempt_local270049115_0001_m_000000_0\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 12518, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->12518\n",
      "15/09/19 17:22:14 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/19 17:22:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/19 17:22:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:22:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: Merged 1 segments, 12518 bytes to disk to satisfy reduce memory limit\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: Merging 1 files, 12522 bytes from disk\n",
      "15/09/19 17:22:14 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/19 17:22:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:22:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/19 17:22:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer.py]\n",
      "15/09/19 17:22:14 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/19 17:22:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: Records R/W=300/1\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:22:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:22:15 INFO mapred.Task: Task:attempt_local270049115_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:22:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:15 INFO mapred.Task: Task attempt_local270049115_0001_r_000000_0 is allowed to commit now\n",
      "15/09/19 17:22:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local270049115_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/multiNBEnron/_temporary/0/task_local270049115_0001_r_000000\n",
      "15/09/19 17:22:15 INFO mapred.LocalJobRunner: Records R/W=300/1 > reduce\n",
      "15/09/19 17:22:15 INFO mapred.Task: Task 'attempt_local270049115_0001_r_000000_0' done.\n",
      "15/09/19 17:22:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local270049115_0001_r_000000_0\n",
      "15/09/19 17:22:15 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/19 17:22:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/19 17:22:15 INFO mapreduce.Job: Job job_local270049115_0001 completed successfully\n",
      "15/09/19 17:22:15 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=235372\n",
      "\t\tFILE: Number of bytes written=757730\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2691\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=300\n",
      "\t\tMap output bytes=11916\n",
      "\t\tMap output materialized bytes=12522\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=12522\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=205\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2691\n",
      "15/09/19 17:22:15 INFO streaming.StreamJob: Output directory: multiNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance,valium,enlargementWithATypo -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output multiNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "Training Error\t0.4\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat multiNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I also included an additional line in the output with the training error, as defined in HW1. It's (almost) the same value that I got in HW1.4 (actually, a bit better: 0.40 now, while it was 0.41 wiht the poor man's implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that will classify the email messages using words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage is a MapReduce job that learns the whole vocabulary from the training set. The output (a dictionary with all the words present) will be used in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "    # We search the word in both the subject and the content\n",
    "        # because one or the other may not exist, but the way the data are\n",
    "        # stored we don't know which one may be missing\n",
    "    content = re.sub('[^a-z]', ' ', content.lower())\n",
    "    # Discard non-alphanumeric characters and also numbers\n",
    "    words = content.split() # extract words\n",
    "    words = set(words) # extract unique words\n",
    "    vocabulary[1:1] = words # append to vocabulary\n",
    "for word in set(vocabulary):\n",
    "    print '%s\\t%s' % (word, 1) # value here is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "vocabulary = []\n",
    "for line in sys.stdin:\n",
    "    # Take key only (the word) and add to vocabulary if not present\n",
    "    word = line.split(\"\\t\")[0]\n",
    "    #if word not in vocabulary:\n",
    "    #    print word\n",
    "    # If we use the 2 lines above instead of the 3 lines below\n",
    "        # each word in the vocabulary goes in a new line\n",
    "        # (and there's no need to sort)\n",
    "    vocabulary.append(word)\n",
    "vocabulary = sorted(set(vocabulary)) # Get unique words\n",
    "print ' '.join(vocabulary) # Print words separated by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:22:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted dictionary\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:22:33 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/19 17:22:33 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/19 17:22:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/19 17:22:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/19 17:22:34 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/19 17:22:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/19 17:22:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local877582841_0001\n",
      "15/09/19 17:22:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/19 17:22:35 INFO mapreduce.Job: Running job: job_local877582841_0001\n",
      "15/09/19 17:22:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/19 17:22:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/19 17:22:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/19 17:22:35 INFO mapred.LocalJobRunner: Starting task: attempt_local877582841_0001_m_000000_0\n",
      "15/09/19 17:22:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/19 17:22:36 INFO mapreduce.Job: Job job_local877582841_0001 running in uber mode : false\n",
      "15/09/19 17:22:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/19 17:22:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/19 17:22:36 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper1.py]\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/19 17:22:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/19 17:22:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/19 17:22:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/19 17:22:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/19 17:22:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:22:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:22:37 INFO mapred.LocalJobRunner: \n",
      "15/09/19 17:22:37 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/19 17:22:37 INFO mapred.MapTask: Spilling map output\n",
      "15/09/19 17:22:37 INFO mapred.MapTask: bufstart = 0; bufend = 49187; bufvoid = 104857600\n",
      "15/09/19 17:22:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26194140(104776560); length = 20257/6553600\n",
      "15/09/19 17:22:38 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/19 17:22:38 INFO mapred.Task: Task:attempt_local877582841_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/19 17:22:38 INFO mapred.Task: Task 'attempt_local877582841_0001_m_000000_0' done.\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local877582841_0001_m_000000_0\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: Starting task: attempt_local877582841_0001_r_000000_0\n",
      "15/09/19 17:22:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:22:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@28cccdf4\n",
      "15/09/19 17:22:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/19 17:22:38 INFO reduce.EventFetcher: attempt_local877582841_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/19 17:22:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/19 17:22:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local877582841_0001_m_000000_0 decomp: 59319 len: 59323 to MEMORY\n",
      "15/09/19 17:22:38 INFO reduce.InMemoryMapOutput: Read 59319 bytes from map-output for attempt_local877582841_0001_m_000000_0\n",
      "15/09/19 17:22:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 59319, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->59319\n",
      "15/09/19 17:22:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/19 17:22:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/19 17:22:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:22:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 59315 bytes\n",
      "15/09/19 17:22:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 59319 bytes to disk to satisfy reduce memory limit\n",
      "15/09/19 17:22:39 INFO reduce.MergeManagerImpl: Merging 1 files, 59323 bytes from disk\n",
      "15/09/19 17:22:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/19 17:22:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:22:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 59315 bytes\n",
      "15/09/19 17:22:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer1.py]\n",
      "15/09/19 17:22:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/19 17:22:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: Records R/W=5065/1\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:22:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:22:40 INFO mapred.Task: Task:attempt_local877582841_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:22:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:22:40 INFO mapred.Task: Task attempt_local877582841_0001_r_000000_0 is allowed to commit now\n",
      "15/09/19 17:22:40 INFO output.FileOutputCommitter: Saved output of task 'attempt_local877582841_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/dictionary/_temporary/0/task_local877582841_0001_r_000000\n",
      "15/09/19 17:22:40 INFO mapred.LocalJobRunner: Records R/W=5065/1 > reduce\n",
      "15/09/19 17:22:40 INFO mapred.Task: Task 'attempt_local877582841_0001_r_000000_0' done.\n",
      "15/09/19 17:22:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local877582841_0001_r_000000_0\n",
      "15/09/19 17:22:40 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/19 17:22:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/19 17:22:40 INFO mapreduce.Job: Job job_local877582841_0001 completed successfully\n",
      "15/09/19 17:22:40 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=328974\n",
      "\t\tFILE: Number of bytes written=898147\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=39058\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=5065\n",
      "\t\tMap output bytes=49187\n",
      "\t\tMap output materialized bytes=59323\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=59323\n",
      "\t\tReduce input records=5065\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=10130\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=202\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=39058\n",
      "15/09/19 17:22:40 INFO streaming.StreamJob: Output directory: dictionary\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "    # Forcing number of reducers to be 1\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -mapper mapper1.py -reducer reducer1.py -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Move output to local\n",
    "# Delete it from local if a previous version exists\n",
    "!rm ~/Downloads/HW2/dictionary.txt\n",
    "!hdfs dfs -copyToLocal dictionary/part-00000 ~/Downloads/HW2/dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionay we've created have the following words on it:\n",
    "\n",
    "`a ab abidjan ability able abn about above absent absenteeism absolute absolutely absorb abuse abused acce accelerate accelerated accept acceptable accepted accepting accepts access accomodate accomodates accompanied according accordingly account accountability accounting accounts accrual accurate aches achieve achieved acid acquire acquisition acrobaat acrobat across act action activate active activists activities actor actress actual actually ad adage adams adapted add added adding addition additional additionally address addressed addresses addressing addtional adequately adhesion adm admin adminder administration admitted admixture adobe adobee adolescent adr adrianbold ads adult adv advance advanced advantage ...`\n",
    "\n",
    "The second stage is a MapReduce job that applies the vocabulary to build the classifier. A parameter (`min_occurr`) is used to drop those words with a frequence less than that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:22:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put -f ~/Downloads/HW2/dictionary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "f = open('dictionary', 'r')\n",
    "word_dict = []\n",
    "for line in f:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        word_dict.append(word)\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    content = re.sub('[^a-z]', ' ', content.lower())\n",
    "    words = content.split() # extract words\n",
    "    for word in set(word_dict):\n",
    "        print word + '\\t' + str(words.count(word)) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "import sys\n",
    "import os\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "min_occurr = env_vars['min_occurr']\n",
    "\n",
    "word = []\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is ONE of the words we're counting\n",
    "    if len(word) != 0:\n",
    "        if word[-1] != key_values[0]:\n",
    "            word.append(key_values[0])\n",
    "    else:\n",
    "        word.append(key_values[0])\n",
    "    word_count.append(int(key_values[1]))\n",
    "    # ID and TRUTH are replicated for each word in the vocabulary\n",
    "        # so we just need to keep track of them once\n",
    "    if len(word) == 1:\n",
    "        ID.append(key_values[2])\n",
    "        TRUTH.append(int(key_values[3]))\n",
    "\n",
    "# The lists above will have a length equal to\n",
    "    # number_different_words_in_vocab * number_emails_in_dataset\n",
    "vocab_size = len(word)\n",
    "num_emails = len(TRUTH)\n",
    "# Reshape the list word_count into a 2-D numpy array\n",
    "word_count = np.array(word_count).reshape(len(word), num_emails)\n",
    "# Drop words with a frequency of less than min_ocur\n",
    "condition = np.sum(word_count,1) >= int(min_occurr)\n",
    "final_word_count = word_count[condition, :]\n",
    "filtered_indices = np.extract(condition, word_count).tolist()\n",
    "final_word = [word[i] for i in filtered_indices]\n",
    "final_vocab_size = len(final_word)\n",
    "                        \n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "total_word_spam = [0]*len(final_word)\n",
    "total_word_ham = [0]*len(final_word)\n",
    "for i,w in enumerate(final_word):\n",
    "    # Total count of word w in spam emails\n",
    "    total_word_spam[i] = sum([x*y for (x,y) in zip(TRUTH,final_word_count[i])]) \n",
    "    # Total count of word w in ham emails\n",
    "    total_word_ham[i] = sum(final_word_count[i]) - total_word_spam[i]\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham)+final_vocab_size) for x \\\n",
    "                 in total_word_ham]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam)+final_vocab_size) for \\\n",
    "                  x in total_word_spam]\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*num_emails\n",
    "for i in range(num_emails): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(final_word_count[:,i],prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(final_word_count[:,i],prob_word_spam)])\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS[i] = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:23:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:23:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r allNBEnron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try with `min_occurr=1`, i.e., using all words, regardless of their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:23:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:23:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/19 17:23:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/19 17:23:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/19 17:23:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/19 17:23:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/19 17:23:13 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/19 17:23:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1293880643_0001\n",
      "15/09/19 17:23:15 INFO mapred.LocalDistributedCacheManager: Creating symlink: /app/hadoop/tmp/mapred/local/1442708594420/dictionary.txt <- /home/hduser/Downloads/HW2/dictionary\n",
      "15/09/19 17:23:15 INFO mapred.LocalDistributedCacheManager: Localized file:/home/hduser/Downloads/HW2/dictionary.txt as file:/app/hadoop/tmp/mapred/local/1442708594420/dictionary.txt\n",
      "15/09/19 17:23:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/19 17:23:15 INFO mapreduce.Job: Running job: job_local1293880643_0001\n",
      "15/09/19 17:23:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/19 17:23:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/19 17:23:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/19 17:23:16 INFO mapred.LocalJobRunner: Starting task: attempt_local1293880643_0001_m_000000_0\n",
      "15/09/19 17:23:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/19 17:23:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/19 17:23:16 INFO mapreduce.Job: Job job_local1293880643_0001 running in uber mode : false\n",
      "15/09/19 17:23:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/19 17:23:16 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper2.py]\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/19 17:23:17 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/19 17:23:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:17 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/19 17:23:19 INFO streaming.PipeMapRed: R/W/S=100/187982/0 in:50=100/2 [rec/s] out:93991=187982/2 [rec/s]\n",
      "15/09/19 17:23:22 INFO mapred.LocalJobRunner: Records R/W=72/1 > map\n",
      "15/09/19 17:23:22 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/19 17:23:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:23:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:23:23 INFO mapred.LocalJobRunner: Records R/W=72/1 > map\n",
      "15/09/19 17:23:23 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/19 17:23:23 INFO mapred.MapTask: Spilling map output\n",
      "15/09/19 17:23:23 INFO mapred.MapTask: bufstart = 0; bufend = 17439754; bufvoid = 104857600\n",
      "15/09/19 17:23:23 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24188400(96753600); length = 2025997/6553600\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: Records R/W=72/1 > sort\n",
      "15/09/19 17:23:25 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/19 17:23:25 INFO mapred.Task: Task:attempt_local1293880643_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/19 17:23:25 INFO mapred.Task: Task 'attempt_local1293880643_0001_m_000000_0' done.\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1293880643_0001_m_000000_0\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/19 17:23:25 INFO mapred.LocalJobRunner: Starting task: attempt_local1293880643_0001_r_000000_0\n",
      "15/09/19 17:23:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:23:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@21788153\n",
      "15/09/19 17:23:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/19 17:23:25 INFO reduce.EventFetcher: attempt_local1293880643_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/19 17:23:25 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1293880643_0001_m_000000_0 decomp: 18452756 len: 18452760 to MEMORY\n",
      "15/09/19 17:23:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/19 17:23:26 INFO reduce.InMemoryMapOutput: Read 18452756 bytes from map-output for attempt_local1293880643_0001_m_000000_0\n",
      "15/09/19 17:23:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18452756, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18452756\n",
      "15/09/19 17:23:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/19 17:23:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:23:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/19 17:23:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:23:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/19 17:23:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 18452756 bytes to disk to satisfy reduce memory limit\n",
      "15/09/19 17:23:27 INFO reduce.MergeManagerImpl: Merging 1 files, 18452760 bytes from disk\n",
      "15/09/19 17:23:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/19 17:23:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:23:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/19 17:23:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:23:27 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer2.py]\n",
      "15/09/19 17:23:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/19 17:23:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/19 17:23:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:27 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:28 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:23:28 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/19 17:23:29 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:100000=200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/19 17:23:29 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/19 17:23:30 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/19 17:23:30 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "15/09/19 17:23:31 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/19 17:23:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/19 17:23:34 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/19 17:23:43 INFO streaming.PipeMapRed: Records R/W=506500/1\n",
      "15/09/19 17:23:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:23:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:23:43 INFO mapred.Task: Task:attempt_local1293880643_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:23:43 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/19 17:23:43 INFO mapred.Task: Task attempt_local1293880643_0001_r_000000_0 is allowed to commit now\n",
      "15/09/19 17:23:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1293880643_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/allNBEnron/_temporary/0/task_local1293880643_0001_r_000000\n",
      "15/09/19 17:23:43 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/19 17:23:43 INFO mapred.Task: Task 'attempt_local1293880643_0001_r_000000_0' done.\n",
      "15/09/19 17:23:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local1293880643_0001_r_000000_0\n",
      "15/09/19 17:23:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/19 17:23:44 INFO mapreduce.Job: Job job_local1293880643_0001 completed successfully\n",
      "15/09/19 17:23:44 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37193964\n",
      "\t\tFILE: Number of bytes written=56164240\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2691\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=506500\n",
      "\t\tMap output bytes=17439754\n",
      "\t\tMap output materialized bytes=18452760\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=18452760\n",
      "\t\tReduce input records=506500\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=1013000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=288\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2691\n",
      "15/09/19 17:23:44 INFO streaming.StreamJob: Output directory: allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -files 'dictionary.txt#dictionary' -cmdenv \\\n",
    "    min_occurr=1  -mapper mapper2.py -reducer reducer2.py -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:23:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "Training Error\t0.0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat allNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above (and as expected) the training error is null, so the accuracy at classifying the same dataset used to train the model is perfect. Now let's drop all words with a frequency of less than three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:23:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:23:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:24:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/19 17:24:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/19 17:24:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/19 17:24:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/19 17:24:03 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/19 17:24:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/19 17:24:03 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/19 17:24:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local615146674_0001\n",
      "15/09/19 17:24:05 INFO mapred.LocalDistributedCacheManager: Creating symlink: /app/hadoop/tmp/mapred/local/1442708644967/dictionary.txt <- /home/hduser/Downloads/HW2/dictionary\n",
      "15/09/19 17:24:05 INFO mapred.LocalDistributedCacheManager: Localized file:/home/hduser/Downloads/HW2/dictionary.txt as file:/app/hadoop/tmp/mapred/local/1442708644967/dictionary.txt\n",
      "15/09/19 17:24:06 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/19 17:24:06 INFO mapreduce.Job: Running job: job_local615146674_0001\n",
      "15/09/19 17:24:06 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/19 17:24:06 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/19 17:24:06 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/19 17:24:06 INFO mapred.LocalJobRunner: Starting task: attempt_local615146674_0001_m_000000_0\n",
      "15/09/19 17:24:06 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:24:06 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/19 17:24:06 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/19 17:24:07 INFO mapreduce.Job: Job job_local615146674_0001 running in uber mode : false\n",
      "15/09/19 17:24:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/19 17:24:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/19 17:24:07 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper2.py]\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/19 17:24:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/19 17:24:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:07 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/19 17:24:09 INFO streaming.PipeMapRed: R/W/S=100/187982/0 in:100=100/1 [rec/s] out:187982=187982/1 [rec/s]\n",
      "15/09/19 17:24:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:24:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:24:11 INFO mapred.LocalJobRunner: \n",
      "15/09/19 17:24:11 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/19 17:24:11 INFO mapred.MapTask: Spilling map output\n",
      "15/09/19 17:24:11 INFO mapred.MapTask: bufstart = 0; bufend = 17439754; bufvoid = 104857600\n",
      "15/09/19 17:24:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24188400(96753600); length = 2025997/6553600\n",
      "15/09/19 17:24:12 INFO mapred.LocalJobRunner: Records R/W=72/1 > sort\n",
      "15/09/19 17:24:13 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/19 17:24:13 INFO mapred.Task: Task:attempt_local615146674_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/19 17:24:13 INFO mapred.Task: Task 'attempt_local615146674_0001_m_000000_0' done.\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local615146674_0001_m_000000_0\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: Starting task: attempt_local615146674_0001_r_000000_0\n",
      "15/09/19 17:24:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/19 17:24:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/19 17:24:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@794e1b66\n",
      "15/09/19 17:24:13 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/19 17:24:13 INFO reduce.EventFetcher: attempt_local615146674_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/19 17:24:13 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local615146674_0001_m_000000_0 decomp: 18452756 len: 18452760 to MEMORY\n",
      "15/09/19 17:24:13 INFO reduce.InMemoryMapOutput: Read 18452756 bytes from map-output for attempt_local615146674_0001_m_000000_0\n",
      "15/09/19 17:24:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18452756, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18452756\n",
      "15/09/19 17:24:13 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/19 17:24:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:24:13 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/19 17:24:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:24:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/19 17:24:14 INFO reduce.MergeManagerImpl: Merged 1 segments, 18452756 bytes to disk to satisfy reduce memory limit\n",
      "15/09/19 17:24:14 INFO reduce.MergeManagerImpl: Merging 1 files, 18452760 bytes from disk\n",
      "15/09/19 17:24:14 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/19 17:24:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/19 17:24:14 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/19 17:24:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/19 17:24:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer2.py]\n",
      "15/09/19 17:24:14 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/19 17:24:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/19 17:24:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/19 17:24:16 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/19 17:24:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/19 17:24:16 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/19 17:24:17 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/19 17:24:17 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "15/09/19 17:24:19 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/19 17:24:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/19 17:24:21 INFO streaming.PipeMapRed: Records R/W=506500/1\n",
      "15/09/19 17:24:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/19 17:24:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/19 17:24:21 INFO mapred.Task: Task:attempt_local615146674_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/19 17:24:21 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/19 17:24:21 INFO mapred.Task: Task attempt_local615146674_0001_r_000000_0 is allowed to commit now\n",
      "15/09/19 17:24:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local615146674_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/allNBEnron/_temporary/0/task_local615146674_0001_r_000000\n",
      "15/09/19 17:24:21 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/19 17:24:21 INFO mapred.Task: Task 'attempt_local615146674_0001_r_000000_0' done.\n",
      "15/09/19 17:24:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local615146674_0001_r_000000_0\n",
      "15/09/19 17:24:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/19 17:24:22 INFO mapreduce.Job: Job job_local615146674_0001 completed successfully\n",
      "15/09/19 17:24:22 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37193964\n",
      "\t\tFILE: Number of bytes written=56161468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2692\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=506500\n",
      "\t\tMap output bytes=17439754\n",
      "\t\tMap output materialized bytes=18452760\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=18452760\n",
      "\t\tReduce input records=506500\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=1013000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=161\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2692\n",
      "15/09/19 17:24:22 INFO streaming.StreamJob: Output directory: allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -files 'dictionary.txt#dictionary' -cmdenv \\\n",
    "    min_occurr=3  -mapper mapper2.py -reducer reducer2.py -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/19 17:24:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t1\n",
      "Training Error\t0.01\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat allNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping very infrequent words causes the training error to be just slightly higher (1% instead of 0%). If we try higher values of `min_occurr` (dropping words unless they are quite frequent), the training error keeps increasing, though at a very low rate.\n",
    "\n",
    "At least in a real case (measuring accuracy with a test set rather than the training set), it makes more sense to use very frequent words as **stopwords** (i.e., to drop words that appear more than $n$ times), because they're likely to be present in both types of emails (spam and ham), and hence do not characterize the kind of email we're trying to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "15/12/09 15:22:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "15/12/09 15:22:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/stop-yarn.sh\n",
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 03\n",
    "- Submission date: 9/22/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Errata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will upload any **minor corrections** I may make to the assignment after I submit it:\n",
    "\n",
    "[https://www.dropbox.com/s/1ftdlfydrcd7exn/HW3-Errata.txt?dl=0](https://www.dropbox.com/s/1ftdlfydrcd7exn/HW3-Errata.txt?dl=0)\n",
    "\n",
    "Of course not changes in the code or so, but things like this (I forgot to include the link to the *Errata* in HW2):\n",
    "\n",
    "> I noticed (after submitting HW2) that at the end of HW2.1 I wrote:\n",
    "><pre>!hadoop jar hadoop‐streaming*.jar ‐D mapred.reduce.tasks=1 ‐mapper mapper.py ‐reducer reducer.py ‐input /user/hadoop/dirhw21/input ‐output sortOutput</pre>\n",
    "to explain what would happen if we used multiple reducers.\n",
    "\n",
    ">That command was the one that I had actually used before (in cell 8); what I meant to say was:\n",
    "<pre>... mapred.reduce.tasks=2...</pre>\n",
    "which was what I actually ran to obtain 2 (unsorted) outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My solution to HW2.5 (dropped from HW2 submission) can be found at:\n",
    "\n",
    "[http://nbviewer.ipython.org/urls/dl.dropbox.com/s/vz557r7tvj4dk3z/MIDS-W261-2015-HWK-Week02-Carin-HW25.ipynb](http://nbviewer.ipython.org/urls/dl.dropbox.com/s/vz557r7tvj4dk3z/MIDS-W261-2015-HWK-Week02-Carin-HW25.ipynb)\n",
    "\n",
    "Or alternatively:\n",
    "\n",
    "[https://gist.github.com/juanjocarin/2b9ad03519cfa1135dd8](https://gist.github.com/juanjocarin/2b9ad03519cfa1135dd8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is a merge sort? Where is it used in Hadoop?**\n",
    "\n",
    "2. **How is a combiner function in the context of Hadoop?**\n",
    "\n",
    "3. **Give an example where it can be used and justify why it should be used in the context of this problem.**\n",
    "\n",
    "4. **What is the Hadoop shuffle?**\n",
    "\n",
    "5. **What is the Apriori algorithm? Describe an example use in your domain of expertise. Define confidence and lift.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A **merge sort** is a sorting algorithm based on a *divide and conquer* approach. First an unsorted list of 1 element is divided into $n$ subsets of 1 element each (and hence sorted), and then those subsets are repeatedly merged (in a sorted way); this algorithm is much more efficient ($\\mathcal{O}(n\\log{n})$ than *selection sorting* ($\\mathcal{O}(n^2)$).\n",
    "    It is performed by the **execution framework** of Hadoop (more specifically, by the **Hadoop Shuffle**).\n",
    "\n",
    "2. A **combiner** function in the context of Hadoop is pretty much like a (mini) reducer: its input and output must be the intermediate key-value pairs emitted by mappers and accepted by reducers (this does not necessarily mean that a reducer can be used as a combiner; the operation it performs must be both associative and commutative to be able to use them interchangeably). Combiners allow for local aggregation before the shuffle-and-sort phase, hence allowing more efficient implementations (for example, by reducing the number of key-value pairs that must be sent to the reducers).\n",
    "\n",
    "3. For example, combiners could be used in a **word count** algorithm: instead of sending to the reducers one key-value pair for every occurrence of a particular word, the use of combiners would aggregate all key-value pairs generated by a mapper corresponding to the same key (i.e., word in this example; imagine, for instance, all the occurrences of the word \"*the*\" in a sentence, and even in all sentences handled by a single mapper), hence dramatically reducing network traffic.\n",
    "\n",
    "4. The **Hadoop Shuffle** can be seen as the \"heart and soul\" of the MapReduce framework, where the \"magic\" happens: it is the process by which the system performs the sort and transfer the map outputs to the reducers. More specifically, where data are partitioned, and records within each partition are sorted, merged, and combined (all in memory instead of disk, whenever possible).\n",
    "\n",
    "5. The **Apriori algorithm** is an algorithm for frequent itemset mining and association rule discovery over transactional databases. By identifying frequent itemsets in the transactions (or *baskets*), it allows to detect association (`if-then`) rules, which highlight general trends in the transactional base (i.e., if an itemset $I$ is frequent, $I \\rightarrow j$ would mean that the item $j$ is likely to appear in the transactions that involve $I$?).\n",
    "\n",
    "    For example, in the energy industry, companies that need and purchase [power analyzers](http://tmi.yokogawa.com/us/products/digital-power-analyzers/digital-power-analyzers/) for high-accuracy power measurement and [oscilloscopes](http://tmi.yokogawa.com/us/products/oscilloscopes/digital-and-mixed-signal-oscilloscopes/) for high frequency measurement, are likely to need [instruments that combine both features, as well as a long-term memory](http://tmi.yokogawa.com/us/products/oscilloscopes/scopecorders/).\n",
    "\n",
    "    The **confidence** or an association rule is the ratio of the *support* (the number of occurrences) for $I \\cup j$ to the support for $I$. So it measures the certainty or trustworthiness associated with the association rule. The **lift** is a measure of the performance of the association rule at predicting or classifying cases as having an enhanced response (with response to the population as a whole): it is the ratio of the target response to the average response. Let's say that there are $200$ transactions per day in a store: $120$ of the $200$ customers buy beer, $40$ buy diapers, and $37$ of the customers that buy diapers also buy beer. The confidence of the association rule $\\text{diapers} \\rightarrow \\text{beer}$ would be $\\frac{37}{40}=92.5\\%$ (another way to calculate it, based on relative frequencies: $\\Pr(\\text{beer}\\mid\\text{diapers})=\\frac{\\Pr(\\text{beer,diapers)}}{\\Pr(\\text{diapers})}=\\frac{0.185}{0.200}=0.925$); so the mentioned association rule would be very interesting, because the support is relatively high ($18.5\\%$, the proportion of transactions that involve both beer and diapers) and the confidence is also high, close to one. The lift in this example would be $\\frac{92.5\\%}{60\\%}=1.542$; $60\\%$ of the customers ($120/200$) purchase beer, but the proportion goes up to $92.5\\%$ ($37/40$) if we focus only on those who buy diapers. Hence, it is worth exploring this association rule—i.e., that segment of the customers (had the lift been close to one, that would have meant that both proportions are equal, implying independence of both products); actually the \"**intestest**\" of an association rule is defined the same way as the lift, but in absolute instead of relative units (in this case, $32.5\\%$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Product Recommendations: The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.**\n",
    "\n",
    "**Use the online browsing behavior dataset at: [https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0](https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0). Each line in this dataset represents a browsing session of a customer. On each line, each string of 8 characters represents the id of an item browsed during that session. The items are separated by spaces.**\n",
    "\n",
    "**Do some exploratory data analysis of this dataset. Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some basic exploration of the input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31101 baskets\r\n",
      "380824 items in total\r\n",
      "12.2448 items per baket, on average\r\n"
     ]
    }
   ],
   "source": [
    "!wc -lw ProductPurchaseData.txt | awk '{print $1 \" baskets\";\\\n",
    "    print $2 \" items in total\" ;print $2/$1 \" items per baket, on average\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-juanjo-VB.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-juanjo-VB.out\n",
      "15/09/22 16:18:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-juanjo-VB.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-juanjo-VB.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-juanjo-VB.out\n",
      "15/09/22 16:19:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Hadoop\n",
    "!/usr/local/hadoop/sbin/start-yarn.sh\n",
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:19:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Create new folder\n",
    "!hdfs dfs -mkdir -p /user/hadoop/dirhw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:19:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: Cannot create file/user/hadoop/dirhw3/ProductPurchaseData.txt._COPYING_. Name node is in safe mode.\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f ProductPurchaseData.txt /user/hadoop/dirhw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    " # Create a dicitionary with unique products as keys\n",
    "unique_product = {}\n",
    "# The values associated with each key are\n",
    "    # largest basket that product appears in\n",
    "    # occurrences of the product\n",
    "    \n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    basket = line.split() # each line is a basket\n",
    "    basket_size = len(basket) # its size is the number of products in it\n",
    "    for product in basket:\n",
    "        # If new product is found, add to dictionary\n",
    "        if product not in unique_product.keys():\n",
    "            # Value for each key is a duple\n",
    "            unique_product[product] = [basket_size, 1]\n",
    "                # The first time it appears, the largest basket size is \n",
    "                    # the current one\n",
    "                # and the number of occurrences is 1 (a product appears \n",
    "                    # only once per basket\n",
    "        else: # if already included\n",
    "            # Keep the largest basket (MAX)\n",
    "            unique_product[product][0] = max(unique_product[product][0], \n",
    "                                             basket_size)\n",
    "            # Increment number of occurrences of that product (SUM)\n",
    "            unique_product[product][1] += 1\n",
    "\n",
    "# For each unique product/item, print:\n",
    "# ID TAB largest basket it appears in TAB number of baskets it appears in\n",
    "for product in unique_product.keys():\n",
    "    print '{0}\\t{1}'.format(product,'\\t'.join(map(str, \n",
    "                                                  unique_product[product])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the output of a mapper has the following form (`DAI11153` appears in 8 baskets, and the largest one (out of those 8) contains 24 products:\n",
    "\n",
    "`DAI11153\t24\t8\n",
    "DAI11261\t29\t6\n",
    "DAI11273\t12\t1\n",
    "DAI11375\t12\t1\n",
    "DAI11462\t22\t8\n",
    "DAI11552\t31\t8\n",
    "DAI11555\t23\t25\n",
    "...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_product = None\n",
    "largest_basket_size = 0\n",
    "sum_occurrences = 0\n",
    "num_unique_products = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin: # each line corresponds to a product\n",
    "    # They key is the product/item id\n",
    "    # The first value is the largest basket size that id appears in\n",
    "    # The second value is the number of baskets that id appears in\n",
    "    product, basket_size, occurrences = line.split('\\t', 2)\n",
    "    # If product is still the same, update\n",
    "    if current_product == product:\n",
    "        sum_occurrences += int(occurrences) # though we don't need to report it\n",
    "        largest_basket_size = max(largest_basket_size, int(basket_size))\n",
    "    # Otherwise (if new product is found) add it\n",
    "    else:\n",
    "        num_unique_products += 1\n",
    "        current_product = product\n",
    "        sum_occurrences = int(occurrences) # though we don't need to report it\n",
    "        largest_basket_size = max(largest_basket_size, int(basket_size))\n",
    "            # Also consider previous cases, because we have to report the\n",
    "                # largest basket for the whole dataset, not for each particular\n",
    "                # product\n",
    "\n",
    "# Output results\n",
    "print '--------------------------------------------'\n",
    "print 'Number of unique products:             {0}'.format(num_unique_products)\n",
    "print 'Number of items in the largest basket: {0}'.format(largest_basket_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few changes in the reducer code, we can build a combiner that performs almost the same job (but producting same intermediate key-value pairs instead of the overall results (count of unique products and size of the largest basket):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_product = None\n",
    "largest_basket_size = 0\n",
    "sum_occurrences = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin: # each line corresponds to a product\n",
    "    # They key is the product/item id\n",
    "    # The first value is the largest basket size that id appears in\n",
    "    # The second value is the number of baskets that id appears in\n",
    "    product, basket_size, occurrences = line.split('\\t', 2)\n",
    "    # If product is still the same, update\n",
    "    if current_product == product:\n",
    "        sum_occurrences += int(occurrences)\n",
    "        largest_basket_size = max(largest_basket_size, int(basket_size))\n",
    "    # Otherwise (if new product is found)\n",
    "    else:\n",
    "        # Print results from previous product\n",
    "        if current_product:\n",
    "            print '{0}\\t{1}\\t{2}'.format(current_product, largest_basket_size, \n",
    "                                         sum_occurrences)\n",
    "        current_product = product\n",
    "        sum_occurrences = int(occurrences)\n",
    "        largest_basket_size = int(basket_size)\n",
    "\n",
    "# Print results of last product found\n",
    "if current_product == product:\n",
    "    print '{0}\\t{1}\\t{2}'.format(current_product, largest_basket_size, \n",
    "                                 sum_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:20:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:20:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted countProduct\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r countProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:20:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:20:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 16:20:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 16:20:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 16:20:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 16:20:52 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 16:20:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/22 16:20:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1884120764_0001\n",
      "15/09/22 16:20:55 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 16:20:55 INFO mapreduce.Job: Running job: job_local1884120764_0001\n",
      "15/09/22 16:20:55 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 16:20:55 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 16:20:55 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 16:20:55 INFO mapred.LocalJobRunner: Starting task: attempt_local1884120764_0001_m_000000_0\n",
      "15/09/22 16:20:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 16:20:55 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 16:20:55 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./mapper.py]\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 16:20:55 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 16:20:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 16:20:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 16:20:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 16:20:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 16:20:56 INFO mapreduce.Job: Job job_local1884120764_0001 running in uber mode : false\n",
      "15/09/22 16:20:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 16:20:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:20:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:20:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:20:56 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:21:01 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:02 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "15/09/22 16:21:04 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:05 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "15/09/22 16:21:07 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:08 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "15/09/22 16:21:10 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:11 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "15/09/22 16:21:13 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:14 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "15/09/22 16:21:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:500=10000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "15/09/22 16:21:16 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:17 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "15/09/22 16:21:19 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:25 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:26 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "15/09/22 16:21:28 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:34 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:35 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "15/09/22 16:21:37 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:43 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:44 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "15/09/22 16:21:49 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:50 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "15/09/22 16:21:55 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:21:56 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "15/09/22 16:22:04 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:05 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "15/09/22 16:22:10 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:11 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "15/09/22 16:22:16 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:17 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "15/09/22 16:22:22 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:23 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "15/09/22 16:22:28 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:29 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "15/09/22 16:22:34 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:35 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "15/09/22 16:22:40 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:41 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "15/09/22 16:22:46 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:47 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "15/09/22 16:22:52 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:53 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "15/09/22 16:22:58 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:22:59 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/22 16:22:59 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "15/09/22 16:22:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:23:00 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:23:00 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 16:23:00 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 16:23:00 INFO mapred.MapTask: bufstart = 0; bufend = 178740; bufvoid = 104857600\n",
      "15/09/22 16:23:00 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164032(104656128); length = 50365/6553600\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./combiner.py]\n",
      "15/09/22 16:23:00 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: Records R/W=9235/1\n",
      "15/09/22 16:23:00 INFO streaming.PipeMapRed: R/W/S=10000/888/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:23:01 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 16:23:01 INFO mapred.Task: Task:attempt_local1884120764_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: Records R/W=9235/1\n",
      "15/09/22 16:23:01 INFO mapred.Task: Task 'attempt_local1884120764_0001_m_000000_0' done.\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1884120764_0001_m_000000_0\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1884120764_0001_r_000000_0\n",
      "15/09/22 16:23:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:23:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@709a2ad4\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 16:23:01 INFO reduce.EventFetcher: attempt_local1884120764_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 16:23:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1884120764_0001_m_000000_0 decomp: 203926 len: 203930 to MEMORY\n",
      "15/09/22 16:23:01 INFO reduce.InMemoryMapOutput: Read 203926 bytes from map-output for attempt_local1884120764_0001_m_000000_0\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 203926, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->203926\n",
      "15/09/22 16:23:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 16:23:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:23:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 203915 bytes\n",
      "15/09/22 16:23:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 203926 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: Merging 1 files, 203930 bytes from disk\n",
      "15/09/22 16:23:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 16:23:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:23:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 203915 bytes\n",
      "15/09/22 16:23:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./reducer.py]\n",
      "15/09/22 16:23:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 16:23:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:01 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:02 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:23:02 INFO streaming.PipeMapRed: Records R/W=12592/1\n",
      "15/09/22 16:23:02 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:23:02 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:23:03 INFO mapred.Task: Task:attempt_local1884120764_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:23:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:23:03 INFO mapred.Task: Task attempt_local1884120764_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 16:23:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1884120764_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/countProduct/_temporary/0/task_local1884120764_0001_r_000000\n",
      "15/09/22 16:23:03 INFO mapred.LocalJobRunner: Records R/W=12592/1 > reduce\n",
      "15/09/22 16:23:03 INFO mapred.Task: Task 'attempt_local1884120764_0001_r_000000_0' done.\n",
      "15/09/22 16:23:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local1884120764_0001_r_000000_0\n",
      "15/09/22 16:23:03 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 16:23:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 16:23:03 INFO mapreduce.Job: Job job_local1884120764_0001 completed successfully\n",
      "15/09/22 16:23:03 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=618198\n",
      "\t\tFILE: Number of bytes written=1336282\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=135\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=178740\n",
      "\t\tMap output materialized bytes=203930\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=12592\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=203930\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=97\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=135\n",
      "15/09/22 16:23:03 INFO streaming.StreamJob: Output directory: countProduct\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -D mapred.reduce.tasks=1 -mapper mapper.py -combiner combiner.py\\\n",
    "    -reducer reducer.py -input /user/hadoop/dirhw3/ProductPurchaseData.txt \\\n",
    "    -output countProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:23:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "--------------------------------------------\t\n",
      "Number of unique products:             12592\t\n",
      "Number of items in the largest basket: 37\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat countProduct/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option, if we want to completely re-use the reducer as a combiner (or vice versa), would be to keep the whole list of key-value pairs (i.e., the whole list of unique products, together with the number of baskets they appear in, and the maximum size of those baskets) and extract the required information (using command lines) from the output. This is what I've done in the following 2 cells (note that `combiner.py`, as defined above, is now also used as reducer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:24:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:24:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted countProduct\n",
      "15/09/22 16:24:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:24:21 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 16:24:21 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 16:24:22 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 16:24:22 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 16:24:22 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 16:24:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/22 16:24:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local335656187_0001\n",
      "15/09/22 16:24:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 16:24:24 INFO mapreduce.Job: Running job: job_local335656187_0001\n",
      "15/09/22 16:24:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 16:24:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 16:24:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 16:24:24 INFO mapred.LocalJobRunner: Starting task: attempt_local335656187_0001_m_000000_0\n",
      "15/09/22 16:24:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 16:24:25 INFO mapreduce.Job: Job job_local335656187_0001 running in uber mode : false\n",
      "15/09/22 16:24:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 16:24:25 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 16:24:25 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./mapper.py]\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 16:24:26 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 16:24:26 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:24:26 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:24:26 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:24:26 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:24:31 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:31 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "15/09/22 16:24:34 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:34 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "15/09/22 16:24:37 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:37 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "15/09/22 16:24:40 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:40 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "15/09/22 16:24:43 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:43 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "15/09/22 16:24:46 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:46 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:500=10000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "15/09/22 16:24:52 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:52 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "15/09/22 16:24:58 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:24:58 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "15/09/22 16:25:04 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:04 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "15/09/22 16:25:10 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:10 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "15/09/22 16:25:13 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:19 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:19 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "15/09/22 16:25:25 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:25 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "15/09/22 16:25:31 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:31 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "15/09/22 16:25:37 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:37 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "15/09/22 16:25:43 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:43 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "15/09/22 16:25:52 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:52 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "15/09/22 16:25:58 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:25:58 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "15/09/22 16:26:04 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:04 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "15/09/22 16:26:10 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:10 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "15/09/22 16:26:16 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:16 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "15/09/22 16:26:22 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:22 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "15/09/22 16:26:28 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:28 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "15/09/22 16:26:37 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:38 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/22 16:26:44 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "15/09/22 16:26:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:26:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:26:45 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:26:45 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 16:26:45 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 16:26:45 INFO mapred.MapTask: bufstart = 0; bufend = 178740; bufvoid = 104857600\n",
      "15/09/22 16:26:45 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164032(104656128); length = 50365/6553600\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./combiner.py]\n",
      "15/09/22 16:26:46 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: Records R/W=9235/1\n",
      "15/09/22 16:26:46 INFO streaming.PipeMapRed: R/W/S=10000/815/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:26:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:26:47 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 16:26:47 INFO mapred.Task: Task:attempt_local335656187_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: Records R/W=9235/1\n",
      "15/09/22 16:26:47 INFO mapred.Task: Task 'attempt_local335656187_0001_m_000000_0' done.\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local335656187_0001_m_000000_0\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: Starting task: attempt_local335656187_0001_r_000000_0\n",
      "15/09/22 16:26:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:26:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64020797\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 16:26:47 INFO reduce.EventFetcher: attempt_local335656187_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 16:26:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local335656187_0001_m_000000_0 decomp: 203926 len: 203930 to MEMORY\n",
      "15/09/22 16:26:47 INFO reduce.InMemoryMapOutput: Read 203926 bytes from map-output for attempt_local335656187_0001_m_000000_0\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 203926, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->203926\n",
      "15/09/22 16:26:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 16:26:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:26:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 203915 bytes\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: Merged 1 segments, 203926 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: Merging 1 files, 203930 bytes from disk\n",
      "15/09/22 16:26:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 16:26:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:26:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 203915 bytes\n",
      "15/09/22 16:26:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:26:47 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./combiner.py]\n",
      "15/09/22 16:26:47 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 16:26:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 16:26:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 16:26:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:26:48 INFO streaming.PipeMapRed: Records R/W=9235/1\n",
      "15/09/22 16:26:49 INFO streaming.PipeMapRed: R/W/S=10000/965/0 in:10000=10000/1 [rec/s] out:965=965/1 [rec/s]\n",
      "15/09/22 16:26:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:26:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:26:49 INFO mapred.Task: Task:attempt_local335656187_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:26:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:26:49 INFO mapred.Task: Task attempt_local335656187_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 16:26:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local335656187_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/countProduct/_temporary/0/task_local335656187_0001_r_000000\n",
      "15/09/22 16:26:49 INFO mapred.LocalJobRunner: Records R/W=9235/1 > reduce\n",
      "15/09/22 16:26:49 INFO mapred.Task: Task 'attempt_local335656187_0001_r_000000_0' done.\n",
      "15/09/22 16:26:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local335656187_0001_r_000000_0\n",
      "15/09/22 16:26:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 16:26:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 16:26:50 INFO mapreduce.Job: Job job_local335656187_0001 completed successfully\n",
      "15/09/22 16:26:50 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=618198\n",
      "\t\tFILE: Number of bytes written=1333518\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=178740\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=178740\n",
      "\t\tMap output materialized bytes=203930\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=12592\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=203930\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178740\n",
      "15/09/22 16:26:50 INFO streaming.StreamJob: Output directory: countProduct\n",
      "15/09/22 16:26:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output\n",
    "!hdfs dfs -rm -r countProduct\n",
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -D mapred.reduce.tasks=1 -mapper mapper.py -combiner combiner.py\\\n",
    "    -reducer combiner.py -input /user/hadoop/dirhw3/ProductPurchaseData.txt \\\n",
    "    -output countProduct\n",
    "# Copy to local\n",
    "!hdfs dfs -cat countProduct/part-00000 > countProduct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the output of MapReduce has the same format of the intermediate output of the mappers, i.e.\n",
    "\n",
    "`DAI11153\t24\t8\n",
    "DAI11223\t34\t155\n",
    "DAI11238\t29\t3\n",
    "DAI11257\t13\t1\n",
    "DAI11261\t29\t6\n",
    "DAI11273\t12\t1\n",
    "...`\n",
    "\n",
    "So we just have to count the number of lines in this (single) output, and the maximum of the values in the 2nd column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Number of unique products:             12592\n",
      "Number of items in the largest basket: 37\n"
     ]
    }
   ],
   "source": [
    "!echo \"--------------------------------------------\"\n",
    "!echo \"Number of unique products:             \"$(cat countProduct | wc -l)\n",
    "!echo \"Number of items in the largest basket: \"$(cat countProduct | cut -f 2 \\\n",
    "                                                  | awk 'BEGIN {max = 0} \\\n",
    "                                                  {if ($1>max) max=$1} END \\\n",
    "                                                  {print max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.**\n",
    "\n",
    "**List the top 5 rules with corresponding confidence scores in decreasing order of confidence score for frequent (100>count) itemsets of size 2. A rule is of the form:**\n",
    "\n",
    "    (item1) ⇒ item2\n",
    "\n",
    "**Fix the ordering of the rule lexicographically (left to right), and break ties in confidence (between rules, if any exist) by taking the first ones in lexicographically increasing order. Use Hadoop MapReduce to complete this part of the assignment; use a single mapper and single reducer; use a combiner if you think it will help and justify.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "# Create a dictionary with item1 as keys\n",
    "primary_keys={}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    basket = line.split() # each line is a basket\n",
    "    # For each product in the basket\n",
    "    for item1 in basket:\n",
    "        # Create the key if found new product\n",
    "        if item1 not in primary_keys.keys():\n",
    "            # The value will be another dictionary\n",
    "            primary_keys[item1] = collections.Counter()\n",
    "        # Increase the number of occurrences\n",
    "        primary_keys[item1].update(['*'])\n",
    "        # 2nd loop\n",
    "        for item2 in basket:    \n",
    "            # Only for subsequent items in lexicographic order\n",
    "            if item2 > item1:\n",
    "                # Add item2 as a key of the dictionary corresponding to item1\n",
    "                primary_keys[item1].update(item2.split())\n",
    "\n",
    "# For each key (one line per item1 => STRIPES)\n",
    "for k, v in primary_keys.items():\n",
    "    print '%s\\t%s' % (k,dict(collections.OrderedDict(sorted(v.items()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import collections\n",
    "import ast\n",
    "\n",
    "# Create a dictionary with item1 as keys\n",
    "primary_keys={}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    itemset = line.split('\\t') # each line corresponds to an itemset\n",
    "    item1 = itemset[0] # first part is the item1 (primary key)\n",
    "    # Create the key if found new product\n",
    "    if item1 not in primary_keys.keys():\n",
    "        primary_keys[item1] = collections.Counter()\n",
    "    # Add/Update the dictionary corresponding to that key\n",
    "    primary_keys[item1].update(ast.literal_eval(itemset[1]))\n",
    "    # Since we've used the STRIPES approach we have all the data we need\n",
    "\n",
    "# For each key (one line per item1 => STRIPES)\n",
    "    # Same output as the mappers (but aggregated :D)\n",
    "for k, v in primary_keys.items():\n",
    "    print '%s\\t%s' % (k,dict(collections.OrderedDict(sorted(v.items()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import collections\n",
    "import ast\n",
    "from operator import itemgetter\n",
    "\n",
    "# Create a dictionary with item1 as keys\n",
    "primary_keys={}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    itemset = line.split('\\t') # each line corresponds to an itemset\n",
    "    item1 = itemset[0] # first part is the item1 (primary key)\n",
    "    # Create the key if found new product\n",
    "    if item1 not in primary_keys.keys():\n",
    "        primary_keys[item1] = collections.Counter()\n",
    "    # Add/Update the dictionary corresponding to that key\n",
    "    primary_keys[item1].update(ast.literal_eval(itemset[1]))\n",
    "    # Since we've used the STRIPES approach we have all the data we need\n",
    "\n",
    "frequent_itemsets = []\n",
    "# For each key (one line per item1 => STRIPES)\n",
    "for item1 in primary_keys.keys():\n",
    "    # For each item2\n",
    "    for item2 in primary_keys[item1].keys():\n",
    "        if item2 != '*':\n",
    "            # For itemsets with Support > 100\n",
    "            if float(primary_keys[item1][item2]) > 100:\n",
    "                # Estimate the Confidence\n",
    "                frequent_itemsets.append((item1,item2, \n",
    "                                          float(primary_keys[item1][item2]) / \\\n",
    "                                          primary_keys[item1]['*']))\n",
    "\n",
    "# Sort in ascending order\n",
    "frequent_itemsets.sort(key=itemgetter(2), reverse=True)\n",
    "# And report top 5 association rules (item1 => item2: confidence)\n",
    "top = frequent_itemsets[:5]\n",
    "for itemset in top:\n",
    "    print '({}) => {}: {:.4f}'.format(itemset[0], itemset[1], itemset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:29:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:29:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted APriori\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r APriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:29:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/22 16:29:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/22 16:29:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/22 16:29:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/22 16:29:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/22 16:29:12 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/22 16:29:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/22 16:29:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local382019511_0001\n",
      "15/09/22 16:29:14 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/22 16:29:14 INFO mapreduce.Job: Running job: job_local382019511_0001\n",
      "15/09/22 16:29:14 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/22 16:29:14 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/22 16:29:14 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/22 16:29:14 INFO mapred.LocalJobRunner: Starting task: attempt_local382019511_0001_m_000000_0\n",
      "15/09/22 16:29:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:29:14 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/22 16:29:15 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/22 16:29:15 INFO mapreduce.Job: Job job_local382019511_0001 running in uber mode : false\n",
      "15/09/22 16:29:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/22 16:29:15 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./mapper.py]\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/22 16:29:15 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/22 16:29:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:29:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:29:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:29:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:29:20 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:21 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "15/09/22 16:29:23 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:24 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "15/09/22 16:29:26 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:27 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "15/09/22 16:29:29 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:30 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "15/09/22 16:29:32 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:33 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "15/09/22 16:29:35 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:40 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:400=10000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "15/09/22 16:29:41 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:42 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "15/09/22 16:29:44 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:53 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:29:54 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "15/09/22 16:30:02 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:30:03 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "15/09/22 16:30:14 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:30:15 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "15/09/22 16:30:26 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:30:27 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "15/09/22 16:30:39 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:30:39 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "15/09/22 16:30:51 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:30:51 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "15/09/22 16:31:06 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:31:06 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "15/09/22 16:31:18 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:31:18 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "15/09/22 16:31:27 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:31:27 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "15/09/22 16:31:39 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:31:39 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "15/09/22 16:31:51 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:31:51 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "15/09/22 16:32:03 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:32:03 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "15/09/22 16:32:21 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:32:21 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "15/09/22 16:32:36 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:32:36 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "15/09/22 16:32:48 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:32:48 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "15/09/22 16:33:12 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:33:12 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "15/09/22 16:33:27 INFO mapred.LocalJobRunner: hdfs://localhost:54310/user/hadoop/dirhw3/ProductPurchaseData.txt:0+3458517 > map\n",
      "15/09/22 16:33:28 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/22 16:33:52 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "15/09/22 16:33:57 INFO mapred.LocalJobRunner: Records R/W=31101/1 > map\n",
      "15/09/22 16:34:00 INFO mapred.LocalJobRunner: Records R/W=31101/1 > map\n",
      "15/09/22 16:34:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:34:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:34:01 INFO mapred.LocalJobRunner: Records R/W=31101/1 > map\n",
      "15/09/22 16:34:01 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/22 16:34:01 INFO mapred.MapTask: Spilling map output\n",
      "15/09/22 16:34:01 INFO mapred.MapTask: bufstart = 0; bufend = 13453643; bufvoid = 104857600\n",
      "15/09/22 16:34:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164032(104656128); length = 50365/6553600\n",
      "15/09/22 16:34:02 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./combiner.py]\n",
      "15/09/22 16:34:02 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "15/09/22 16:34:02 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:02 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:02 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:03 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:04 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:1000=1000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 16:34:06 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:09 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:12 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:15 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:18 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:18 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:666=10000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "15/09/22 16:34:21 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:24 INFO mapred.LocalJobRunner: Records R/W=31101/1 > sort\n",
      "15/09/22 16:34:25 INFO streaming.PipeMapRed: Records R/W=12592/1\n",
      "15/09/22 16:34:27 INFO mapred.LocalJobRunner: Records R/W=12592/1 > sort\n",
      "15/09/22 16:34:30 INFO mapred.LocalJobRunner: Records R/W=12592/1 > sort\n",
      "15/09/22 16:34:35 INFO streaming.PipeMapRed: Records R/W=12592/11162\n",
      "15/09/22 16:34:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:34:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:34:37 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/22 16:34:37 INFO mapred.Task: Task:attempt_local382019511_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:34:37 INFO mapred.LocalJobRunner: Records R/W=12592/11162\n",
      "15/09/22 16:34:37 INFO mapred.Task: Task 'attempt_local382019511_0001_m_000000_0' done.\n",
      "15/09/22 16:34:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local382019511_0001_m_000000_0\n",
      "15/09/22 16:34:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/22 16:34:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/22 16:34:37 INFO mapred.LocalJobRunner: Starting task: attempt_local382019511_0001_r_000000_0\n",
      "15/09/22 16:34:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/22 16:34:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c19cee5\n",
      "15/09/22 16:34:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/22 16:34:37 INFO reduce.EventFetcher: attempt_local382019511_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/22 16:34:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local382019511_0001_m_000000_0 decomp: 13479935 len: 13479939 to MEMORY\n",
      "15/09/22 16:34:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/22 16:34:38 INFO reduce.InMemoryMapOutput: Read 13479935 bytes from map-output for attempt_local382019511_0001_m_000000_0\n",
      "15/09/22 16:34:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 13479935, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->13479935\n",
      "15/09/22 16:34:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/22 16:34:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:34:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/22 16:34:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:34:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13479922 bytes\n",
      "15/09/22 16:34:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 13479935 bytes to disk to satisfy reduce memory limit\n",
      "15/09/22 16:34:39 INFO reduce.MergeManagerImpl: Merging 1 files, 13479939 bytes from disk\n",
      "15/09/22 16:34:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/22 16:34:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/22 16:34:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13479922 bytes\n",
      "15/09/22 16:34:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/22 16:34:39 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW3/./reducer.py]\n",
      "15/09/22 16:34:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/22 16:34:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/22 16:34:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/22 16:34:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:1000=1000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/22 16:34:43 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:44 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "15/09/22 16:34:46 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:47 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "15/09/22 16:34:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:50 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "15/09/22 16:34:52 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:53 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "15/09/22 16:34:55 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:56 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "15/09/22 16:34:58 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:34:59 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "15/09/22 16:35:01 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:02 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "15/09/22 16:35:04 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:416=10000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "15/09/22 16:35:04 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:05 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "15/09/22 16:35:07 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:08 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "15/09/22 16:35:10 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:11 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "15/09/22 16:35:13 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:14 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "15/09/22 16:35:16 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:17 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "15/09/22 16:35:19 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/22 16:35:20 INFO streaming.PipeMapRed: Records R/W=12592/1\n",
      "15/09/22 16:35:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/22 16:35:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/22 16:35:21 INFO mapred.Task: Task:attempt_local382019511_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/22 16:35:21 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/22 16:35:21 INFO mapred.Task: Task attempt_local382019511_0001_r_000000_0 is allowed to commit now\n",
      "15/09/22 16:35:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local382019511_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/APriori/_temporary/0/task_local382019511_0001_r_000000\n",
      "15/09/22 16:35:21 INFO mapred.LocalJobRunner: Records R/W=12592/1 > reduce\n",
      "15/09/22 16:35:21 INFO mapred.Task: Task 'attempt_local382019511_0001_r_000000_0' done.\n",
      "15/09/22 16:35:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local382019511_0001_r_000000_0\n",
      "15/09/22 16:35:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/22 16:35:22 INFO mapreduce.Job: Job job_local382019511_0001 completed successfully\n",
      "15/09/22 16:35:22 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27170216\n",
      "\t\tFILE: Number of bytes written=41162207\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=160\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=13453643\n",
      "\t\tMap output materialized bytes=13479939\n",
      "\t\tInput split bytes=117\n",
      "\t\tCombine input records=12592\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=13479939\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=216\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=160\n",
      "15/09/22 16:35:22 INFO streaming.StreamJob: Output directory: APriori\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "    # Sort by 2nd field too (though not needed since I'm using stripes)\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper.py -combiner combiner.py -reducer reducer.py \\\n",
    "    -input /user/hadoop/dirhw3/ProductPurchaseData.txt \\\n",
    "    -output APriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/22 16:35:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(DAI93865) => FRO40251: 1.0000\t\n",
      "(ELE12951) => FRO40251: 0.9906\t\n",
      "(DAI88079) => FRO40251: 0.9867\t\n",
      "(DAI43868) => SNA82528: 0.9730\t\n",
      "(DAI23334) => DAI62779: 0.9545\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat APriori/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#HW3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose that you wished to perform the Apriori algorithm once again, though this time now with the goal of listing the top 5 rules with corresponding confidence scores in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce. A rule is now of the form:**\n",
    "\n",
    "    (item1, item2) ⇒ item3 \n",
    "\n",
    "**Recall that the Apriori algorithm is iterative for increasing itemset size, working off of the frequent itemsets of the previous size to explore ONLY the NECESSARY subset of a large combinatorial space. Describe how you might design a framework to perform this exercise.**\n",
    "\n",
    "**In particular, focus on the following:**\n",
    "\n",
    "- **map-reduce steps required**\n",
    "- **enumeration of item sets and filtering for frequent candidates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k$-itemsets we need $k-1$ MapReduce jobs or passes. Hence, for this task (itemsets of size 3) we would need 2 MapReduce jobs or steps.\n",
    "\n",
    "1. The first step would be pretty much as the one implemented above, in HW3.2. The mapper finds the product ids (i.e., the unique items), so the keys in its output will be of the form $\\text{item}_1$, $\\text{item}_2$, ... The values (if we take the stripes approach, as in HW3.2) will be all the other items found together with those 1-itemsets, and their corresponding counts (as well as the total count of occurrences of the 1-itemsets).\n",
    "\n",
    "        {item_i: {*: n_i, item_j: n_ij, item_k: n_ik, ...}, \n",
    "         item_j: {*: n_j, item_k: n_jk, ...}, \n",
    "         ...}\n",
    "\n",
    "    The reducer aggregates the results from all the mappers, and after that discards those values (for a certain key) whose count does not exceed the chosen support (i.e., the non-frequent 2-itemsets: $n_{ij} \\leq 100$), as well as those keys (i.e., the non-frequent 1-itemsets) whose total count does not exceed that support, either ($n_i \\leq 100$). Since a single reducer will handle all mappers' outputs with the same key, we can discard those non-frequent itemsets even if we use more than 1 reducer in this first step. After that, it combines the keys (1-itemsets) and values (the 2nd element of 2-itemsets) into pairs.\n",
    "    \n",
    "        (item_i, item_j), (item_i, item_k), ...\n",
    "\n",
    "2. The mapper in the second step takes as input the output from the previous reducer, as well as the original set of baskets, constructing key-value pairs where the keys are the frequent 2-itemsets returned by the previous reducer, and the values are the co-occurring items with their counts:\n",
    "\n",
    "        {item_ij: {*: n_ij, item_k: n_ijk, ...},\n",
    "         ...}\n",
    "\n",
    "    The reducer aggregates the results from all the mappers, and after that discards those values (for a certain key) whose count does not exceed the chosen support (i.e.,, the non-ferquent 3-itemsets: $n_{ijk} \\leq 100$). All keys already correspond to frequent 2-itemsets (they were filtered in the first step), so there's no need to discard any of them. From this point, we can calculate the confidence for each 3-itemset ($n_{ijk} / n_{ij}$) and present the results as in HW3.2:\n",
    "    \n",
    "        (item1, item2) => item3\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

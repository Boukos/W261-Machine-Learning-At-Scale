{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 04\n",
    "- Submission date: 9/29/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Errata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will upload any **minor corrections** I may make to the assignment after I submit it:\n",
    "\n",
    "[https://www.dropbox.com/s/lf9ijexdvqobtm6/HW4-Errata.txt?dl=0](https://www.dropbox.com/s/lf9ijexdvqobtm6/HW4-Errata.txt?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is MrJob? How is it different to Hadoop MapReduce?**\n",
    "\n",
    "2. **What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **MrJob** is a Python package for running Hadoop streaming jobs, i.e., a Hadoop streaming framework. Its **differences** with Hadoop MapReduce are:\n",
    "\n",
    "    * It allows (or assists in) producing multistep jobs (be it a sequential pipelining or an iterative one).\n",
    "    * Unlike Hadoop MapReduce, MRJob accepts as input/output text formats raw text and JSON, and it currently does not support binary serialization schemes. That makes it much slower than Hadoop streaming via Python, because deserialization and serialization of records incurs a lot of CPU, storage, and network overhead...\n",
    "    * ...but it is also very easy to write, maintain, and communicate with, and it can work seamlessly with EMR and complex objects, which compensates for that reduction in speed.\n",
    "\n",
    "2. **`mapper_final()`, `combiner_final()`, `reducer_final()` methods** are defined within a subclass of MRJob class, and perform the work (and shutdown) phases of the Mapper, Combiner, and Reducer in a MapReduce job (`mapper_init()`, `combiner_init()`, `reducer_init()` could optionally be created too, in order to initizalize those phases). They are called when we call the `.py` file that contains the subclass definition, or when working with a Python driver such as...\n",
    "\n",
    "    ```python\n",
    "    from file import MRJob_subclass\n",
    "    mr_job = MRJob_subclass(args=...)\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "    ...the moment we run the process (`runner.run`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. **What is serialization in the context of MrJob or Hadoop?**\n",
    "2. **When it used in these frameworks?**\n",
    "3. **What is the default serialization mode for input and outputs for MrJob?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Serialization** is the process of converting structured objects (raw text, JSON) into a byte stream in order to store the object or transmit it. Its main purpose is to save the state of an object in order to be able to recreate it when needed.**\n",
    "\n",
    "2. In **Hadoop** serialization is used heavilyt throughout the entire framework. In **MrJob** it is used in a limited fashion: neither the input nor the output can be in binary format, which slows down our pipelines because we have to convert the data previously.\n",
    "\n",
    "3. Each job has an input protocol, an internal, and an output protocol. By default, those protocols assume raw values, JSON, and JSON again, respectively. So the **default serialization mode for inputs and outputs** are from raw text (input; lines are read as strings) or JSON (output; lines are read as JSON strings separated by a tab character) to binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:**\n",
    "\n",
    "**[https://kdd.ics.uci.edu/databases/msweb/msweb.html](https://kdd.ics.uci.edu/databases/msweb/msweb.html)**\n",
    "\n",
    "**[http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/](http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/)**\n",
    "\n",
    "**This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.**\n",
    "\n",
    "**Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:**\n",
    "\n",
    "    C,\"10001\",10001   #Visitor id 10001\n",
    "    V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "    V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "    V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "    C,\"10002\",10002   #Visitor id 10001\n",
    "    V**`\n",
    "\n",
    "**Note: #denotes comments to the format:**\n",
    "\n",
    "    V,1000,1,C, 10001\n",
    "    V,1001,1,C, 10001\n",
    "    V,1002,1,C, 10001\n",
    "\n",
    "**Write the python code to accomplish this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have to report the URL in HW4.4, I will also include that information in the transformed logfile, having something like:\n",
    "\n",
    "    V,1000,1,/regwiz,C,10001\n",
    "    V,1001,1,/support,C,10001\n",
    "    V,1002,1,/athome,C,10001\n",
    "\n",
    "That could be reduced to:\n",
    "\n",
    "    1000,/regwiz,10001\n",
    "    1001,/support,10001\n",
    "    1002,/athome,10001\n",
    "    \n",
    "eliminating all the `V`s in the first field, the `1`s in the third field, and the `C`s in the fifth one, keeping only the webpage ID, the webpage (relative) URL, and the Visitor ID, by I've kept the format as we were asked to, just inserting the (relative) URL between the Vroot and the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Instances  32711\n",
      "Attributes  294\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/' +\\\n",
    "    'anonymous-msweb.data'\n",
    "# A dictionary to link webpages URLs to IDs\n",
    "Vroot = {}\n",
    "# Two counters to keep track of number of distinct webpages and visitors\n",
    "A = 0\n",
    "C = 0\n",
    "with open('preprocessed_anonymous-msweb.data', 'w') as output_file:\n",
    "    for line in urllib2.urlopen(url):\n",
    "        record = line.strip().split(',')\n",
    "        record = [x.strip('\"') for x in record]\n",
    "        # If the record corresponds to an attribute, link the URL to the Vroot\n",
    "            # All lines starting with 'A' are at the beginning of the file, so \n",
    "            # the whole dictionary is already created when lines starting with \n",
    "            # 'C' or 'V' are read\n",
    "        if record[0] == 'A':\n",
    "            A += 1\n",
    "            Vroot[record[1]] = record[4]\n",
    "        # If the record corresponds to a case (Visitor ID), save that info to \n",
    "            # pass it to the Vroot\n",
    "        elif record[0] == 'C':\n",
    "            C += 1\n",
    "            case = record[:2]\n",
    "        # If the line contains a vote line for a case, concatenate the user ID\n",
    "        elif record[0] == 'V':\n",
    "            output_file.write(','.join(record+[Vroot[record[1]]]+case) + '\\n')\n",
    "\n",
    "print 'Training Instances  {}'.format(C)\n",
    "print 'Attributes  {}'.format(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [https://kdd.ics.uci.edu/databases/msweb/msweb.data.html](https://kdd.ics.uci.edu/databases/msweb/msweb.data.html) there were:\n",
    "\n",
    "`Training Instances  32711\n",
    "Attributes  294`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines (visits):     98654\n",
      "-------------------------------\n",
      "First 10 lines:\n",
      "V,1000,1,/regwiz,C,10001\n",
      "V,1001,1,/support,C,10001\n",
      "V,1002,1,/athome,C,10001\n",
      "V,1001,1,/support,C,10002\n",
      "V,1003,1,/kb,C,10002\n",
      "V,1001,1,/support,C,10003\n",
      "V,1003,1,/kb,C,10003\n",
      "V,1004,1,/search,C,10003\n",
      "V,1005,1,/norge,C,10004\n",
      "V,1006,1,/misc,C,10005\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of lines (visits): \\\n",
    "    \"$(cat preprocessed_anonymous-msweb.data | wc -l)\n",
    "!echo \"-------------------------------\"\n",
    "!echo \"First 10 lines:\"\n",
    "!head preprocessed_anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the 5 most frequently visited pages using mrjob from the output of 4.2 (i.e., transfromed log file).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.3.a\n",
    "With my **first approach** the MRJob program just aggregates the number of visits of each webpage, and the Top5 is extracted using the **command line**. In HW4.3.b my **second approach** is shown, where **MRJob** does all the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting VisitsPerVroot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile VisitsPerVroot.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "class MRVisitsPerVroot(MRJob):\n",
    "\n",
    "    # We can re-use the code of the reducer as combiner\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper, \n",
    "                   combiner = self.reducer,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the Vroot\"\"\"\n",
    "        cell = line.split(',')\n",
    "        yield cell[1],1\n",
    "\n",
    "    def reducer(self, vroot, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together\"\"\"\n",
    "        total = sum(visit_counts)\n",
    "        yield vroot, total\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRVisitsPerVroot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impor the class\n",
    "from VisitsPerVroot import MRVisitsPerVroot\n",
    "\n",
    "# Use file generated in HW4.2\n",
    "mr_job = MRVisitsPerVroot(args=['preprocessed_anonymous-msweb.data'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    # For every webpage in the dataset\n",
    "    with open('Visits','w') as results:\n",
    "        for line in runner.stream_output():\n",
    "            # Add a line in the file containing the webpageID and the times it \n",
    "                # was visited\n",
    "            results.writelines('\\t'.join([str(x) for x in \\\n",
    "                                          mr_job.parse_output_line(line)])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've generated a text file with one line per Vroot, each one containing:\n",
    "\n",
    "    Vroot_id TAB Visit_count\n",
    "\n",
    "Note that the new file contains **285** lines (i.e., **Vroots**), while **the original file listed 294: 9 of the Vroots were not visited by any user**.\n",
    "\n",
    "All the values in the second column should sum up the number of lines in the file generated in HW4.2 (`preprocessed_anonymous-msweb.data`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visits: 98654\n",
      "Number of Vroots: 285\n",
      "-----------------------\n",
      "First 10 lines:\n",
      "1000\t912\n",
      "1001\t4451\n",
      "1002\t749\n",
      "1003\t2968\n",
      "1004\t8463\n",
      "1005\t42\n",
      "1006\t135\n",
      "1007\t865\n",
      "1008\t10836\n",
      "1009\t4628\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of visits: \"$(cat Visits | cut -f 2 | paste -sd+ -|bc)\n",
    "!echo \"Number of Vroots: \"$(cat Visits | wc -l)\n",
    "!echo \"-----------------------\"\n",
    "!echo \"First 10 lines:\"\n",
    "!head Visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most frequently visited pages could be generated this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vroot\tVisits\n",
      "1008\t10836\n",
      "1034\t9383\n",
      "1004\t8463\n",
      "1018\t5330\n",
      "1017\t5108\n"
     ]
    }
   ],
   "source": [
    "!echo Vroot'\\t'Visits\n",
    "!sort Visits -k2 -n -r | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos\tVroot\tVisits\n",
      "1\t1008\t10836\n",
      "2\t1034\t9383\n",
      "3\t1004\t8463\n",
      "4\t1018\t5330\n",
      "5\t1017\t5108\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "# Create a list of results\n",
    "results = []\n",
    "with open('Visits','r') as f:\n",
    "    # For every line (i.e., webpage)\n",
    "    for line in f:\n",
    "        Vroot = line.split()\n",
    "        # Create a list containing WepageID and Visits\n",
    "        Vroot = [int(x) for x in Vroot]\n",
    "        # And add it to the whole list\n",
    "        results.append(Vroot)\n",
    "# Convert the list of lists into a nparray\n",
    "    # where each row relates to a webpage\n",
    "arrayResults = np.array(results)\n",
    "# Order the indices of rows\n",
    "index = np.argsort(arrayResults[:,1])\n",
    "# Get the indices of the Top5\n",
    "top5 = index[-5:][::-1]\n",
    "# Show results\n",
    "print 'Pos\\tVroot\\tVisits'\n",
    "for i, ind in enumerate(top5):\n",
    "    print str(i+1) + '\\t' + '\\t'.join([str(x) for x in arrayResults[ind,:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW4.3.b\n",
    "As mentioned, now we make **MRJob** do all the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Top5Pages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Top5Pages.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "class Top5Pages(MRJob):\n",
    "\n",
    "    # IMPORTANT: if we're sure that only 1 reducer is run at the 1st MRJob step,\n",
    "        # we could use just this 1st step:\n",
    "            # combiner = self.reducer1\n",
    "            # reducer = self.reducer2\n",
    "        # (Or just don't import MRStep, delete steps and combiner methods, and\n",
    "            # rename reducer1 as combiner and reducer2 as reducer\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper, \n",
    "                   combiner = self.combiner,\n",
    "                   reducer=self.reducer1),\n",
    "            MRStep(reducer = self.reducer2)\n",
    "               ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"Extracts the Vroot\"\"\"\n",
    "        cell = line.split(',')\n",
    "        yield cell[1],1\n",
    "\n",
    "    def combiner(self, vroot, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together\"\"\"\n",
    "        total = sum(visit_counts)\n",
    "        yield vroot, total\n",
    "\n",
    "    def reducer1(self, vroot, visit_counts):\n",
    "        \"\"\"Group all webpages using key=None; the values will be dictionaries\n",
    "        with wepage as key and its visits as value\"\"\"\n",
    "        total = sum(visit_counts)\n",
    "        yield None, {vroot: total}\n",
    "\n",
    "    def reducer2(self, _, dictionaries):\n",
    "        \"\"\"The 2nd reducer gets a list of dictionaries, one for each webpage\"\"\"\n",
    "        # We start creating a special type of dictionary: Counter\n",
    "        final_dict = Counter()\n",
    "        # For every item in the list (of dictionaries), i.e., for every webpage\n",
    "        for x in dictionaries:\n",
    "            # Update number of occurrences for each key\n",
    "                # (Also valid to include new keys)\n",
    "            final_dict.update(x)\n",
    "        # Now create 2 lists, one containing the webpageIDs, and the other the \n",
    "            # visits of each one\n",
    "        webpages=[]\n",
    "        visits=[]\n",
    "        for k,v in final_dict.iteritems():\n",
    "            webpages.append(k)\n",
    "            visits.append(v)\n",
    "        # For the Top 5\n",
    "        for i in range(5):\n",
    "            # Find the index of the webpage with most visits\n",
    "            index, value = max(enumerate(visits), key=itemgetter(1))\n",
    "            # Output its ID and number of visits, taking out of the list for the \n",
    "                # next iteration\n",
    "            yield webpages.pop(index), visits.pop(index)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Top5Pages.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos\tVroot\tVisits\n",
      "1\t1008\t10836\n",
      "2\t1034\t9383\n",
      "3\t1004\t8463\n",
      "4\t1018\t5330\n",
      "5\t1017\t5108\n"
     ]
    }
   ],
   "source": [
    "# Impor the class\n",
    "from Top5Pages import Top5Pages\n",
    "\n",
    "# Use file generated in HW4.2\n",
    "mr_job = Top5Pages(args=['preprocessed_anonymous-msweb.data', '--no-strict-protocols'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print 'Pos\\tVroot\\tVisits'\n",
    "    # For every webpage in the Top 5\n",
    "    for i, line in enumerate(runner.stream_output()):\n",
    "        # Print position in ranking, webpageID, and number of visits\n",
    "        print str(i+1) + '\\t' + '\\t'.join([str(x) for x in \\\n",
    "                                           mr_job.parse_output_line(line)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transformed log file). In this output please include the webpage URL, webpageID and Visitor ID.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a directory to put files (one for each wepage: 285)\n",
    "# (Delete previous version if it existed)\n",
    "!rm -r MostFrequentVisitors\n",
    "!mkdir MostFrequentVisitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file will be of the form\n",
    "\n",
    "    1074: /ntworkstation\n",
    "    11520\n",
    "    18559\n",
    "    19498\n",
    "    33089\n",
    "    ...\n",
    "\n",
    "where the first line has the form `webpageID: webpageURL` and each line corresponds to a `visitorID` (there will be more than one in case of a tie (which will always be the case in this dataset, where no user visits a webpage more than once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFreqVisitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFreqVisitor.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "\n",
    "class MostFreqVisitor(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Extract information of each line\n",
    "        cell = line.split(',')\n",
    "        # Concatenate webpageID and webpageURL\n",
    "            # These are relative URLs so all start with '/'\n",
    "        page = cell[1]+': '+cell[3]\n",
    "        # Output:\n",
    "            # key: webpage \n",
    "            # value: a dict with key=visitorID and value=1\n",
    "                # (Each line relates to a single visit)\n",
    "        yield page, {cell[5]:1}\n",
    "\n",
    "    def combiner(self, page, dictionary):\n",
    "        # For each key (webpage) the combiner receives a list of dictionaries,\n",
    "            # each one with a single key (visitorID) and value 1\n",
    "        # The combiner has to aggregate count of visits for each visitor\n",
    "            # in case there is more than one (not the case in this dataset)\n",
    "        # We start creating a special type of dictionary: Counter\n",
    "        agg_dictionary = Counter()\n",
    "        # For every item in the list (of dictionaries)\n",
    "        for x in dictionary:\n",
    "            # Update number of occurrences for each key\n",
    "                # (Also valid to include new keys)\n",
    "            agg_dictionary.update(x)\n",
    "        # Now create a new list of dictionaries, but this time it would look\n",
    "            # like [{visitor1: 2}, {visitor2: 3}, ...] instead of\n",
    "            # [{visitor1: 1}, {visitor2: 1}, {visitor2: 1}, ...]\n",
    "        list_dicts = []\n",
    "        # And fullfil for every user that visited the webpage\n",
    "        for k,v in agg_dictionary.iteritems():\n",
    "            list_dicts.append({k:v})\n",
    "        # Output:\n",
    "            # key: webpage\n",
    "            # value: a dict with key=visitorID and value=#visits of visitorID\n",
    "        yield page, agg_dictionary\n",
    "\n",
    "    def reducer(self, page, agg_dictionary):\n",
    "        # Similar to combiner\n",
    "        final_dict = Counter()\n",
    "        for x in agg_dictionary:\n",
    "            final_dict.update(x)\n",
    "        # But now (assuming a single reducer) we have to filter, keeping only\n",
    "            # the most frequent visitor\n",
    "        maximum = max(final_dict.values())\n",
    "        # Just in case there's a tie (which is the case in this dataset)\n",
    "            # keep a list of most frequent visitorS\n",
    "        MFvisitor_list = []\n",
    "        for visitor, visits in final_dict.iteritems():\n",
    "            # Only put most frequent visitor(s) in the list\n",
    "            if visits == maximum:\n",
    "                MFvisitor_list.append(visitor)\n",
    "        # Sort visitors by ID\n",
    "        MFvisitor_list.sort()\n",
    "        # Output:\n",
    "            # key: webpage (it contains ID and URL in a single string)\n",
    "            # value: a list of most frequent visitors\n",
    "                # (Could also report the (maximum) count of visits\n",
    "        yield page, MFvisitor_list\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MostFreqVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import class\n",
    "from MostFreqVisitor import MostFreqVisitor\n",
    "\n",
    "# Use file generated in HW4.2\n",
    "mr_job = MostFreqVisitor(args=['preprocessed_anonymous-msweb.data'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    # For every webpage in the dataset\n",
    "    for line in runner.stream_output():\n",
    "        # Create a file identified just by the webpageID (4 digits)\n",
    "        page_id = mr_job.parse_output_line(line)[0][:4]\n",
    "        # Write the file (in the folder created at the beginning)\n",
    "        with open('MostFrequentVisitors/FrequentVisitors.'+page_id,'w') as f:\n",
    "            # First line with webpage info, then one line per most frequent user\n",
    "            f.writelines(mr_job.parse_output_line(line)[0] + '\\n' + \n",
    "                        '\\n'.join([x for x in \\\n",
    "                                   mr_job.parse_output_line(line)[1]]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\r\n"
     ]
    }
   ],
   "source": [
    "# Check number of files (should be the number obtained in HW4.3)\n",
    "!ls MostFrequentVisitors/FrequentVisitors.* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98654\r\n"
     ]
    }
   ],
   "source": [
    "# Check number of lines in all files: should be the number of visits since\n",
    "    # all visitors are the most frequent in this dataset\n",
    "# But delete 1st line in each file!!!\n",
    "# Should be the number obtained in HW4.3\n",
    "!cd MostFrequentVisitors; lines=$(find . -type f -exec cat {} + | wc -l); \\\n",
    "    files=$(ls | wc -l); echo $(echo $lines-$files | bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT**: Since almost 100,000 lines seems excessive, I only show here the 4 most frequent visitors of a few webpages.\n",
    "\n",
    "I've selected 10 webpageIDs, the first five (1000–1014) and 1120–1124 (because some of them had less than 4 visitors and hence the output is shorter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: /regwiz\n",
      "10001\n",
      "10010\n",
      "10039\n",
      "10073\n",
      "1001: /support\n",
      "10001\n",
      "10002\n",
      "10003\n",
      "10020\n",
      "1002: /athome\n",
      "10001\n",
      "10019\n",
      "10020\n",
      "10031\n",
      "1003: /kb\n",
      "10002\n",
      "10003\n",
      "10006\n",
      "10019\n",
      "1004: /search\n",
      "10003\n",
      "10006\n",
      "10008\n",
      "10018\n",
      "1120: /switch\n",
      "10241\n",
      "1121: /magazine\n",
      "10241\n",
      "10522\n",
      "13153\n",
      "13595\n",
      "1122: /mindshare\n",
      "10243\n",
      "21553\n",
      "21782\n",
      "23581\n",
      "1123: /germany\n",
      "10254\n",
      "10267\n",
      "10326\n",
      "10335\n",
      "1124: /industry\n",
      "10263\n",
      "10602\n",
      "10607\n",
      "10697\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    !head -5 MostFrequentVisitors/FrequentVisitors.100$i\n",
    "for i in range(0,5):\n",
    "    !head -5 MostFrequentVisitors/FrequentVisitors.112$i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you will use a different dataset consisting of word-frequency distributions for 1,000 Twitter users. These Twitter users use language in very different ways, and were classified by hand according to the criteria:**\n",
    "\n",
    "**0: Human, where only basic human-human communication is observed.**\n",
    "\n",
    "**1: Cyborg, where language is primarily borrowed from other sources (e.g., jobs listings, classifieds postings, advertisements, etc...).**\n",
    "\n",
    "**2: Robot, where language is formulaically derived from unrelated sources (e.g., weather/seismology, police/fire event logs, etc...).**\n",
    "\n",
    "**3: Spammer, where language is replicated to high multiplicity (e.g., celebrity obsessions, personal promotion, etc... )**\n",
    "\n",
    "**The main data lie in the accompanying file: topUsers_Apr-Jul_2014_1000-words.txt. And and are of the form:**\n",
    "\n",
    "    USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    "\n",
    "**where**\n",
    "\n",
    "    USERID = unique user identifier\n",
    "    CODE = 0/1/2/3 class code\n",
    "    TOTAL = sum of the word counts\n",
    "\n",
    "**Using this data, you will implement a 1000-dimensional K-means algorithm on the users by their 1000-dimensional word stripes/vectors using several centroid initializations and values of K.**\n",
    "\n",
    "**Note that each \"point\" is a user as represented by 1000 words, and that word-frequency distributions are generally heavy-tailed power-laws (often called Zipf distributions), and are very rare in the larger class of discrete, random distributions. For each user you will have to normalize by its \"TOTAL\" column. Try several parameterizations and initializations:**\n",
    "\n",
    "**(A) K=4 uniform random centroid-distributions over the 1000 words**\n",
    "\n",
    "**(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution**\n",
    "\n",
    "**(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution** \n",
    "\n",
    "**(D) K=4 \"trained\" centroids, determined by the sums across the classes.**\n",
    "\n",
    "**and iterate until a threshold (try 0.001) is reached.**\n",
    "\n",
    "**After convergence, print out a summary of the classes present in each cluster. In particular, report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D.**\n",
    "\n",
    "**Note that you do not have to compute the aggregated distribution or the class-aggregated distributions, which are rows in the auxiliary file: topUsers_Apr-Jul_2014_1000-words_summaries.txt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs let's normalize the data for each user (and print a summary of the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with code 0: 752 (75.2%)\n",
      "User with code 1:  91 ( 9.1%)\n",
      "User with code 2:  54 ( 5.4%)\n",
      "User with code 3: 103 (10.3%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_users = 0\n",
    "\n",
    "with open('topUsers_Apr-Jul_2014_1000-words.txt','r') as dataset:\n",
    "    # For each user\n",
    "    for ind,line in enumerate(dataset):\n",
    "        num_users += 1\n",
    "        line = line.split(',')\n",
    "        if ind==0:\n",
    "            num_coord = len(line)-3\n",
    "        # 1st value is userID\n",
    "        user = [int(line[0])]\n",
    "        # Append 2nd value (class code)\n",
    "        user.append(int(line[1]))\n",
    "        # Append the 1000 coordinates normalized by the total\n",
    "        for i in range(num_coord):\n",
    "            user.append(float(line[i+3])/int(line[2]))\n",
    "        # Convert to nparray\n",
    "        user = np.array(user).reshape(1,num_coord+2)\n",
    "        if ind==0:\n",
    "            data=user\n",
    "        else:\n",
    "            data=np.append(data,user,axis=0)\n",
    "np.savetxt('normalized_dataset.csv',data,delimiter = \",\")\n",
    "\n",
    "# Occurrences of each code\n",
    "for i in range(4):\n",
    "    occurrences = sum(data[:,1]==i)\n",
    "    portion = 100 * float(occurrences) / num_users\n",
    "    print 'User with code {0}: {1:3} ({2:4.1f}%)'.format(i, occurrences, \n",
    "                                                         portion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MRJob process that we'll run for each part (A-D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=4\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,\n",
    "                   combiner = self.combiner, reducer = self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) \\\n",
    "                                for s in open('/tmp/Centroids2.txt').\\\n",
    "                                readlines()]\n",
    "        open('/tmp/Centroids2.txt', 'w').close()\n",
    "    \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        # Get all components for each record\n",
    "        D = (map(float,line.split(',')))\n",
    "        # 1st component is the user ID, 2nd is the code, the rest are the \n",
    "            # coordinates\n",
    "        coord = D[2:]\n",
    "        # Get the code (class) of the user\n",
    "        code = int(D[1])\n",
    "        # Include in the value a binary vector of lenght k\n",
    "            # 1 for the corresponding code, 0 for the rest\n",
    "        code_info = [0]*4\n",
    "        code_info[code]=1\n",
    "        # Append 1 (added 1 element to the cluster output as key)\n",
    "        code_info.append(1)\n",
    "        # Output:\n",
    "            # key: cluster assigned\n",
    "            # value: tuple with the thousand coordinates plus a 4-vector with \n",
    "                # the code/class plus 1\n",
    "        yield int(MinDist(coord,self.centroid_points)), tuple(coord+code_info)\n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sum_coord = [0]*1000\n",
    "        count_code = [0]*4\n",
    "        num = 0\n",
    "        for x in inputdata:\n",
    "            for i in range(-1,-5,-1):\n",
    "                count_code[i] = count_code[i] + x[i-1]\n",
    "            for i in range(1000):\n",
    "                sum_coord[i] = sum_coord[i] + x[i]\n",
    "            num = num + x[-1]\n",
    "        yield idx,(sum_coord,count_code,num)\n",
    "\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        sum_coord = [0]*1000\n",
    "        count_code = [0] * 4\n",
    "        num = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            centroids.append([0]*1000)\n",
    "        for x in inputdata:\n",
    "            for i in range(-1,-5,-1):\n",
    "                count_code[i] = count_code[i] + x[1][i]\n",
    "            for i in range(1000):\n",
    "                sum_coord[i] = sum_coord[i] + x[0][i]\n",
    "            num[idx] = num[idx] + x[2]\n",
    "        centroids[idx] = sum_coord\n",
    "        for i in range(1000):\n",
    "            centroids[idx][i] = centroids[idx][i]/num[idx]\n",
    "        with open('/tmp/Centroids2.txt', 'a') as f:\n",
    "            f.writelines(','.join([str(x) for x in centroids[idx]]) + '\\n')\n",
    "        yield idx,(centroids[idx],count_code)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##(A) K=4 uniform random centroid-distributions over the 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "#Generate initial centroids\n",
    "centroid_points=[]\n",
    "k = 4\n",
    "for i in range(k):\n",
    "    numbers = random.sample(1000)\n",
    "    numbers = numbers / sum(numbers)\n",
    "    centroid_points.append(numbers)\n",
    "with open('/tmp/Centroids2.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in \\\n",
    "                     centroid_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver will be common to the 4 approaches; the only thing that changes is the seeds we use as initial centroids (and the number of clusters). Hence, it seems appropriate to define a function to be called each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations until convergence: 5\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "|Percentage of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "---------------------------------------------------------------------\n",
      "|    in Cluster 0       |   98.14% |    3.30% |   20.37% |   96.12% |\n",
      "|    in Cluster 1       |    1.73% |    0.00% |    9.26% |    0.00% |\n",
      "|    in Cluster 2       |    0.00% |   56.04% |    0.00% |    0.00% |\n",
      "|    in Cluster 3       |    0.13% |   40.66% |   70.37% |    3.88% |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "|Number of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "----------------------------------------------------------------------------\n",
      "|    in Cluster 0   |      738 |        3 |       11 |       99 |      851 |\n",
      "|    in Cluster 1   |       13 |        0 |        5 |        0 |       18 |\n",
      "|    in Cluster 2   |        0 |       51 |        0 |        0 |       51 |\n",
      "|    in Cluster 3   |        1 |       37 |       38 |        4 |       80 |\n",
      "----------------------------------------------------------------------------\n",
      "                    |      752 |       91 |       54 |      103 |\n",
      "                    ---------------------------------------------\n",
      "\n",
      "PURITY:    0.840\n",
      "ACCURACY:  0.873\n",
      "PRECISION: 0.867\n",
      "RECALL:    0.981\n"
     ]
    }
   ],
   "source": [
    "# Define a function that we'll run for each part (A-D)\n",
    "def hw45(k):\n",
    "    import numpy as np\n",
    "    from Kmeans import MRKmeans, stop_criterion\n",
    "    mr_job = MRKmeans(args=['normalized_dataset.csv'])\n",
    "\n",
    "    # Update centroids iteratively\n",
    "    it = 0 # count of iterations\n",
    "    count_code=[[0]*4]*k\n",
    "    while(1):\n",
    "        # save previous centoids to check convergency\n",
    "        centroid_points_old = centroid_points[:]\n",
    "        with mr_job.make_runner() as runner: \n",
    "            runner.run()\n",
    "            # stream_output: get access of the output \n",
    "            for i,line in enumerate(runner.stream_output()):\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                centroid_points[key] = value[0]\n",
    "                count_code[key] = value[1]\n",
    "        it = it + 1\n",
    "        if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "            break\n",
    "\n",
    "    # Print iterations\n",
    "    print 'Iterations until convergence: {}\\n'.format(it)\n",
    "\n",
    "    # Summary of classes in each cluster\n",
    "    count_code = np.array(count_code) # a k x 4 matrix\n",
    "    # Sum row-wise\n",
    "    total_code = np.sum(count_code, axis = 0).astype(float)\n",
    "    portion = count_code/total_code*100\n",
    "\n",
    "    classifier =['  Class 0 ', '  Class 1 ', '  Class 2 ', '  Class 3 ']\n",
    "    print '---------------------------------------------------------------------'\n",
    "    print '|Percentage of users of |{}|{}|{}|{}|'.format(*classifier)\n",
    "    print '---------------------------------------------------------------------'\n",
    "    for j in range(k):\n",
    "        portion_row = [j]\n",
    "        portion_row.extend(list(portion[j,:]))\n",
    "        print '|    in Cluster {}       |{:8.2f}% |{:8.2f}% |{:8.2f}% '\\\n",
    "            '|{:8.2f}% |'.format(*portion_row)\n",
    "    print '---------------------------------------------------------------------'\n",
    "    \n",
    "    total_code = total_code.astype(int)\n",
    "    # Sum column-wise\n",
    "    total_cluster = list(np.sum(count_code, axis = 1).reshape(k,))\n",
    "    print '\\n'\n",
    "    print '-----------------------------------------------------------------'\n",
    "    print '|Number of users of |{}|{}|{}|{}|'.format(*classifier)\n",
    "    print '----------------------------------------------------------------------------'\n",
    "    for j in range(k):\n",
    "        count_row = [j]\n",
    "        count_row.extend(list(count_code[j,:]))\n",
    "        count_row.extend([total_cluster[j]])\n",
    "\n",
    "        print '|    in Cluster {}   |{:9} |{:9} |{:9} |{:9} |{:9} |'\\\n",
    "            .format(*count_row)\n",
    "    print '----------------------------------------------------------------------------'\n",
    "    print '                    |{:9} |{:9} |{:9} |{:9} |'.format(*total_code)\n",
    "    print '                    ---------------------------------------------'\n",
    "    \n",
    "    # Purity\n",
    "    majority = np.max(count_code, axis = 1)\n",
    "    purity = float(np.sum(majority)) / np.sum(count_code)\n",
    "    print '\\nPURITY:    {:.3f}'.format(purity)\n",
    "    \n",
    "    # Since there are many more cases of class 0 (humans), search what's the \n",
    "        # cluster most of them are assigned to\n",
    "    code0_cluster = np.argmax(count_code[:,0])\n",
    "    # Estimate False and True Positives and Negatives, and send them as output \n",
    "        # of the function.\n",
    "    # My reasoning is largely explained at the end of HW4.5\n",
    "    \n",
    "    # 1st column of the matrix count_code corresponds to class 0\n",
    "    TP = count_code[code0_cluster,0]\n",
    "    # Rest of the elemnts in the row are FPs (robots, spammers... assigned to \n",
    "        # the cluster where the majority of humans were assigned to)\n",
    "    FP = np.sum(count_code[code0_cluster,:]) - TP\n",
    "    # Rest of the elements in the 1st column are FNs (humans assigned to other \n",
    "        # clusters)\n",
    "    FN = np.sum(count_code[:,0]) - TP\n",
    "    TN = np.sum(count_code) - TP - FP - FN\n",
    "    return [TP,FP,FN,TN]\n",
    "    \n",
    "[TP,FP,FN,TN] = hw45(k)\n",
    "\n",
    "# Define a function that calculates Accuracy, Precision, and Recall, from \n",
    "    #previous output\n",
    "def metrics(TP,FP,FN,TN):\n",
    "    print 'ACCURACY:  {:.3f}'.format(float(TP+TN)/(TP+FP+FN+TN))\n",
    "    print 'PRECISION: {:.3f}'.format(float(TP)/(TP+FP))\n",
    "    print 'RECALL:    {:.3f}'.format(float(TP)/(TP+FN))\n",
    "\n",
    "metrics(TP,FP,FN,TN)\n",
    "# The results are commented at the end of HW4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to generate the seeds and call the function previously defined. We'll call it inside other function, since part (B) and (C) are almost equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations until convergence: 4\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "|Percentage of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "---------------------------------------------------------------------\n",
      "|    in Cluster 0       |    0.13% |   96.70% |   74.07% |    3.88% |\n",
      "|    in Cluster 1       |   99.87% |    3.30% |   25.93% |   96.12% |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "|Number of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "----------------------------------------------------------------------------\n",
      "|    in Cluster 0   |        1 |       88 |       40 |        4 |      133 |\n",
      "|    in Cluster 1   |      751 |        3 |       14 |       99 |      867 |\n",
      "----------------------------------------------------------------------------\n",
      "                    |      752 |       91 |       54 |      103 |\n",
      "                    ---------------------------------------------\n",
      "\n",
      "PURITY:    0.839\n",
      "ACCURACY:  0.883\n",
      "PRECISION: 0.866\n",
      "RECALL:    0.999\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt','r') as summary:\n",
    "    for line in summary:\n",
    "        line = line.split(',')\n",
    "        if line[0]=='ALL_CODES':\n",
    "            coord = [float(x)/int(line[2]) for x in line[3:]]\n",
    "\n",
    "def hw45BC(coord, k):\n",
    "    array_centroid_points = random.sample(k*1000).reshape(k,1000)*0.001\n",
    "    array_centroid_points = array_centroid_points + np.array(coord)\n",
    "\n",
    "    array_centroid_points = (array_centroid_points.T / array_centroid_points.\\\n",
    "                             sum(axis=1)).T\n",
    "\n",
    "    centroid_points = []\n",
    "    for i in range(k):\n",
    "        centroid_points.append(list(array_centroid_points[i,:]))\n",
    "\n",
    "    with open('/tmp/Centroids2.txt', 'w+') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in \\\n",
    "                         centroid_points)\n",
    "\n",
    "    [TP,FP,FN,TN] = hw45(k)\n",
    "    return [TP,FP,FN,TN]\n",
    "\n",
    "[TP,FP,FN,TN] = hw45BC(coord, 2)\n",
    "\n",
    "metrics(TP,FP,FN,TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations until convergence: 8\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "|Percentage of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "---------------------------------------------------------------------\n",
      "|    in Cluster 0       |   26.20% |    2.20% |   25.93% |   24.27% |\n",
      "|    in Cluster 1       |    0.13% |   40.66% |   70.37% |    3.88% |\n",
      "|    in Cluster 2       |   73.67% |    1.10% |    0.00% |   71.84% |\n",
      "|    in Cluster 3       |    0.00% |   56.04% |    3.70% |    0.00% |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "|Number of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "----------------------------------------------------------------------------\n",
      "|    in Cluster 0   |      197 |        2 |       14 |       25 |      238 |\n",
      "|    in Cluster 1   |        1 |       37 |       38 |        4 |       80 |\n",
      "|    in Cluster 2   |      554 |        1 |        0 |       74 |      629 |\n",
      "|    in Cluster 3   |        0 |       51 |        2 |        0 |       53 |\n",
      "----------------------------------------------------------------------------\n",
      "                    |      752 |       91 |       54 |      103 |\n",
      "                    ---------------------------------------------\n",
      "\n",
      "PURITY:    0.840\n",
      "ACCURACY:  0.727\n",
      "PRECISION: 0.881\n",
      "RECALL:    0.737\n"
     ]
    }
   ],
   "source": [
    "[TP,FP,FN,TN] = hw45BC(coord, 4)\n",
    "\n",
    "metrics(TP,FP,FN,TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##(D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations until convergence: 5\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "|Percentage of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "---------------------------------------------------------------------\n",
      "|    in Cluster 0       |   99.60% |    3.30% |   25.93% |   36.89% |\n",
      "|    in Cluster 1       |    0.00% |   56.04% |    0.00% |    0.00% |\n",
      "|    in Cluster 2       |    0.13% |   40.66% |   74.07% |    3.88% |\n",
      "|    in Cluster 3       |    0.27% |    0.00% |    0.00% |   59.22% |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "|Number of users of |  Class 0 |  Class 1 |  Class 2 |  Class 3 |\n",
      "----------------------------------------------------------------------------\n",
      "|    in Cluster 0   |      749 |        3 |       14 |       38 |      804 |\n",
      "|    in Cluster 1   |        0 |       51 |        0 |        0 |       51 |\n",
      "|    in Cluster 2   |        1 |       37 |       40 |        4 |       82 |\n",
      "|    in Cluster 3   |        2 |        0 |        0 |       61 |       63 |\n",
      "----------------------------------------------------------------------------\n",
      "                    |      752 |       91 |       54 |      103 |\n",
      "                    ---------------------------------------------\n",
      "\n",
      "PURITY:    0.901\n",
      "ACCURACY:  0.942\n",
      "PRECISION: 0.932\n",
      "RECALL:    0.996\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "\n",
    "coord = []\n",
    "with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt','r') as summary:\n",
    "    for line in summary:\n",
    "        line = line.split(',')\n",
    "        if line[0]=='CODE':\n",
    "            coord.append([float(x)/int(line[2]) for x in line[3:]])\n",
    "\n",
    "with open('/tmp/Centroids2.txt', 'w+') as f:\n",
    "    f.writelines(','.join(str(j) for j in i) + '\\n' for i in coord)\n",
    "\n",
    "[TP,FP,FN,TN] = hw45(4)\n",
    "\n",
    "metrics(TP,FP,FN,TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's the summary for each approach:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "\n",
    "    PURITY:    0.840\n",
    "    ACCURACY:  0.873\n",
    "    PRECISION: 0.867\n",
    "    RECALL:    0.981\n",
    "\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "\n",
    "    PURITY:    0.839\n",
    "    ACCURACY:  0.883\n",
    "    PRECISION: 0.866\n",
    "    RECALL:    0.999\n",
    "\n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "\n",
    "    PURITY:    0.840\n",
    "    ACCURACY:  0.727\n",
    "    PRECISION: 0.881\n",
    "    RECALL:    0.737\n",
    "\n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "    PURITY:    0.901\n",
    "    ACCURACY:  0.942\n",
    "    PRECISION: 0.932\n",
    "    RECALL:    0.996\n",
    "\n",
    "The results do not differ too much, but they're definitely better when we use \"trained\" centroids.\n",
    "\n",
    "The **Purity** is not always a good indication of the \"quality\" of the clusters, especially for a large number of them, but this is not the case. Here it's about 0.84 for the first thre approaches, andd much larger, about 0.90, for the \"trained\" centroids.\n",
    "\n",
    "To define **Accuracy**, **Precision**, and **Recall** I didn't follow mentioned in \"*An introduction of information retrieval*\" but the typical definitions in classification problems:a true positive would correspond to a human properly assigned to a cluster where humans are the norm, a false positive to a human assigned to a cluster dominated by the other classes, a true negative corresponds to any cyborg, robot, or spammer, assigned to any cluster where humans are the exception, and a false negative would correspond to the opposite situation. I've followed this approach because my guess is that in this kind of problem (as in the spam vs. ham classification), a false negative (i.e., discarding a human message) is much worse or more inconvenient than a false positive (accepting undesired messages from the other classes). Hence, the most important metric would probably be the **Recall**. I think it's not surprising that the 2 clusters obtained in part (B) have an almost perfect recall: human messages that with more clusters might have been incorrectly assigned to different clusters, are in this case assigned to a \"big cluster;\" this big cluster also contains messages from other classes, and hence its accuracy and precsion are lower than those achieved in (D), as expected. The purity is also not too high, because only 2 classes (0: humans; and 1: cyborgs) can be dominant in the 2 clusters; robots are mostly assigned to the same cluster than cyborgs, and spammers are mainly assigned to the same cluster than humans. This happens in the 4 approaches.\n",
    "\n",
    "The bad results in (C) are not very surprising: we just added noise to the centroid of the single cluster that covers all classes, so the starting points are not dissimilar enough. I scaled down the perturbation by a factor of 0.001, but even when using a $U(0,1)$ distribution the results are pretty similar.\n",
    "\n",
    "Overall, the results of the 4 approaches have surprised given the relatively high dimensionality of the problem (if we think of the clusters as hyperspheres, two members of a cluster, in a high-dimensional space, are very likely to be closer to the the border of the hypersphere than to each other (or its centroid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket failed: s3://ucb-mids-mls-juanjocarin/tmp/HW74/ A client error (TooManyBuckets) occurred when calling the CreateBucket operation: You have attempted to create more buckets than allowed\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://ucb-mids-mls-juanjocarin/tmp/HW74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

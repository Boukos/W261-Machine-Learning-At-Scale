{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 02\n",
    "- Submission date: 9/15/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is a race condition in the context of parallel computation? Give an example.**\n",
    "\n",
    "2. **What is MapReduce?**\n",
    "\n",
    "3. **How does it differ from Hadoop?**\n",
    "\n",
    "4. **Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A **race condition** is a situation, in parallel computation, in which the final value can be different depending on the order of parallel processes. For example, it can occur if two threads read and write (after performing some computation) the same variable: it one thread reads and writes the variable, and then the other thread reads and writes that same variable after that, the result will be different than what it would be if the second threads reads the variable before the first one has written it.\n",
    "\n",
    "2. **MapReduce** is a programming model (and an associated implementation) for processing and generating large data sets with a parallel, distributed algorithm on a cluster. An implementation of this model (i.e., a MapReduce program) is composed of a `map` procedure (that performs filtering and sorting) and a `reduce` method (that performs a summary operation).\n",
    "\n",
    "3. (Apache) **Hadoop** uses MapReduce, but is more than that: it is a software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware, whose core consists of MapReduce (for processing) and HDFS (Hadoop Distributed File System; for storage). It is also composed of other modules, apart from those two: Hadoop Common, which contains libraries and utilities needed by other Hadoop modules, and Hadoop YARN, which is a resource-management platform responsible for managing computing resources in clusters and using them for scheduling of users' applications. Hadoop can also refer to the whole ecosystem or collection of additional software packages that can be installed on top of or alongside it, such as Apache Pig, Apache Hive, Apache Spark, Apache Storm, etc.\n",
    "\n",
    "4. As mentioned, Hadoop is based on the MapReduce (or map-and-reduce) programming paradigm, which in turn is based on functional programming and parallel computation. What Hadoop does (in its simplest form) is splitting the input into several chunks, processing those chunks in parallel with a `map` task, and combine the intermediate result of each mapper by means of `reducer` tasks.  **An example is given below, in HW2.1, where the first 10,000 integers, which are shuffled and in string format, are sorted.** Sorting the strings would not work, because they would be sorted according to the leading digit(s) . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2043,\r\n",
      "798,\r\n",
      " 98,\r\n"
     ]
    }
   ],
   "source": [
    "!echo '798,\\n' '98,\\n' '2043,' | sort -k1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". . . so the mappers include leading zeros to each number (in string format) in the portion of the data passed to each one, and the reducer discards those leading zeros (and the sorting is done by the Hadoop framework, no need to code it!).\n",
    "\n",
    "So the command line equivalent to Hadoop code (given the same mapper and reducer, as briefly described above) would be equivalent to:\n",
    "\n",
    "```python\n",
    "!echo '798,\\n' '98,\\n' '2043,' | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```\n",
    "\n",
    "which would sort those three numbers correctly:\n",
    "\n",
    "`98,\n",
    "798,\n",
    "2043`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort in Hadoop MapReduce**\n",
    "\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where `integer` is any integer, and `“NA”` is just the empty string.**\n",
    "\n",
    "**Output: sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "**Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000.**\n",
    "\n",
    "**Write the python Hadoop streaming map-reduce job to perform this sort.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4324', '3955', '4127', '598', '6060', '7684', '849', '6367', '2424', '4939']\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "from random import sample\n",
    "with open('input', 'w') as myfile:\n",
    "    # Sample (without replacement) from 1 to N\n",
    "    integer = sample(range(1, N+1), N) \n",
    "    for i in range(N):\n",
    "        # Add \",\" and an empty string to each integer\n",
    "        myfile.write(str(integer[i])+',\\n')\n",
    "    myfile.close()\n",
    "\n",
    "# Let's check a few of the generated records, to see if they're random\n",
    "with open('input', 'r') as myfile:\n",
    "    text = [word.strip(\",.\") for line in myfile for word in line.split()]\n",
    "print text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Discard the \",\" (keep only the integer=key)\n",
    "    line = line.strip().rstrip(',')\n",
    "    # Convert to string and add leading zeros\n",
    "    integer = str(line).zfill(5)\n",
    "    # Intermediate result (empty value)\n",
    "    print '%s\\t%s' % (integer, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Remove value (empty) keeping only the key and convert to int \n",
    "        # (which discards leading zeros)\n",
    "    integer = int(line.strip())\n",
    "    # Same format as the original input (\",\" and empty string)\n",
    "    print '%s%s' % (integer, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 30751. Stop it first.\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-juanjo-VB.out\n",
      "15/09/15 18:43:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 31165. Stop it first.\n",
      "localhost: datanode running as process 31432. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 31736. Stop it first.\n",
      "15/09/15 18:43:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Hadoop\n",
    "!/usr/local/hadoop/sbin/start-yarn.sh\n",
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:43:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Create new folder\n",
    "!hdfs dfs -mkdir -p /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:43:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f input /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:44:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted sortOutput\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r sortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:44:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:44:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:44:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:44:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:44:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:44:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/15 18:44:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2131440745_0001\n",
      "15/09/15 18:44:13 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:44:13 INFO mapreduce.Job: Running job: job_local2131440745_0001\n",
      "15/09/15 18:44:13 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:44:13 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:44:13 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:44:13 INFO mapred.LocalJobRunner: Starting task: attempt_local2131440745_0001_m_000000_0\n",
      "15/09/15 18:44:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/input:0+58894\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:44:14 INFO mapreduce.Job: Job job_local2131440745_0001 running in uber mode : false\n",
      "15/09/15 18:44:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:44:14 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:44:14 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./mapper.py]\n",
      "15/09/15 18:44:14 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:44:14 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:44:14 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:44:14 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:44:15 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:44:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:16 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 18:44:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:44:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:44:16 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:44:16 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:44:16 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:44:16 INFO mapred.MapTask: bufstart = 0; bufend = 70000; bufvoid = 104857600\n",
      "15/09/15 18:44:16 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "15/09/15 18:44:17 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:44:17 INFO mapred.Task: Task:attempt_local2131440745_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:44:17 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "15/09/15 18:44:17 INFO mapred.Task: Task 'attempt_local2131440745_0001_m_000000_0' done.\n",
      "15/09/15 18:44:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local2131440745_0001_m_000000_0\n",
      "15/09/15 18:44:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:44:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:44:17 INFO mapred.LocalJobRunner: Starting task: attempt_local2131440745_0001_r_000000_0\n",
      "15/09/15 18:44:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:44:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1de56319\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:44:18 INFO reduce.EventFetcher: attempt_local2131440745_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:44:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2131440745_0001_m_000000_0 decomp: 90002 len: 90006 to MEMORY\n",
      "15/09/15 18:44:18 INFO reduce.InMemoryMapOutput: Read 90002 bytes from map-output for attempt_local2131440745_0001_m_000000_0\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 90002, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->90002\n",
      "15/09/15 18:44:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:44:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:44:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:44:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89994 bytes\n",
      "15/09/15 18:44:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: Merged 1 segments, 90002 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: Merging 1 files, 90006 bytes from disk\n",
      "15/09/15 18:44:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:44:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:44:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89994 bytes\n",
      "15/09/15 18:44:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:44:18 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./reducer.py]\n",
      "15/09/15 18:44:18 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:44:18 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:44:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:19 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:44:20 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/15 18:44:20 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "15/09/15 18:44:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:44:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:44:21 INFO mapred.Task: Task:attempt_local2131440745_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:44:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:44:21 INFO mapred.Task: Task attempt_local2131440745_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:44:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2131440745_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/sortOutput/_temporary/0/task_local2131440745_0001_r_000000\n",
      "15/09/15 18:44:21 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "15/09/15 18:44:21 INFO mapred.Task: Task 'attempt_local2131440745_0001_r_000000_0' done.\n",
      "15/09/15 18:44:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local2131440745_0001_r_000000_0\n",
      "15/09/15 18:44:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:44:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:44:21 INFO mapreduce.Job: Job job_local2131440745_0001 completed successfully\n",
      "15/09/15 18:44:21 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=390314\n",
      "\t\tFILE: Number of bytes written=992874\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=117788\n",
      "\t\tHDFS: Number of bytes written=68894\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=70000\n",
      "\t\tMap output materialized bytes=90006\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=90006\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=401\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=58894\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=68894\n",
      "15/09/15 18:44:21 INFO streaming.StreamJob: Output directory: sortOutput\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "    # Forcing number of reducers to be 1\n",
    "!hadoop jar hadoop-streaming*.jar -D mapred.reduce.tasks=1 -mapper mapper.py \\\n",
    "    -reducer reducer.py -input /user/hadoop/dirhw21/input -output sortOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Move output to local (rather than direcly reading its content)\n",
    "#!hdfs dfs -cat sortOutput/part-00000\n",
    "# Delete it from local if a previous version exists\n",
    "!rm ~/Downloads/HW2/part-00000\n",
    "!hdfs dfs -copyToLocal sortOutput/part-00000 ~/Downloads/HW2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\t\r\n",
      "2,\t\r\n",
      "3,\t\r\n",
      "4,\t\r\n",
      "5,\t\r\n",
      "6,\t\r\n",
      "7,\t\r\n",
      "8,\t\r\n",
      "9,\t\r\n",
      "10,\t\r\n",
      "11,\t\r\n",
      "12,\t\r\n",
      "13,\t\r\n",
      "14,\t\r\n",
      "15,\t\r\n",
      "16,\t\r\n",
      "17,\t\r\n",
      "18,\t\r\n",
      "19,\t\r\n",
      "20,\t\r\n"
     ]
    }
   ],
   "source": [
    "# Read first records to check they were sorted\n",
    "!head -20 ~/Downloads/HW2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use multiple reducers, running the following, for example:\n",
    "\n",
    "```python\n",
    "# Hadoop streaming command\n",
    "!hadoop jar hadoop-streaming*.jar -D mapred.reduce.tasks=1 -mapper mapper.py \\\n",
    "    -reducer reducer.py -input /user/hadoop/dirhw21/input -output sortOutput\n",
    "```\n",
    "the first records of output `part-00000` are:\n",
    "\n",
    "`1,\t\n",
    "3,\t\n",
    "5,\t\n",
    "7,\t\n",
    "9,\t\n",
    "10,\t\n",
    "12,\t\n",
    "14,\t\n",
    "16,\t\n",
    "18,\t\n",
    "21,\t\n",
    "23,\t\n",
    "25,\t\n",
    "27,\t\n",
    "29,\t\n",
    "30,\t\n",
    "32,\t\n",
    "34,\t\n",
    "36,\t\n",
    "38,\t`\n",
    "\n",
    "I.e., there are more than one output (as many as reducers), and each one is sorted, but comes from just one part of the mappers, so it does not necessarily have to include a consecutive subset of records.\n",
    "\n",
    "Say there are 4 mappers whose inputs are `<4,>`, `<11,>`, `<7,>`, `<2,>`, `<6,>`, `<10,>`, `<8,>`, `<5,>`, `<12,>`, `<3,>`, `<9,>`, `<1,>`. If the outputs of the first 2 mappers are processed by a reducer, and the outputs of the remaining 2 mappers are processed by a second reducer, their outputs would be `<2,>`, `<4,>`, `<6,>`, `<7,>`, `<10,>`, `<11,>`, and `<1,>`, `<3,>`, `<5,>`, `<8,>`, `<9,>`, `<12,>`, respectively.\n",
    "\n",
    "**We would have to apply a 2nd MapReduce layer, this one with only one Reducer at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.**\n",
    "\n",
    "**To do so, make sure that**\n",
    "\n",
    "- **mapper.py counts all occurrences of a single word, and**\n",
    "- **reducer.py collates the counts of the single word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword = env_vars['findword']\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "word_count = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    for w in WORD_RE.findall(line):\n",
    "        if findword.lower() == w.lower():\n",
    "            word_count += 1\n",
    "print findword + '\\t' + str(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sum_words = 0\n",
    "for line in sys.stdin:\n",
    "    key_value = line.split('\\t')\n",
    "    # The key is the single word we're counting\n",
    "    key = key_value[0]\n",
    "    # And each value, its count from a mapper\n",
    "    value = key_value[1]\n",
    "    sum_words += int(value)\n",
    "print key + '\\t' + str(sum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f enronemail_1h.txt /user/hadoop/dirhw21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:44:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted countWordEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r countWordEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:44:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:44:57 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:44:57 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:44:57 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:44:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:44:57 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:44:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local612556684_0001\n",
      "15/09/15 18:44:58 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:44:58 INFO mapreduce.Job: Running job: job_local612556684_0001\n",
      "15/09/15 18:44:58 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:44:58 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:44:59 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:44:59 INFO mapred.LocalJobRunner: Starting task: attempt_local612556684_0001_m_000000_0\n",
      "15/09/15 18:44:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:44:59 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:44:59 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./mapper.py]\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:44:59 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:44:59 INFO mapreduce.Job: Job job_local612556684_0001 running in uber mode : false\n",
      "15/09/15 18:44:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 18:45:00 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:45:00 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:45:00 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:45:00 INFO mapred.MapTask: bufstart = 0; bufend = 14; bufvoid = 104857600\n",
      "15/09/15 18:45:00 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "15/09/15 18:45:00 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:45:00 INFO mapred.Task: Task:attempt_local612556684_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 18:45:00 INFO mapred.Task: Task 'attempt_local612556684_0001_m_000000_0' done.\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local612556684_0001_m_000000_0\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: Starting task: attempt_local612556684_0001_r_000000_0\n",
      "15/09/15 18:45:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:45:00 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2e62ba82\n",
      "15/09/15 18:45:00 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:45:00 INFO reduce.EventFetcher: attempt_local612556684_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:45:00 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local612556684_0001_m_000000_0 decomp: 18 len: 22 to MEMORY\n",
      "15/09/15 18:45:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:45:00 INFO reduce.InMemoryMapOutput: Read 18 bytes from map-output for attempt_local612556684_0001_m_000000_0\n",
      "15/09/15 18:45:00 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18\n",
      "15/09/15 18:45:00 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:45:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:45:00 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:45:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:45:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5 bytes\n",
      "15/09/15 18:45:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 18 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:45:01 INFO reduce.MergeManagerImpl: Merging 1 files, 22 bytes from disk\n",
      "15/09/15 18:45:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:45:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:45:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5 bytes\n",
      "15/09/15 18:45:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:45:01 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./reducer.py]\n",
      "15/09/15 18:45:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:45:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:45:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:45:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/09/15 18:45:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:45:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:45:01 INFO mapred.Task: Task:attempt_local612556684_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:45:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:45:01 INFO mapred.Task: Task attempt_local612556684_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:45:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local612556684_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/countWordEnron/_temporary/0/task_local612556684_0001_r_000000\n",
      "15/09/15 18:45:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "15/09/15 18:45:01 INFO mapred.Task: Task 'attempt_local612556684_0001_r_000000_0' done.\n",
      "15/09/15 18:45:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local612556684_0001_r_000000_0\n",
      "15/09/15 18:45:01 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:45:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:45:01 INFO mapreduce.Job: Job job_local612556684_0001 completed successfully\n",
      "15/09/15 18:45:01 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210372\n",
      "\t\tFILE: Number of bytes written=720126\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=14\n",
      "\t\tMap output materialized bytes=22\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=22\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=177\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "15/09/15 18:45:01 INFO streaming.StreamJob: Output directory: countWordEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar hadoop-streaming*.jar -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance -input /user/hadoop/dirhw21/enronemail_1h.txt \\\n",
    "    -output countWordEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:45:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat countWordEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result than in HW1, because I'm including the subject of the emails, and all occurrences in a single email are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that**\n",
    "\n",
    "- **mapper.py**\n",
    "- **reducer.py**\n",
    "\n",
    "**performs a single word multinomial Naive Bayes classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword = env_vars['findword']\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    word_count = WORD_RE.findall(content).count(findword)\n",
    "    print findword + '\\t' + str(word_count) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is the single word we're counting\n",
    "    word = key_values[0]\n",
    "    word_count.append(int(key_values[1]))\n",
    "    ID.append(key_values[2])\n",
    "    TRUTH.append(int(key_values[3]))\n",
    "\n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "# Total count of word in spam emails\n",
    "total_word_spam = sum([x*y for (x,y) in zip(TRUTH,word_count)]) \n",
    "# Total count of word in ham emails\n",
    "total_word_ham = sum(word_count) - total_word_spam\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = float(total_word_ham + 1) / (total_word_ham + 1)\n",
    "prob_word_spam = float(total_word_spam + 1) / (total_word_spam + 1)\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*len(TRUTH)\n",
    "for i in range(len(TRUTH)): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + word_count[i]*log(prob_word_ham,10)\n",
    "    prob_spam_word = log(prob_spam,10) + word_count[i]*log(prob_word_spam,10)\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:46:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:46:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted singleNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r singleNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:46:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:46:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:46:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:46:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:46:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:46:24 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:46:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local259752051_0001\n",
      "15/09/15 18:46:25 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:46:25 INFO mapreduce.Job: Running job: job_local259752051_0001\n",
      "15/09/15 18:46:25 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:46:25 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:46:26 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:46:26 INFO mapred.LocalJobRunner: Starting task: attempt_local259752051_0001_m_000000_0\n",
      "15/09/15 18:46:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:46:26 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:46:26 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./mapper.py]\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:46:26 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:46:26 INFO mapreduce.Job: Job job_local259752051_0001 running in uber mode : false\n",
      "15/09/15 18:46:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:46:27 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:46:27 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:46:27 INFO mapred.MapTask: bufstart = 0; bufend = 3772; bufvoid = 104857600\n",
      "15/09/15 18:46:27 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 18:46:27 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:46:27 INFO mapred.Task: Task:attempt_local259752051_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 18:46:27 INFO mapred.Task: Task 'attempt_local259752051_0001_m_000000_0' done.\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local259752051_0001_m_000000_0\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: Starting task: attempt_local259752051_0001_r_000000_0\n",
      "15/09/15 18:46:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:46:27 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72650926\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:46:27 INFO reduce.EventFetcher: attempt_local259752051_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:46:27 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local259752051_0001_m_000000_0 decomp: 3974 len: 3978 to MEMORY\n",
      "15/09/15 18:46:27 INFO reduce.InMemoryMapOutput: Read 3974 bytes from map-output for attempt_local259752051_0001_m_000000_0\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3974, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3974\n",
      "15/09/15 18:46:27 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:46:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:46:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 3974 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: Merging 1 files, 3978 bytes from disk\n",
      "15/09/15 18:46:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:46:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:46:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3961 bytes\n",
      "15/09/15 18:46:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:46:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:46:27 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./reducer.py]\n",
      "15/09/15 18:46:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:46:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:46:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:46:28 INFO mapred.Task: Task:attempt_local259752051_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:46:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:46:28 INFO mapred.Task: Task attempt_local259752051_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:46:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_local259752051_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/singleNBEnron/_temporary/0/task_local259752051_0001_r_000000\n",
      "15/09/15 18:46:28 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 18:46:28 INFO mapred.Task: Task 'attempt_local259752051_0001_r_000000_0' done.\n",
      "15/09/15 18:46:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local259752051_0001_r_000000_0\n",
      "15/09/15 18:46:28 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:46:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:46:28 INFO mapreduce.Job: Job job_local259752051_0001 completed successfully\n",
      "15/09/15 18:46:29 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218284\n",
      "\t\tFILE: Number of bytes written=731990\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2692\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=3772\n",
      "\t\tMap output materialized bytes=3978\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=3978\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2692\n",
      "15/09/15 18:46:29 INFO streaming.StreamJob: Output directory: singleNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar hadoop-streaming*.jar -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance -input /user/hadoop/dirhw21/enronemail_1h.txt \\\n",
    "    -output singleNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:46:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0018.2003-12-18.GP\t1\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "Training Error\t0.44\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat singleNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I also included an additional line in the output with the training error, as defined in HW1. It's the same value that I got in HW1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#HW2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results**\n",
    "\n",
    "**To do so, make sure that**\n",
    "\n",
    "- **mapper.py **\n",
    "- **reducer.py**\n",
    "\n",
    "**performs the multiple-word multinomial Naive Bayes classification via the chosen list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "findword_list = env_vars['findword'].split(',')\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    for findword in findword_list:\n",
    "        word_count = WORD_RE.findall(content).count(findword)\n",
    "        print findword + '\\t' + str(word_count) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "word = []\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is ONE of the words we're counting\n",
    "    if len(word) != 0:\n",
    "        if word[-1] != key_values[0]:\n",
    "            word.append(key_values[0])\n",
    "    else:\n",
    "        word.append(key_values[0])\n",
    "    word_count.append(int(key_values[1]))\n",
    "    # ID and TRUTH are replicated for each word in the vocabulary\n",
    "        # so we just need to keep track of them once\n",
    "    if len(word) == 1:\n",
    "        ID.append(key_values[2])\n",
    "        TRUTH.append(int(key_values[3]))\n",
    "\n",
    "# The lists above will have a length equal to\n",
    "    # number_different_words_in_vocab * number_emails_in_dataset\n",
    "vocab_size = len(word)\n",
    "num_emails = len(TRUTH)\n",
    "# Reshape the list word_count into a 2-D numpy array\n",
    "word_count = np.array(word_count).reshape(len(word), num_emails)\n",
    "\n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "total_word_spam = [0]*len(word)\n",
    "total_word_ham = [0]*len(word)\n",
    "for i,w in enumerate(word):\n",
    "    # Total count of word w in spam emails\n",
    "    total_word_spam[i] = sum([x*y for (x,y) in zip(TRUTH,word_count[i])]) \n",
    "    # Total count of word w in ham emails\n",
    "    total_word_ham[i] = sum(word_count[i]) - total_word_spam[i]\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham)+vocab_size) for x \\\n",
    "                 in total_word_ham]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam)+vocab_size) for \\\n",
    "                  x in total_word_spam]\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*num_emails\n",
    "for i in range(num_emails): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count[:,i],prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count[:,i],prob_word_spam)])\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS[i] = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:46:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:46:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted multiNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r multiNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:46:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:46:54 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:46:54 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:46:54 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:46:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:46:55 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:46:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1498805937_0001\n",
      "15/09/15 18:46:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:46:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:46:57 INFO mapreduce.Job: Running job: job_local1498805937_0001\n",
      "15/09/15 18:46:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:46:58 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:46:58 INFO mapred.LocalJobRunner: Starting task: attempt_local1498805937_0001_m_000000_0\n",
      "15/09/15 18:46:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/15 18:46:58 INFO mapreduce.Job: Job job_local1498805937_0001 running in uber mode : false\n",
      "15/09/15 18:46:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:46:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:46:58 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./mapper.py]\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:46:58 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: R/W/S=100/102/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:46:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:46:59 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:46:59 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:46:59 INFO mapred.MapTask: bufstart = 0; bufend = 11916; bufvoid = 104857600\n",
      "15/09/15 18:46:59 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213200(104852800); length = 1197/6553600\n",
      "15/09/15 18:46:59 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:46:59 INFO mapred.Task: Task:attempt_local1498805937_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/15 18:46:59 INFO mapred.Task: Task 'attempt_local1498805937_0001_m_000000_0' done.\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1498805937_0001_m_000000_0\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:46:59 INFO mapred.LocalJobRunner: Starting task: attempt_local1498805937_0001_r_000000_0\n",
      "15/09/15 18:46:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/15 18:46:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@28cccdf4\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:47:00 INFO reduce.EventFetcher: attempt_local1498805937_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:47:00 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1498805937_0001_m_000000_0 decomp: 12518 len: 12522 to MEMORY\n",
      "15/09/15 18:47:00 INFO reduce.InMemoryMapOutput: Read 12518 bytes from map-output for attempt_local1498805937_0001_m_000000_0\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 12518, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->12518\n",
      "15/09/15 18:47:00 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:47:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:47:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:47:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: Merged 1 segments, 12518 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: Merging 1 files, 12522 bytes from disk\n",
      "15/09/15 18:47:00 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:47:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:47:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 12505 bytes\n",
      "15/09/15 18:47:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:47:00 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/./reducer.py]\n",
      "15/09/15 18:47:00 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:47:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:47:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:47:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:47:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:47:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:47:01 INFO streaming.PipeMapRed: Records R/W=300/1\n",
      "15/09/15 18:47:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:47:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:47:02 INFO mapred.Task: Task:attempt_local1498805937_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:47:02 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:47:02 INFO mapred.Task: Task attempt_local1498805937_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:47:02 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1498805937_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/multiNBEnron/_temporary/0/task_local1498805937_0001_r_000000\n",
      "15/09/15 18:47:02 INFO mapred.LocalJobRunner: Records R/W=300/1 > reduce\n",
      "15/09/15 18:47:02 INFO mapred.Task: Task 'attempt_local1498805937_0001_r_000000_0' done.\n",
      "15/09/15 18:47:02 INFO mapred.LocalJobRunner: Finishing task: attempt_local1498805937_0001_r_000000_0\n",
      "15/09/15 18:47:02 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:47:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:47:02 INFO mapreduce.Job: Job job_local1498805937_0001 completed successfully\n",
      "15/09/15 18:47:02 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=235372\n",
      "\t\tFILE: Number of bytes written=760490\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2691\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=300\n",
      "\t\tMap output bytes=11916\n",
      "\t\tMap output materialized bytes=12522\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=12522\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=212\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2691\n",
      "15/09/15 18:47:02 INFO streaming.StreamJob: Output directory: multiNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar hadoop-streaming*.jar -mapper mapper.py -reducer reducer.py -cmdenv \\\n",
    "    findword=assistance,valium,enlargementWithATypo -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output multiNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:47:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "Training Error\t0.4\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat multiNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I also included an additional line in the output with the training error, as defined in HW1. It's (almost) the same value that I got in HW1.4 (actually, a bit better: 0.40 now, while it was 0.41 wiht the poor man's implementation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

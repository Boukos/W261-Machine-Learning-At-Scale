{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 01\n",
    "- Submission date: 9/15/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define big data. Provide an example of a big data problem in your domain of expertise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Big Data* refers to data sets so large or complex that traditional data-processing applications are inadequate to handle them (mainly in terms of processing, storing, or transfering them, but also with regards to data curation, search, visualization, security, privacy, and so on).\n",
    "\n",
    "I come from the B2B sales domain (test & measurement solutions for energy, transportation, optics & electronics, etc.), so the amount of transactions, even in a world basis, were pretty low (and hence easy to handle). So I will mention an example from Marketing & Advertising, one of the areas I feel more attracted: *sentiment analysis*, applied to branding. Say we want to know the feelings and opinions of a given company's customers, based on what those customers tweet: we would want to know not only how many times the brand is mentioned (that would be a mere word count, although within millions of tweets per day) and by whom, but also the content—the meaning—of the tweets containing that brand's name. That involves a very complex task (natural language processing, in multiple languages) over a tremendous amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If we use the test set $T$ to select a model, from a collection of them built from a training set, we might also refer to that dataset as *validation set* or *development set*; the term *test set* commonly refers to a third dataset used not to select but to assess or evaluate the model that is finally chosen.)\n",
    "\n",
    "The expected prediction error in the test dataset $T$ could be decomposed as the sum of some irreducible error, the squared bias of the model $g$ we've built (based on training data), and the variance:\n",
    "\n",
    "$$E\\left[\\left(g(x^*)-y^* \\right )^2 \\right ]=E\\left[\\left(y^*-f(x^*)\\right )^2 \\right ]+\\left(E\\left[g(x^*)-f(x^*)\\right] \\right )^2+E\\left[\\left(g(x^*)-E\\left(g(x^*) \\right ) \\right )^2 \\right ]$$\n",
    "\n",
    "If we were facing a **theoretical example** (a **deterministic process** whose function $f$ we know; something like *force (y) = mass (x) × acceleration* or *space (y) =speed × time (x)*), we would be given that $f$ function, as well as the $(x^*, y^*)$ points in $T$ and the $g$ function given by each of the five polynomial regression models, so we just would need to apply the formula above for each of those models, hence obtaining the irreducible error, the bias, and the variance for each model  \n",
    "\n",
    "> To estimate the variance we need to calculate $E[g(x^*)]$, which implies different models—versions of $g$—depending on the points $x^*$. To do that we would need to split the test set in some datasets, for example using **bootstrapping**. This is commonly done with a training set, when we're builiding the models (with the test set we just assess the overall prediction error).\n",
    "\n",
    "The model we should select (the best among the five) is the one that yields the lower prediction error (the sum of those three components) . . . But if we knew $f$—the mechanism that generates $y$ from $x$—, there would be no need for a model $g$.\n",
    "\n",
    "In a **practical case** we do not know $f$, and hence we cannot estimate neither the noise nor the bias (because $f$ appears in both expressions), i.e., we cannot isolate the irreducible error ($\\varepsilon^*=y^*-f(x^*)$) or the difference between the true function $f$ and its approximation $g$ (but we can estimate the sum of those two terms as the difference between the expected prediction error and the variance, as explained later).\n",
    "\n",
    "That wouldn't matter, anway; the information we have (the $g$ functions given by each model, as well as the points $(x^*, y^*) \\in T$ ) would be enough to select the best model: as mentioned, we are interested in the model that minimizes the expected prediction error (i.e., the one that gives values of $g(x^*)$ closer to the values of $y^*$, on average), so we just have to estimate that error (the MSE) for each one of the five $g$ functions, and select the one that yields the lower value.\n",
    "\n",
    "We could also estimate the variance, because it only depends on $g(x^*)$, which is given by each model (that we've supposedly built with some training dataset; as mentioned above, splitting the training set in several datasets would allow us to estimate $E[g(x^*)]$, so we would have to do the same with the test set if we want to estimate the variance for it). This way we might estimate the sum of the irreducible error and the squared bias as the difference between the prediction error and the variance. And the reason why the expected prediction error reaches a minimum is because:\n",
    "\n",
    "- the variance increases with the complexity of the model, i.e., with the degree of the polynomial function,\n",
    "- the irreducible error is independent of the model we use (its mean value is constant for a given test set, regardless of the function $g$ that we use), and\n",
    "- the bias (and hence its squared value) decreases as the complexity of the model increases.\n",
    "\n",
    "The so-called *bias vs. variance tradeoff* implies that, as the complexity of the model increases (the higher the degree of the polynomial function, in our case), one component of the prediction error—the bias—decreases, another one—the variance—increases, and the third one—the irreducible noise—remains constant, so there may be a minimum (an optimal complexity level, so to speak) where the bias has already decreased enough while the variance has not increased too much yet (of course, that minimum might be outside the 1–5 range we're considering in this example; in that case we would not reach it, and the polynomial function of degree 5 would be the best approximation; or there may not be such a minimum—if the variance increases with the model complexity at a higher rate than the bias decreases, in which case the best degree would be 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### HW1.1 ###\n",
    "# Read through the provided control script (pNaiveBayes.sh) and all of its \n",
    "    # comments.\n",
    "# When you are comfortable with their purpose and function, respond to the \n",
    "    # remaining homework questions below.\n",
    "# A simple cell in the notebook with a print statmement with a \"done\" string \n",
    "    # will suffice here.\n",
    "\n",
    "def hw1_1():\n",
    "    print 'done'\n",
    "\n",
    "hw1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide a mapper/reducer pair that, when executed by `pNaiveBayes.sh` will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started with the same approach used in Quiz 1.12.2, with different cells for each part of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count_words = 0 # count of occurrences of the word\n",
    "count_emails = 0 # count of emails containing the word\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open(filename, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        # Count IF the word appears in a line (email)\n",
    "        if re.search(findword,line,re.IGNORECASE):\n",
    "            count_emails += 1\n",
    "        # Count HOW MANY TIMES the word appears in a line (email)\n",
    "        for w in WORD_RE.findall(line):\n",
    "            if findword.lower() == w.lower():\n",
    "                count_words += 1                \n",
    "print str(count_words) + '\\t' + str(count_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I counted not only the occurrences of the word but also the emails in which that word appears (for instance, `\"assistance\"` appears 10 times, in 8 emails: it appears 3 times in one of the emails).\n",
    "\n",
    "I could also have counted the occurrences of the word **per email**, as well as printed the category of each email, hence being able to re-use this mapper in HW1.3, but I preferred to keep things simple here, and increase the complexity of all mapper/reducer pairs in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify permissions of the mapper.py file\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "files = sys.argv[1:] #Accept several arguments (same as mapper outputs)\n",
    "sum_words = 0 # count of occurrences of the word\n",
    "sum_emails = 0 # count of emails containing the word\n",
    "for filename in files:\n",
    "    with open (filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        words_emails = line.split('\\t')\n",
    "        sum_words += int(words_emails[0])\n",
    "        sum_emails += int(words_emails[1])\n",
    "print str(sum_words) + '\\t' + str(sum_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify permissions of the reducer.py file\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run pNaiveBayes.sh with word 'assistance' and an arbitrary number of partitions\n",
    "!./pNaiveBayes.sh 10 'assistance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occcurrences of the word \"assistance\":   10\n",
      "Number of emails containing the word \"assistance\": 8\n"
     ]
    }
   ],
   "source": [
    "### HW1.2 ###\n",
    "# Report results\n",
    "with open ('enronemail_1h.txt.output', \"r\") as f:\n",
    "    line = f.readline()\n",
    "    words_emails = line.split('\\t')\n",
    "    sum_words = int(words_emails[0])\n",
    "    sum_emails = int(words_emails[1])\n",
    "print 'Number of occcurrences of the word \\\"{0}\\\":   {1}'.format('assistance', \n",
    "    sum_words)\n",
    "print 'Number of emails containing the word \\\"{0}\\\": {1}'.format('assistance', \n",
    "    sum_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then I adapted the code to put all of it in a function (called **hw1_2**) that accepts a number of partitions and a word as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occcurrences of the word \"assistance\":   10\n",
      "Number of emails containing the word \"assistance\": 8\n"
     ]
    }
   ],
   "source": [
    "### HW1.2 ###\n",
    "def hw1_2(n, word):\n",
    "    # Create mapper.py\n",
    "        # which counts both occurrences of the word and mails containing that word\n",
    "    with open('mapper.py', 'w') as f:\n",
    "        f.write('#!/usr/bin/python\\n')\n",
    "        f.write('import sys\\n')\n",
    "        f.write('import re\\n')\n",
    "        f.write('count_words = 0\\n')\n",
    "        f.write('count_emails = 0\\n')\n",
    "        f.write('WORD_RE = re.compile(r\"[\\w'']+\")\\n')\n",
    "        f.write('filename = sys.argv[1]\\n')\n",
    "        f.write('findword = sys.argv[2]\\n')\n",
    "        f.write('with open(filename, \"r\") as f:\\n')\n",
    "        f.write('\\tfor line in f.readlines():\\n')\n",
    "        f.write('\\t\\tif re.search(findword,line,re.IGNORECASE):\\n')\n",
    "        f.write('\\t\\t\\tcount_emails += 1\\n')        \n",
    "        f.write('\\t\\tfor w in WORD_RE.findall(line):\\n')\n",
    "        f.write('\\t\\t\\tif findword.lower() == w.lower():\\n')\n",
    "        f.write('\\t\\t\\t\\tcount_words +=1\\n')\n",
    "        f.write('print str(count_words) + '\"'\\t'\"' + str(count_emails)\\n')\n",
    "    import subprocess\n",
    "    output = subprocess.check_output(['bash','-c', 'chmod a+x mapper.py'])\n",
    "    \n",
    "    # Create reducer.py\n",
    "    with open('reducer.py', 'w') as f:\n",
    "        f.write('#!/usr/bin/python\\n')\n",
    "        f.write('import sys\\n')\n",
    "        f.write('files = sys.argv[1:]\\n')\n",
    "        f.write('sum_words = 0\\n')\n",
    "        f.write('sum_emails = 0\\n')\n",
    "        f.write('for filename in files:\\n')\n",
    "        f.write('\\twith open (filename, \"r\") as f:\\n')\n",
    "        f.write('\\t\\tline = f.readline()\\n')\n",
    "        f.write('\\t\\twords_emails = line.split(''\\t'')\\n')\n",
    "        f.write('\\t\\tsum_words += int(words_emails[0])\\n')\n",
    "        f.write('\\t\\tsum_emails += int(words_emails[1])\\n')\n",
    "        f.write('print str(sum_words) + '\"'\\t'\"' + str(sum_emails)\\n')\n",
    "    output = subprocess.check_output(['bash','-c', 'chmod a+x reducer.py'])\n",
    "    \n",
    "    # Call pNaiveBayes.sh\n",
    "    bashCommand = './pNaiveBayes.sh ' + str(n) + ' ' + word\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    \n",
    "    # Report results\n",
    "    with open ('enronemail_1h.txt.output', \"r\") as myfile:\n",
    "        line = myfile.readline()\n",
    "        words_emails = line.split('\\t')\n",
    "        sum_words = int(words_emails[0])\n",
    "        sum_emails = int(words_emails[1])\n",
    "    print 'Number of occcurrences of the word \\\"{0}\\\":   {1}'.format('assistance', \n",
    "        sum_words)\n",
    "    print 'Number of emails containing the word \\\"{0}\\\": {1}'.format('assistance', \n",
    "        sum_emails)\n",
    "    \n",
    "# Call the function with word 'assistance' and an arbitrary number of partitions\n",
    "hw1_2(10, 'assistance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remaining of the assignment, I kept the definition of the `mapper` and `reducer` functions outside the definition of the function `HW1.x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide a mapper/reducer pair that, when executed by `pNaiveBayes.sh`, will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that**\n",
    "\n",
    "- **`mapper.py` is same as in part (2), and**\n",
    "- **`reducer.py` performs a single word Naive Bayes classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I modified `mapper.py` to include occurrences of the word per email, as well as the category of each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "total_ham = 0 # count of ham emails\n",
    "total_spam = 0 # count of spam emails\n",
    "total_word_ham = 0 # count of word in ham emails\n",
    "total_word_spam = 0 # count of word in spam emails\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1] # 1st argument is a file portion\n",
    "findword = sys.argv[2] # 2nd argument is a single word\n",
    "with open(filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines(): # for each line/email in the file\n",
    "        word_count = 0 # count of word in the email\n",
    "        id = line.split(\"\\t\")[0]\n",
    "        category = line.split(\"\\t\")[1]\n",
    "        content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "            # We search the word in both the subject and the content\n",
    "                # because one or the other may not exist, but the way the data are\n",
    "                # stored we don't know which one may be missing\n",
    "        for w in WORD_RE.findall(content):\n",
    "            if findword.lower() == w.lower():\n",
    "                word_count += 1\n",
    "        if int(category) == 0: # increase count of emails in ham or spam \n",
    "            # as well as the occurrences of the word in each category\n",
    "            total_ham += 1\n",
    "            total_word_ham += word_count\n",
    "        elif int(category) == 1:\n",
    "            total_spam += 1\n",
    "            total_word_spam += word_count\n",
    "        print id + '\\t' + category + '\\t' + str(word_count)\n",
    "            # Output one line per mail, with id, category, and occurrences of word\n",
    "# Print 2 additional lines, with count of emails and total occurrences of the \n",
    "    # word, in each category\n",
    "print 'ham' + '\\t' + str(total_ham) + '\\t' + str(total_word_ham)\n",
    "print 'spam' + '\\t' + str(total_spam) + '\\t' + str(total_word_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 2 lines that each mapper outputs could have been easily calculated in the reducer using all the previous lines in each mapper's output.\n",
    "\n",
    "But it's only that information (the total count of occurrences of a fixed vocabulary in each partition) which is needed to train the Naive Bayes model, so that would be enough for this section and the next (HW1.4): the information about every single email is also passed so the reducer can evaluate the training set that has been used to build the model.\n",
    "\n",
    "Of course, in HW1.5—where we have to find all words present in all emails—these last 2 lines that summarize each mapper's output will not be necessary, and the reducer will have to combine the words found in the emails of each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "files = sys.argv[1:] #Accept several arguments (same as mapper outputs)\n",
    "total_ham = 0 # total count of ham emails\n",
    "total_spam = 0 # total count of spam emails\n",
    "total_word_ham = 0 # total count of word in ham emails\n",
    "total_word_spam = 0 # total count of word in spam emails\n",
    "email_list = []\n",
    "for filename in files: # for each file passed to the reducer\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f: # place all lines in each mapper's output into a list\n",
    "            email_list.append(line)\n",
    "    # Extract last 2 entries of the list, which correspond to totals\n",
    "    spam = email_list.pop()\n",
    "    spam = spam.split('\\t')\n",
    "    ham = email_list.pop()\n",
    "    ham = ham.split('\\t')\n",
    "    # Increase total count of emails and word occurrences per category\n",
    "    total_ham += int(ham[1])\n",
    "    total_word_ham += int(ham[2])\n",
    "    total_spam += int(spam[1])\n",
    "    total_word_spam += int(spam[2])\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam) # PRIORS\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = float(total_word_ham + 1) / (total_word_ham + 1)\n",
    "prob_word_spam = float(total_word_spam + 1) / (total_word_spam + 1)\n",
    "\n",
    "# Assess classification with the training set \n",
    "errors = 0 # count of misclassification errors\n",
    "for i in range(len(email_list)): # for each line in the mappers' outputs \n",
    "    # Extract category\n",
    "    category = int(email_list[i].split(\"\\t\")[1])\n",
    "    # Extract count of occurrences of the word\n",
    "    word_count = int(email_list[i].split(\"\\t\")[2])\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + word_count*log(prob_word_ham,10)\n",
    "    prob_spam_word = log(prob_spam,10) + word_count*log(prob_word_spam,10)\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        predicted_category = 1\n",
    "    else:\n",
    "        predicted_category = 0\n",
    "    # Output category and predicted category for each email\n",
    "    print email_list[i].split(\"\\t\")[0] + '\\t' + str(category) + '\\t' + \\\n",
    "        str(predicted_category) \n",
    "    if predicted_category != category:\n",
    "        # Increase count of errors the category predicted is wrong\n",
    "        errors += 1\n",
    "training_error = float(errors) / (total_ham+total_spam)\n",
    "print training_error # output the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of the classifier only using the word \"assistance\": 0.44\n"
     ]
    }
   ],
   "source": [
    "### HW1.3 ###\n",
    "# Report results\n",
    "def hw1_3(n, word):\n",
    "    # Call pNaiveBayes.sh\n",
    "    bashCommand = './pNaiveBayes.sh ' + str(n) + ' ' + word\n",
    "    import subprocess\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    \n",
    "    # Report results\n",
    "        # Just the last line, containing the training error\n",
    "    training_error = !(tail -n 1 enronemail_1h.txt.output | cut -d' ' -f1)\n",
    "    print 'Training error of the classifier only using the word \\\"{0}\\\": {1}'.\\\n",
    "        format('assistance', training_error[0])\n",
    "    \n",
    "# Call the function with word 'assistance' and an arbitrary number of partitions\n",
    "hw1_3(10, 'assistance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide a mapper/reducer pair that, when executed by `pNaiveBayes.sh`, will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results. To do so, make sure that**\n",
    "\n",
    "- **`mapper.py` counts all occurrences of a list of words, and**\n",
    "- **`reducer.py` performs the multiple-word Naive Bayes classification via the chosen list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from operator import add\n",
    "total_ham = 0 # count of ham emails\n",
    "total_spam = 0 # count of spam emails\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1] # 1st argument is a file portion\n",
    "findwords = sys.argv[2] # 2nd argument is a list of words\n",
    "findwords = findwords.split(' ')\n",
    "vocab_size = len(findwords) # (fixed) size of the vocabulary: \n",
    "    # the count of words in the list\n",
    "total_word_ham = [0] *  vocab_size # count of each word in ham emails\n",
    "total_word_spam = [0] * vocab_size # count of each word in spam emails\n",
    "with open(filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines(): # for each line/email in the file\n",
    "        id = line.split(\"\\t\")[0]\n",
    "        word_count= [0] * vocab_size\n",
    "        category = line.split(\"\\t\")[1]\n",
    "        content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "            # We search the words in both the subject and the content\n",
    "                # because one or the other may not exist, but the way the data are  \n",
    "                # stored we don't know which one may be missing\n",
    "        # For each word in the list, find its occurrences\n",
    "        for i in range(vocab_size):\n",
    "            for w in WORD_RE.findall(content):\n",
    "                if findwords[i].lower() == w.lower():\n",
    "                    word_count[i] += 1\n",
    "        if int(category) == 0: # increase count of emails in ham or spam \n",
    "            # as well as the occurrences of each word in each category\n",
    "            total_ham += 1\n",
    "            total_word_ham = map(add, total_word_ham, word_count)\n",
    "        elif int(category) == 1:\n",
    "            total_spam += 1\n",
    "            total_word_spam = map(add, total_word_spam, word_count)\n",
    "        word_count_str = [str(x) for x in word_count]\n",
    "        print id + '\\t' + category + '\\t' + '\\t'.join(word_count_str)\n",
    "            # Output one line per mail, with id, category, and occurrences of each\n",
    "                # word\n",
    "# Print 2 additional lines, with count of emails and total occurrences of each \n",
    "    # word, in each category, plus total count of words in the list\n",
    "total_word_ham_str = [str(x) for x in total_word_ham] \n",
    "total_word_ham_str.append(str(sum(total_word_ham)))\n",
    "total_word_spam_str = [str(x) for x in total_word_spam]\n",
    "total_word_spam_str.append(str(sum(total_word_spam)))\n",
    "print 'ham' + '\\t' + str(total_ham) + '\\t' + '\\t'.join(total_word_ham_str)\n",
    "print 'spam' + '\\t' + str(total_spam) + '\\t' + '\\t'.join(total_word_spam_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from math import log\n",
    "from operator import add\n",
    "files = sys.argv[1:] #Accept several arguments (same as mapper outputs)\n",
    "total_ham = 0 # total count of ham emails\n",
    "total_spam = 0 # total count of spam emails\n",
    "\n",
    "email_list = []\n",
    "# Get the size of the vocabulary used from the first line from the first mapper's \n",
    "    # output\n",
    "with open(sys.argv[1], \"r\") as myfile:\n",
    "    first_line = myfile.readline()\n",
    "    vocab_size = len(first_line.split('\\t')) - 2 # exclude id and category\n",
    "\n",
    "total_word_ham = [0] * vocab_size # total count of word in ham emails\n",
    "total_word_spam = [0] * vocab_size # total count of word in spam emails\n",
    "\n",
    "for filename in files: # for each file passed to the reducer\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f: # place all lines in each mapper's output into a list\n",
    "            email_list.append(line)\n",
    "    # Extract last 2 entries of the list, which correspond to totals\n",
    "    spam = email_list.pop()\n",
    "    spam = [int(x) for x in spam.split('\\t')[1:]]\n",
    "    ham = email_list.pop()\n",
    "    ham = [int(x) for x in ham.split('\\t')[1:]]\n",
    "    # Increase total count of emails and word occurrences per category\n",
    "    total_ham += int(ham[0])\n",
    "    total_word_ham = map(add, total_word_ham, ham[1:(1+vocab_size)])\n",
    "    total_spam += int(spam[0])\n",
    "    total_word_spam = map(add, total_word_spam, spam[1:(1+vocab_size)])\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham)+vocab_size) for x \\\n",
    "                 in total_word_ham]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam)+vocab_size) for \\\n",
    "                  x in total_word_spam]\n",
    "\n",
    "# Assess classification with the training set \n",
    "errors = 0 # count of misclassification errors\n",
    "for i in range(len(email_list)): # for each line in the mappers' outputs\n",
    "    category = int(email_list[i].split(\"\\t\")[1]) # extract category\n",
    "    # Extract count of occurrences of each word\n",
    "    word_count = [int(x) for x in email_list[i].split(\"\\t\")[2:]]\n",
    "     \n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count,prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(word_count,prob_word_spam)])\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        predicted_category = 1\n",
    "    else:\n",
    "        predicted_category = 0\n",
    "    # Output category and predicted category for each email\n",
    "    print email_list[i].split(\"\\t\")[0] + '\\t' + str(category) + '\\t' + \\\n",
    "        str(predicted_category) \n",
    "    if predicted_category != category:\n",
    "        # Increase count of errors the category predicted is wrong\n",
    "        errors += 1\n",
    "training_error = float(errors) / (total_ham+total_spam)\n",
    "print training_error # output the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of the classifier only using the word \"assistance\": 0.41\n"
     ]
    }
   ],
   "source": [
    "### HW1.4 ###\n",
    "# Report results\n",
    "def hw1_4(n, list_of_words):\n",
    "    # Call pNaiveBayes.sh\n",
    "    bashCommand = './pNaiveBayes.sh ' + str(n) + ' \\'' + \\\n",
    "        ' '.join(list_of_words) + '\\''\n",
    "    import subprocess\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    \n",
    "    # Report results\n",
    "        # Just the last line, containing the training error\n",
    "    training_error = !(tail -n 1 enronemail_1h.txt.output | cut -d' ' -f1)\n",
    "    print 'Training error of the classifier only using the word \\\"{0}\\\": {1}'.\\\n",
    "        format('assistance', training_error[0])\n",
    "\n",
    "# Call the function with the 3 words, and an arbitrary number of partitions\n",
    "hw1_4(10, ['assistance', 'valium', 'enlargementWithATypo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide a mapper/reducer pair that, when executed by `pNaiveBayes.sh`, will classify the email messages by all words present. To do so, make sure that**\n",
    "\n",
    "- **mapper.py counts all occurrences of all words, and**\n",
    "- **reducer.py performs a word-distribution-wide Naive Bayes classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from operator import add\n",
    "total_ham = 0 # count of ham emails\n",
    "total_spam = 0 # count of spam emails\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1] # 1st argument is a file portion\n",
    "\n",
    "# Learn the vocabulary\n",
    "vocabulary = []\n",
    "count_email = 0 # count of lines in file passed to mapper\n",
    "with open(filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines(): # for each line/email in the file\n",
    "        count_email += 1\n",
    "        content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "            # We search the words in both the subject and the content\n",
    "                # because one or the other may not exist, but the way the data are \n",
    "                # stored we don't know which one may be missing\n",
    "        for w in WORD_RE.findall(content):\n",
    "            if w.lower() not in vocabulary:\n",
    "                vocabulary.append(w.lower())\n",
    "# Print vocabulary in the first line of the mapper's output\n",
    "print 'id' + '\\t' + 'category' + '\\t' + '\\t'.join(sorted(vocabulary))\n",
    "vocab_size = len(vocabulary)\n",
    "word_count = [{key: 0 for key in vocabulary} for x in range(count_email)]\n",
    "\n",
    "# Find occurrences per email                \n",
    "with open(filename, \"r\") as myfile:\n",
    "    for (i,line) in enumerate(myfile.readlines()): # for each line in the file\n",
    "        id = line.split(\"\\t\")[0]\n",
    "        category = line.split(\"\\t\")[1]\n",
    "        content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        for k in vocabulary:\n",
    "            for w in WORD_RE.findall(content):\n",
    "                if w.lower() == k:\n",
    "                    word_count[i][k] = word_count[i][k] + 1\n",
    "        if int(category) == 0: # increase count of emails in ham or spam \n",
    "            # as well as the occurrences of each word in each category\n",
    "            total_ham += 1\n",
    "        elif int(category) == 1:\n",
    "            total_spam += 1\n",
    "        word_count_str = [str(word_count[i][k]) for k \\\n",
    "            in sorted(word_count[i].keys())]\n",
    "        print id + '\\t' + category + '\\t' + '\\t'.join(word_count_str)\n",
    "            # Output one line per mail, with id, category, and occurrences of each\n",
    "                # word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "files = sys.argv[1:] #Accept several arguments (same as mapper outputs)\n",
    "total_ham = 0 # total count of ham emails\n",
    "total_spam = 0 # total count of spam emails\n",
    "\n",
    "email_list = []\n",
    "count_emails = -1*len(files) # got to discount 1st lines, with vocabulary\n",
    "# Get the full vocabulary\n",
    "vocabulary = []\n",
    "count_email = 0 # count of total lines/emails\n",
    "for filename in files: # for each file passed to the reducer\n",
    "    with open(filename, \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        first_line = ' '.join(first_line.strip().split(\"\\t\")[2:])\n",
    "        for w in WORD_RE.findall(first_line):\n",
    "            if w.lower() not in vocabulary:\n",
    "                vocabulary.append(w)\n",
    "    count_emails += sum(1 for line in open(filename))\n",
    "vocab_size = len(vocabulary)\n",
    "email_list = []\n",
    "total_word_ham = {key: 0 for key in vocabulary}\n",
    "total_word_spam = {key: 0 for key in vocabulary}\n",
    "# Reconstruct dictionaries\n",
    "for filename in files: # for each file passed to the reducer\n",
    "    with open(filename, \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        partial_vocabulary = first_line.strip().split(\"\\t\")[2:]\n",
    "        for line in f.readlines(): # for each line/email in the file\n",
    "            word_count = {key: 0 for key in vocabulary}\n",
    "            id = line.strip().split(\"\\t\")[0]\n",
    "            category = line.strip().split(\"\\t\")[1]\n",
    "            content = line.strip().split(\"\\t\")[2:]\n",
    "            if int(category) == 0: # increase count of emails in ham or spam\n",
    "                total_ham += 1\n",
    "                for i in range(len(content)):\n",
    "                    total_word_ham[partial_vocabulary[i]] = \\\n",
    "                        total_word_ham[partial_vocabulary[i]] + int(content[i])\n",
    "                    word_count[partial_vocabulary[i]] = int(content[i])                    \n",
    "            elif int(category) == 1:\n",
    "                total_spam += 1\n",
    "                for i in range(len(content)):\n",
    "                    total_word_spam[partial_vocabulary[i]] = \\\n",
    "                        total_word_spam[partial_vocabulary[i]] + int(content[i])\n",
    "                    word_count[partial_vocabulary[i]] = int(content[i])\n",
    "            email_list.append(id+'\\t'+category+'\\t'+\n",
    "                '\\t'.join([str(x) for x in word_count.values()]))\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham.values()) + vocab_size) for x \\\n",
    "    in total_word_ham.values()]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam.values()) + vocab_size) for x \\\n",
    "    in total_word_spam.values()]\n",
    "\n",
    "# Assess classification with the training set \n",
    "errors = 0 # count of misclassification errors\n",
    "for i in range(len(email_list)): # for each line in the mappers' outputs\n",
    "    category = int(email_list[i].split(\"\\t\")[1]) # extract category\n",
    "    # Extract count of occurrences\n",
    "    word_count = [int(x) for x in email_list[i].split(\"\\t\")[2:]] \n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + sum([x*log(y,10) for (x,y) \\\n",
    "        in zip(word_count,prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + sum([x*log(y,10) for (x,y) \\\n",
    "        in zip(word_count,prob_word_spam)])\n",
    "        # The right side of the equations are not equal to prob_category_word, but \n",
    "            # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "            # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        predicted_category = 1\n",
    "    else:\n",
    "        predicted_category = 0\n",
    "    # Output category and predicted category for each email\n",
    "    print email_list[i].split(\"\\t\")[0] + '\\t' + str(category) + '\\t' + \\\n",
    "        str(predicted_category) \n",
    "    if predicted_category != category:\n",
    "        # Increase count of errors the category predicted is wrong\n",
    "        errors += 1\n",
    "training_error = float(errors) / (total_ham+total_spam)\n",
    "print training_error # output the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error of the classifier using all words in the training set:         0.0\n"
     ]
    }
   ],
   "source": [
    "### HW1.5 ###\n",
    "# Report results\n",
    "def hw1_5(n):\n",
    "    # Call pNaiveBayes.sh\n",
    "    bashCommand = './pNaiveBayes.sh ' + str(n)\n",
    "    import subprocess\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    \n",
    "    # Report results\n",
    "        # Just the last line, containing the training error\n",
    "    training_error = !(tail -n 1 enronemail_1h.txt.output | cut -d' ' -f1)\n",
    "    print 'Training error of the classifier using all words in the training set: \\\n",
    "        {1}'.format('assistance', training_error[0])\n",
    "    \n",
    "# Call the function with an arbitrary number of partitions\n",
    "hw1_5(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes**\n",
    "\n",
    "**Let's define $\\text{Training error} = \\text{misclassification rate with respect to a training set}$. It is more formally defined here:**\n",
    "\n",
    "**Let $DF$ represent the training set in the following:**\n",
    "\n",
    "$$Err(Model, DF) = \\frac{|{(X, c(X)) \\in DF : c(X) \\neq Model(X)}|}{|DF|}$$\n",
    "\n",
    "**Where $||$ denotes set cardinality; $c(X)$ denotes the class of the tuple X in DF; and $Model(X)$ denotes the class inferred by the Model $Model$.**\n",
    "\n",
    "- **Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SciKit-Learn to run over this dataset)**\n",
    "- **Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error**\n",
    "- **Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error**\n",
    "\n",
    "**Please prepare a table to present your results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|Classifier     | MapReduce Multinomial| SKLearn Multinomial| SKLearn Bernoulli|\n",
      "--------------------------------------------------------------------------------\n",
      "|Training Error |                  0.00|                0.00|              0.16|\n"
     ]
    }
   ],
   "source": [
    "### HW1.6 ###  \n",
    "import subprocess\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "    \n",
    "def hw1_6(namefile, num_chunks):\n",
    "    ### SKLEARN ###\n",
    "    \n",
    "    content=[]\n",
    "    category=[]\n",
    "    f = open(namefile)\n",
    "    for line in f.readlines():\n",
    "        category.append(line.strip().split(\"\\t\")[1])\n",
    "        #content.append(l.strip().split(\"\\t\")[-1])\n",
    "        # Uncomment line above and comment line below to use only the content and  \n",
    "            # not the subject of each email\n",
    "        # (That yields a 0.02 training error using BinomialNB and 0.19 using \n",
    "            # BernoulliNB, instead of 0.00 and 0.16, respectively)\n",
    "        content.append(' '.join(line.strip().split(\"\\t\")[2:]))\n",
    "        f.close()\n",
    "    category = map(int, category)\n",
    "    CV = CountVectorizer()\n",
    "    feature_vectors = CV.fit_transform(raw_documents=content)\n",
    "    feature_strings = CV.get_feature_names()\n",
    "    #print 'SKLEARN: Average number of (non-zero) features per observation/email: \\\n",
    "    #    {0:.2f}'.format(feature_vectors.nnz / float(len(content)))\n",
    "    #print 'SKLEARN: Some random words that appear in the emails:\\n\\t{0}'.\\\n",
    "    #    format(np.random.choice(feature_strings, 5, replace=False))\n",
    "    \n",
    "    ### SKLEARN MULTINOMIAL NB ###\n",
    "    MultinomialNB_model = MultinomialNB()\n",
    "    MultinomialNB_model.fit(feature_vectors, category)\n",
    "    pred_category_sk_multinomialNB = MultinomialNB_model.predict(feature_vectors)\n",
    "    training_error_sk_multinomialNB = 1 - \\\n",
    "        accuracy_score(category, pred_category_sk_multinomialNB)\n",
    "    #errors = np.sum([category[i] != pred_category_sk_multinomialNB[i] \\\n",
    "    #    for i in range(len(content))], dtype='f')\n",
    "    #print 'Training error: {0:.2f}'.format(errors/len(content))\n",
    "    \n",
    "    ### SKLEARN BERNOULLI NB ###\n",
    "    BernoulliNB_model = BernoulliNB()\n",
    "    BernoulliNB_model.fit(feature_vectors, category)\n",
    "    pred_category_sk_BernoulliNB = BernoulliNB_model.predict(feature_vectors)\n",
    "    training_error_sk_BernoulliNB = 1 - accuracy_score(category, \n",
    "        pred_category_sk_BernoulliNB)\n",
    "    #errors = np.sum([category[i] != pred_category_sk_BernoulliNB[i] \\\n",
    "    #    for i in range(len(content))], dtype='f')\n",
    "    #print 'Training error: {0:.2f}'.format(errors/len(content))\n",
    "    \n",
    "    ### HW1.5: POOR MAN'S MAPREDUCE IMPLEMENTATION OF MULTINOMIAL NB ###\n",
    "    # Call pNaiveBayes.sh\n",
    "    bashCommand = './pNaiveBayes.sh ' + str(num_chunks)\n",
    "    output = subprocess.check_output(['bash','-c', bashCommand])\n",
    "    # Report results\n",
    "        # Just the last line, containing the training error\n",
    "    training_error_mr_multinomialNB = \\\n",
    "        !(tail -n 1 enronemail_1h.txt.output | cut -d' ' -f1)\n",
    "\n",
    "    # Present results\n",
    "    classifier =[' MapReduce Multinomial', ' SKLearn Multinomial', \n",
    "        ' SKLearn Bernoulli']\n",
    "    print\n",
    "    print '|Classifier     |{}|{}|{}|'.format(*classifier)\n",
    "    print '---------------------------------------------------------------------'\\\n",
    "        '-----------'\n",
    "    values = ['Training Error ', float(training_error_mr_multinomialNB[0]), \n",
    "        training_error_sk_multinomialNB, training_error_sk_BernoulliNB]\n",
    "    print '|{}|{:22.2f}|{:20.2f}|{:18.2f}|'.format(*values)\n",
    "\n",
    "hw1_6('enronemail_1h.txt', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same for both implementations: none of them missclassify any observation (email) from the training set. That might be because:\n",
    "\n",
    "1. both extract exactly the same words from the emails, or\n",
    "2. the dataset is not large enough to detect any differences.\n",
    "\n",
    "But I'd dare to say the 1st possibility is the correct one: once they have extracted the vocabulary, the maths behind both implementations should be exactly the same, and hence their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the Multinomial NB implementation in SciKit-Learn performs better than the Bernoulli NB with this training set: a **100% vs. 84% accuracy**. In general, the Bernoulli model performs better for short documents (as emails are; at least most of the ones contained in our sample), but:\n",
    "\n",
    "1. it's not only a short length of the documents but also a small size of the vocabulary where the Bernoulli model excels (and our vocabulary was not small, since we used all words, i.e., there were no *stopwords*), and\n",
    "\n",
    "2. according to some authors (see [McCallum and Nigam, 1998](http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf)) \"*another  point  to  consider  is that  the  multinomial event  model should  be  a  more  accurate  classiffier  for data sets that have a large variance in document length. The multinomial event model naturally handles documents of varying length by incorporating the evidence of each appearing word. The [...] Bernoulli model is a somewhat poor fit for data with varying length, in that it is more likely for a word to occur in a\n",
    "long document regardless of the class.*\" And that was our case, with emails with content ranging from just a few words to more than a thousand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juanjo Carin\n",
    "- [juanjose.carin@ischool.berkeley.edu](mailto:juanjose.carin@ischol.berkeley.com)\n",
    "- W261-2\n",
    "- Week 02\n",
    "- Submission date: 9/15/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-juanjo-VB.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-juanjo-VB.out\n",
      "15/09/19 17:17:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-juanjo-VB.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-juanjo-VB.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-juanjo-VB.out\n",
      "15/09/19 17:18:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Hadoop\n",
    "!/usr/local/hadoop/sbin/start-yarn.sh\n",
    "!/usr/local/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:14:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Create new folder\n",
    "!hdfs dfs -mkdir -p /user/hadoop/dirhw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:14:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "## Upload input file to HDFS\n",
    "!hdfs dfs -put -f enronemail_1h.txt /user/hadoop/dirhw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HW2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Enron data from HW1 an in the  Hadoop MapReduce framework, write  a mapper/reducer for a multinomial Naive Bayes Classifier that will classify the email messages using words present. Also drop words with a frequency of less than three (3). How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifiers on the training dataset:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage is a MapReduce job that learns the whole vocabulary from the training set. The output (a dictionary with all the words present) will be used in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "    # We search the word in both the subject and the content\n",
    "        # because one or the other may not exist, but the way the data are\n",
    "        # stored we don't know which one may be missing\n",
    "    content = re.sub('[^a-z]', ' ', content.lower())\n",
    "    # Discard non-alphanumeric characters and also numbers\n",
    "    words = content.split() # extract words\n",
    "    words = set(words) # extract unique words\n",
    "    vocabulary[1:1] = words # append to vocabulary\n",
    "for word in set(vocabulary):\n",
    "    print '%s\\t%s' % (word, 1) # value here is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "vocabulary = []\n",
    "for line in sys.stdin:\n",
    "    # Take key only (the word) and add to vocabulary if not present\n",
    "    word = line.split(\"\\t\")[0]\n",
    "    #if word not in vocabulary:\n",
    "    #    print word\n",
    "    # If we use the 2 lines above instead of the 3 lines below\n",
    "        # each word in the vocabulary goes in a new line\n",
    "        # (and there's no need to sort)\n",
    "    vocabulary.append(word)\n",
    "vocabulary = sorted(set(vocabulary)) # Get unique words\n",
    "print ' '.join(vocabulary) # Print words separated by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:14:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:14:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted dictionary\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:14:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:14:37 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/20 10:14:37 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/20 10:14:37 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/20 10:14:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 10:14:38 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/20 10:14:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/20 10:14:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1585235232_0001\n",
      "15/09/20 10:14:39 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/20 10:14:39 INFO mapreduce.Job: Running job: job_local1585235232_0001\n",
      "15/09/20 10:14:39 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/20 10:14:39 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/20 10:14:39 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/20 10:14:39 INFO mapred.LocalJobRunner: Starting task: attempt_local1585235232_0001_m_000000_0\n",
      "15/09/20 10:14:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:14:39 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw2/enronemail_1h.txt:0+203983\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/20 10:14:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/20 10:14:40 INFO mapreduce.Job: Job job_local1585235232_0001 running in uber mode : false\n",
      "15/09/20 10:14:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/20 10:14:40 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper1.py]\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/20 10:14:40 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:14:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:14:41 INFO mapred.LocalJobRunner: \n",
      "15/09/20 10:14:41 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/20 10:14:41 INFO mapred.MapTask: Spilling map output\n",
      "15/09/20 10:14:41 INFO mapred.MapTask: bufstart = 0; bufend = 49187; bufvoid = 104857600\n",
      "15/09/20 10:14:41 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26194140(104776560); length = 20257/6553600\n",
      "15/09/20 10:14:42 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/20 10:14:42 INFO mapred.Task: Task:attempt_local1585235232_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/20 10:14:42 INFO mapred.Task: Task 'attempt_local1585235232_0001_m_000000_0' done.\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local1585235232_0001_m_000000_0\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: Starting task: attempt_local1585235232_0001_r_000000_0\n",
      "15/09/20 10:14:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:14:42 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@14d4e8fd\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/20 10:14:42 INFO reduce.EventFetcher: attempt_local1585235232_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/20 10:14:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/20 10:14:42 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1585235232_0001_m_000000_0 decomp: 59319 len: 59323 to MEMORY\n",
      "15/09/20 10:14:42 INFO reduce.InMemoryMapOutput: Read 59319 bytes from map-output for attempt_local1585235232_0001_m_000000_0\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 59319, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->59319\n",
      "15/09/20 10:14:42 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/20 10:14:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:14:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 59315 bytes\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: Merged 1 segments, 59319 bytes to disk to satisfy reduce memory limit\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: Merging 1 files, 59323 bytes from disk\n",
      "15/09/20 10:14:42 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/20 10:14:42 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:14:42 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 59315 bytes\n",
      "15/09/20 10:14:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:14:42 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer1.py]\n",
      "15/09/20 10:14:42 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/20 10:14:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: Records R/W=5065/1\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:14:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:14:43 INFO mapred.Task: Task:attempt_local1585235232_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:14:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:14:44 INFO mapred.Task: Task attempt_local1585235232_0001_r_000000_0 is allowed to commit now\n",
      "15/09/20 10:14:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1585235232_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/dictionary/_temporary/0/task_local1585235232_0001_r_000000\n",
      "15/09/20 10:14:44 INFO mapred.LocalJobRunner: Records R/W=5065/1 > reduce\n",
      "15/09/20 10:14:44 INFO mapred.Task: Task 'attempt_local1585235232_0001_r_000000_0' done.\n",
      "15/09/20 10:14:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local1585235232_0001_r_000000_0\n",
      "15/09/20 10:14:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/20 10:14:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/20 10:14:44 INFO mapreduce.Job: Job job_local1585235232_0001 completed successfully\n",
      "15/09/20 10:14:44 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=328972\n",
      "\t\tFILE: Number of bytes written=900901\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=39058\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=5065\n",
      "\t\tMap output bytes=49187\n",
      "\t\tMap output materialized bytes=59323\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=59323\n",
      "\t\tReduce input records=5065\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=10130\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=205\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=39058\n",
      "15/09/20 10:14:44 INFO streaming.StreamJob: Output directory: dictionary\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "    # Forcing number of reducers to be 1\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -mapper mapper1.py -reducer reducer1.py -input \\\n",
    "    /user/hadoop/dirhw2/enronemail_1h.txt -output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:14:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Move output to local\n",
    "# Delete it from local if a previous version exists\n",
    "!rm ~/Downloads/HW2/dictionary.txt\n",
    "!hdfs dfs -copyToLocal dictionary/part-00000 ~/Downloads/HW2/dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionay we've created have the following words on it:\n",
    "\n",
    "`a ab abidjan ability able abn about above absent absenteeism absolute absolutely absorb abuse abused acce accelerate accelerated accept acceptable accepted accepting accepts access accomodate accomodates accompanied according accordingly account accountability accounting accounts accrual accurate aches achieve achieved acid acquire acquisition acrobaat acrobat across act action activate active activists activities actor actress actual actually ad adage adams adapted add added adding addition additional additionally address addressed addresses addressing addtional adequately adhesion adm admin adminder administration admitted admixture adobe adobee adolescent adr adrianbold ads adult adv advance advanced advantage ...`\n",
    "\n",
    "The second stage is a MapReduce job that applies the vocabulary to build the classifier. A parameter (`min_occurr`) is used to drop those words with a frequence less than that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:15:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put -f ~/Downloads/HW2/dictionary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "f = open('dictionary', 'r')\n",
    "word_dict = []\n",
    "for line in f:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        word_dict.append(word)\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word_count = 0 # count of word in the email\n",
    "    ID = line.split(\"\\t\")[0]\n",
    "    TRUTH = line.split(\"\\t\")[1]\n",
    "    content = ' '.join(line.strip().split(\"\\t\")[2:])\n",
    "        # We search the word in both the subject and the content\n",
    "            # because one or the other may not exist, but the way the data are\n",
    "            # stored we don't know which one may be missing\n",
    "    content = re.sub('[^a-z]', ' ', content.lower())\n",
    "    words = content.split() # extract words\n",
    "    for word in set(word_dict):\n",
    "        print word + '\\t' + str(words.count(word)) + '\\t' + ID + '\\t' + TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/home/hduser/anaconda/bin/python\n",
    "import sys\n",
    "import os\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# Word to use find/use\n",
    "env_vars = os.environ\n",
    "min_occurr = env_vars['min_occurr']\n",
    "\n",
    "word = []\n",
    "word_count = []\n",
    "ID = []\n",
    "TRUTH = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key_values = line.split('\\t')\n",
    "    # The key is ONE of the words we're counting\n",
    "    if len(word) != 0:\n",
    "        if word[-1] != key_values[0]:\n",
    "            word.append(key_values[0])\n",
    "    else:\n",
    "        word.append(key_values[0])\n",
    "    word_count.append(int(key_values[1]))\n",
    "    # ID and TRUTH are replicated for each word in the vocabulary\n",
    "        # so we just need to keep track of them once\n",
    "    if len(word) == 1:\n",
    "        ID.append(key_values[2])\n",
    "        TRUTH.append(int(key_values[3]))\n",
    "\n",
    "# The lists above will have a length equal to\n",
    "    # number_different_words_in_vocab * number_emails_in_dataset\n",
    "vocab_size = len(word)\n",
    "num_emails = len(TRUTH)\n",
    "# Reshape the list word_count into a 2-D numpy array\n",
    "word_count = np.array(word_count).reshape(len(word), num_emails)\n",
    "# Drop words with a frequency of less than min_ocur\n",
    "condition = np.sum(word_count,1) >= int(min_occurr)\n",
    "final_word_count = word_count[condition, :]\n",
    "filtered_indices = np.extract(condition, word_count).tolist()\n",
    "final_word = [word[i] for i in filtered_indices]\n",
    "final_vocab_size = len(final_word)\n",
    "                        \n",
    "total_spam = sum(TRUTH) # total count of spam emails\n",
    "total_ham = len(TRUTH) - total_spam # total count of ham emails\n",
    "total_word_spam = [0]*len(final_word)\n",
    "total_word_ham = [0]*len(final_word)\n",
    "for i,w in enumerate(final_word):\n",
    "    # Total count of word w in spam emails\n",
    "    total_word_spam[i] = sum([x*y for (x,y) in zip(TRUTH,final_word_count[i])]) \n",
    "    # Total count of word w in ham emails\n",
    "    total_word_ham[i] = sum(final_word_count[i]) - total_word_spam[i]\n",
    "\n",
    "# PRIORS\n",
    "prob_ham = float(total_ham)/(total_ham+total_spam)\n",
    "prob_spam = 1 - prob_ham\n",
    "# CONDITIONAL LIKELIHOODS\n",
    "prob_word_ham = [float(x+1) / (sum(total_word_ham)+final_vocab_size) for x \\\n",
    "                 in total_word_ham]\n",
    "prob_word_spam = [float(x+1) / (sum(total_word_spam)+final_vocab_size) for \\\n",
    "                  x in total_word_spam]\n",
    "\n",
    "# Assess classification with the training set \n",
    "CLASS = [0]*num_emails\n",
    "for i in range(num_emails): # for each email\n",
    "    # POSTERIORS\n",
    "    prob_ham_word = log(prob_ham,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(final_word_count[:,i],prob_word_ham)])\n",
    "    prob_spam_word = log(prob_spam,10) + \\\n",
    "        sum([x*log(y,10) for (x,y) in zip(final_word_count[:,i],prob_word_spam)])\n",
    "    # The right side of the equations are not equal to prob_category_word, but \n",
    "        # to log(prob_category_word) - log(prob_word) (where prob_word is the \n",
    "        # EVIDENCE). It's OK since we only want to compare the POSTERIORS\n",
    "    if prob_spam_word > prob_ham_word: # classify as spam if posterior is higher\n",
    "        CLASS[i] = 1\n",
    "    # Output for each email\n",
    "    print ID[i] + '\\t' + str(TRUTH[i]) + '\\t' + str(CLASS[i])\n",
    "\n",
    "# Training error\n",
    "# Count of misclassification errors\n",
    "errors = sum([x!=y for (x,y) in zip(TRUTH,CLASS)])\n",
    "training_error = float(errors) / len(TRUTH)\n",
    "# Additional line\n",
    "print 'Training Error\\t' + str(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:15:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:15:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r allNBEnron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try with `min_occurr=1`, i.e., using all words, regardless of their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:15:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:15:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/20 10:15:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/20 10:15:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/20 10:15:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/20 10:15:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 10:15:13 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/20 10:15:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1243911755_0001\n",
      "15/09/20 10:15:15 INFO mapred.LocalDistributedCacheManager: Creating symlink: /app/hadoop/tmp/mapred/local/1442769314573/dictionary.txt <- /home/hduser/Downloads/HW2/dictionary\n",
      "15/09/20 10:15:15 INFO mapred.LocalDistributedCacheManager: Localized file:/home/hduser/Downloads/HW2/dictionary.txt as file:/app/hadoop/tmp/mapred/local/1442769314573/dictionary.txt\n",
      "15/09/20 10:15:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/20 10:15:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/20 10:15:15 INFO mapreduce.Job: Running job: job_local1243911755_0001\n",
      "15/09/20 10:15:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/20 10:15:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/20 10:15:16 INFO mapred.LocalJobRunner: Starting task: attempt_local1243911755_0001_m_000000_0\n",
      "15/09/20 10:15:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw2/enronemail_1h.txt:0+203983\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/20 10:15:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/20 10:15:16 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper2.py]\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/20 10:15:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/20 10:15:16 INFO mapreduce.Job: Job job_local1243911755_0001 running in uber mode : false\n",
      "15/09/20 10:15:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/20 10:15:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:17 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/20 10:15:18 INFO streaming.PipeMapRed: R/W/S=100/187751/0 in:100=100/1 [rec/s] out:187751=187751/1 [rec/s]\n",
      "15/09/20 10:15:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:15:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:15:20 INFO mapred.LocalJobRunner: \n",
      "15/09/20 10:15:20 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/20 10:15:20 INFO mapred.MapTask: Spilling map output\n",
      "15/09/20 10:15:20 INFO mapred.MapTask: bufstart = 0; bufend = 17439754; bufvoid = 104857600\n",
      "15/09/20 10:15:20 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24188400(96753600); length = 2025997/6553600\n",
      "15/09/20 10:15:22 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/20 10:15:22 INFO mapred.Task: Task:attempt_local1243911755_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/20 10:15:22 INFO mapred.Task: Task 'attempt_local1243911755_0001_m_000000_0' done.\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local1243911755_0001_m_000000_0\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: Starting task: attempt_local1243911755_0001_r_000000_0\n",
      "15/09/20 10:15:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:15:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f87a22f\n",
      "15/09/20 10:15:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/20 10:15:22 INFO reduce.EventFetcher: attempt_local1243911755_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/20 10:15:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1243911755_0001_m_000000_0 decomp: 18452756 len: 18452760 to MEMORY\n",
      "15/09/20 10:15:22 INFO reduce.InMemoryMapOutput: Read 18452756 bytes from map-output for attempt_local1243911755_0001_m_000000_0\n",
      "15/09/20 10:15:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18452756, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18452756\n",
      "15/09/20 10:15:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/20 10:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:15:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/20 10:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/20 10:15:22 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/20 10:15:23 INFO reduce.MergeManagerImpl: Merged 1 segments, 18452756 bytes to disk to satisfy reduce memory limit\n",
      "15/09/20 10:15:23 INFO reduce.MergeManagerImpl: Merging 1 files, 18452760 bytes from disk\n",
      "15/09/20 10:15:23 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/20 10:15:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:15:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/20 10:15:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:15:23 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer2.py]\n",
      "15/09/20 10:15:23 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/20 10:15:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/20 10:15:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:23 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:15:25 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/20 10:15:25 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:50000=100000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/20 10:15:26 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:100000=200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/20 10:15:27 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:100000=300000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "15/09/20 10:15:27 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:100000=400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "15/09/20 10:15:28 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/20 10:15:28 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:125000=500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "15/09/20 10:15:28 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "15/09/20 10:15:31 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/20 10:15:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/20 10:15:34 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/20 10:15:49 INFO streaming.PipeMapRed: Records R/W=506500/1\n",
      "15/09/20 10:15:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:15:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:15:49 INFO mapred.Task: Task:attempt_local1243911755_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:15:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/20 10:15:49 INFO mapred.Task: Task attempt_local1243911755_0001_r_000000_0 is allowed to commit now\n",
      "15/09/20 10:15:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1243911755_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/allNBEnron/_temporary/0/task_local1243911755_0001_r_000000\n",
      "15/09/20 10:15:49 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/20 10:15:49 INFO mapred.Task: Task 'attempt_local1243911755_0001_r_000000_0' done.\n",
      "15/09/20 10:15:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1243911755_0001_r_000000_0\n",
      "15/09/20 10:15:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/20 10:15:50 INFO mapreduce.Job: Job job_local1243911755_0001 completed successfully\n",
      "15/09/20 10:15:50 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37193962\n",
      "\t\tFILE: Number of bytes written=56164234\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2691\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=506500\n",
      "\t\tMap output bytes=17439754\n",
      "\t\tMap output materialized bytes=18452760\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=18452760\n",
      "\t\tReduce input records=506500\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=1013000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2691\n",
      "15/09/20 10:15:50 INFO streaming.StreamJob: Output directory: allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -files 'dictionary.txt#dictionary' -cmdenv \\\n",
    "    min_occurr=1  -mapper mapper2.py -reducer reducer2.py -input \\\n",
    "    /user/hadoop/dirhw2/enronemail_1h.txt -output allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:15:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "Training Error\t0.0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat allNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above (and as expected) the training error is **zero**, so the accuracy at classifying the same dataset used to train the model is perfect. Now let's drop all words with a frequency of less than three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:15:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:16:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Delete previous output (if it exists)\n",
    "!hdfs dfs -rm -r allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:16:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/20 10:16:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "15/09/20 10:16:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/20 10:16:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/20 10:16:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/20 10:16:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 10:16:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/20 10:16:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local524957098_0001\n",
      "15/09/20 10:16:11 INFO mapred.LocalDistributedCacheManager: Creating symlink: /app/hadoop/tmp/mapred/local/1442769371337/dictionary.txt <- /home/hduser/Downloads/HW2/dictionary\n",
      "15/09/20 10:16:11 INFO mapred.LocalDistributedCacheManager: Localized file:/home/hduser/Downloads/HW2/dictionary.txt as file:/app/hadoop/tmp/mapred/local/1442769371337/dictionary.txt\n",
      "15/09/20 10:16:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/20 10:16:12 INFO mapreduce.Job: Running job: job_local524957098_0001\n",
      "15/09/20 10:16:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/20 10:16:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/20 10:16:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/20 10:16:12 INFO mapred.LocalJobRunner: Starting task: attempt_local524957098_0001_m_000000_0\n",
      "15/09/20 10:16:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:16:12 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hadoop/dirhw21/enronemail_1h.txt:0+203983\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/20 10:16:13 INFO mapreduce.Job: Job job_local524957098_0001 running in uber mode : false\n",
      "15/09/20 10:16:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/20 10:16:13 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/20 10:16:13 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./mapper2.py]\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/20 10:16:13 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/20 10:16:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:14 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "15/09/20 10:16:16 INFO streaming.PipeMapRed: R/W/S=100/187751/0 in:50=100/2 [rec/s] out:93875=187751/2 [rec/s]\n",
      "15/09/20 10:16:18 INFO mapred.LocalJobRunner: Records R/W=72/1 > map\n",
      "15/09/20 10:16:19 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/09/20 10:16:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:16:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:16:20 INFO mapred.LocalJobRunner: Records R/W=72/1 > map\n",
      "15/09/20 10:16:20 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/20 10:16:20 INFO mapred.MapTask: Spilling map output\n",
      "15/09/20 10:16:20 INFO mapred.MapTask: bufstart = 0; bufend = 17439754; bufvoid = 104857600\n",
      "15/09/20 10:16:20 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24188400(96753600); length = 2025997/6553600\n",
      "15/09/20 10:16:21 INFO mapred.LocalJobRunner: Records R/W=72/1 > sort\n",
      "15/09/20 10:16:22 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/20 10:16:22 INFO mapred.Task: Task:attempt_local524957098_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:16:22 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "15/09/20 10:16:22 INFO mapred.Task: Task 'attempt_local524957098_0001_m_000000_0' done.\n",
      "15/09/20 10:16:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local524957098_0001_m_000000_0\n",
      "15/09/20 10:16:22 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/20 10:16:22 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/20 10:16:22 INFO mapred.LocalJobRunner: Starting task: attempt_local524957098_0001_r_000000_0\n",
      "15/09/20 10:16:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "15/09/20 10:16:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@75ee2516\n",
      "15/09/20 10:16:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/20 10:16:22 INFO reduce.EventFetcher: attempt_local524957098_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/20 10:16:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local524957098_0001_m_000000_0 decomp: 18452756 len: 18452760 to MEMORY\n",
      "15/09/20 10:16:23 INFO reduce.InMemoryMapOutput: Read 18452756 bytes from map-output for attempt_local524957098_0001_m_000000_0\n",
      "15/09/20 10:16:23 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18452756, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18452756\n",
      "15/09/20 10:16:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/20 10:16:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:16:23 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/20 10:16:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:16:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/20 10:16:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/20 10:16:23 INFO reduce.MergeManagerImpl: Merged 1 segments, 18452756 bytes to disk to satisfy reduce memory limit\n",
      "15/09/20 10:16:23 INFO reduce.MergeManagerImpl: Merging 1 files, 18452760 bytes from disk\n",
      "15/09/20 10:16:23 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/20 10:16:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/20 10:16:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18452752 bytes\n",
      "15/09/20 10:16:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/20 10:16:23 INFO streaming.PipeMapRed: PipeMapRed exec [/home/hduser/Downloads/HW2/./reducer2.py]\n",
      "15/09/20 10:16:23 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/20 10:16:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/20 10:16:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:24 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:24 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/20 10:16:25 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "15/09/20 10:16:26 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:100000=200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/20 10:16:26 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "15/09/20 10:16:27 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:133333=400000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "15/09/20 10:16:28 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:125000=500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "15/09/20 10:16:28 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "15/09/20 10:16:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/20 10:16:31 INFO streaming.PipeMapRed: Records R/W=506500/1\n",
      "15/09/20 10:16:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/20 10:16:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/20 10:16:31 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/20 10:16:31 INFO mapred.Task: Task:attempt_local524957098_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/20 10:16:31 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/20 10:16:31 INFO mapred.Task: Task attempt_local524957098_0001_r_000000_0 is allowed to commit now\n",
      "15/09/20 10:16:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local524957098_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/allNBEnron/_temporary/0/task_local524957098_0001_r_000000\n",
      "15/09/20 10:16:31 INFO mapred.LocalJobRunner: Records R/W=506500/1 > reduce\n",
      "15/09/20 10:16:31 INFO mapred.Task: Task 'attempt_local524957098_0001_r_000000_0' done.\n",
      "15/09/20 10:16:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local524957098_0001_r_000000_0\n",
      "15/09/20 10:16:31 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/20 10:16:32 INFO mapreduce.Job: Job job_local524957098_0001 completed successfully\n",
      "15/09/20 10:16:32 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37193964\n",
      "\t\tFILE: Number of bytes written=56161468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407966\n",
      "\t\tHDFS: Number of bytes written=2692\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=506500\n",
      "\t\tMap output bytes=17439754\n",
      "\t\tMap output materialized bytes=18452760\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=18452760\n",
      "\t\tReduce input records=506500\n",
      "\t\tReduce output records=101\n",
      "\t\tSpilled Records=1013000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=250\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=335290368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203983\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2692\n",
      "15/09/20 10:16:32 INFO streaming.StreamJob: Output directory: allNBEnron\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar /home/hduser/Downloads/hadoop-streaming*.jar -D \\\n",
    "    mapred.reduce.tasks=1 -files 'dictionary.txt#dictionary' -cmdenv \\\n",
    "    min_occurr=3  -mapper mapper2.py -reducer reducer2.py -input \\\n",
    "    /user/hadoop/dirhw21/enronemail_1h.txt -output allNBEnron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/20 10:16:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t1\n",
      "Training Error\t0.01\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat allNBEnron/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping very infrequent words causes the training error to be just slightly higher (**1%** instead of 0%). If we try higher values of `min_occurr` (dropping words unless they are quite frequent), the training error keeps increasing, though at a very low rate.\n",
    "\n",
    "At least in a real case (measuring accuracy with a test set rather than the training set), it makes more sense to use very frequent words as **stopwords** (i.e., to drop words that appear more than $n$ times), because they're likely to be present in both types of emails (spam and ham), and hence do not characterize the kind of email we're trying to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "15/09/19 17:26:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "15/09/19 17:27:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop/sbin/stop-yarn.sh\n",
    "!/usr/local/hadoop/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
